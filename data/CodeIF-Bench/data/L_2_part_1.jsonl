{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/socketutils.py", "signature_position": [667, 667], "body_position": [668, 669], "dependency": {"intra_class": ["boltons.socketutils.NetstringSocket._calc_msgsize_maxsize", "boltons.socketutils.NetstringSocket._msgsize_maxsize", "boltons.socketutils.NetstringSocket.maxsize"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Set the maximum size for receiving netstrings in the NetstringSocket instance. It updates the maxsize of the instance and calculates the maximum size for a netstring message based on the new maxsize value.", "Arguments": ":param self: NetstringSocket. An instance of the NetstringSocket class.\n:param maxsize: The maximum size for receiving netstrings.\n:return: No return values."}, "tests": ["tests/test_socketutils.py::test_socketutils_netstring"], "indent": 4, "domain": "Utilities", "code": "    def setmaxsize(self, maxsize):\n        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n", "context": "class NetstringSocket(object):\n    \"\"\"\n    Reads and writes using the netstring protocol.\n\n    More info: https://en.wikipedia.org/wiki/Netstring\n    Even more info: http://cr.yp.to/proto/netstrings.txt\n    \"\"\"\n    def __init__(self, sock, timeout=DEFAULT_TIMEOUT, maxsize=DEFAULT_MAXSIZE):\n        self.bsock = BufferedSocket(sock)\n        self.timeout = timeout\n        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n\n    def fileno(self):\n        return self.bsock.fileno()\n\n    def settimeout(self, timeout):\n        self.timeout = timeout\n\n###The function: setmaxsize###\n    def _calc_msgsize_maxsize(self, maxsize):\n        return len(str(maxsize)) + 1  # len(str()) == log10\n\n    def read_ns(self, timeout=_UNSET, maxsize=_UNSET):\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n            msgsize_maxsize = self._msgsize_maxsize\n        else:\n            msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n\n        size_prefix = self.bsock.recv_until(b':',\n                                            timeout=timeout,\n                                            maxsize=msgsize_maxsize)\n        try:\n            size = int(size_prefix)\n        except ValueError:\n            raise NetstringInvalidSize('netstring message size must be valid'\n                                       ' integer, not %r' % size_prefix)\n\n        if size > maxsize:\n            raise NetstringMessageTooLong(size, maxsize)\n        payload = self.bsock.recv_size(size)\n        if self.bsock.recv(1) != b',':\n            raise NetstringProtocolError(\"expected trailing ',' after message\")\n\n        return payload\n\n    def write_ns(self, payload):\n        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)\n", "prompt": "Please write a python function called 'setmaxsize' base the context. Set the maximum size for receiving netstrings in the NetstringSocket instance. It updates the maxsize of the instance and calculates the maximum size for a netstring message based on the new maxsize value.:param self: NetstringSocket. An instance of the NetstringSocket class.\n:param maxsize: The maximum size for receiving netstrings.\n:return: No return values..\n        The context you need to refer to is as follows: class NetstringSocket(object):\n    \"\"\"\n    Reads and writes using the netstring protocol.\n\n    More info: https://en.wikipedia.org/wiki/Netstring\n    Even more info: http://cr.yp.to/proto/netstrings.txt\n    \"\"\"\n    def __init__(self, sock, timeout=DEFAULT_TIMEOUT, maxsize=DEFAULT_MAXSIZE):\n        self.bsock = BufferedSocket(sock)\n        self.timeout = timeout\n        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n\n    def fileno(self):\n        return self.bsock.fileno()\n\n    def settimeout(self, timeout):\n        self.timeout = timeout\n\n###The function: setmaxsize###\n    def _calc_msgsize_maxsize(self, maxsize):\n        return len(str(maxsize)) + 1  # len(str()) == log10\n\n    def read_ns(self, timeout=_UNSET, maxsize=_UNSET):\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n            msgsize_maxsize = self._msgsize_maxsize\n        else:\n            msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n\n        size_prefix = self.bsock.recv_until(b':',\n                                            timeout=timeout,\n                                            maxsize=msgsize_maxsize)\n        try:\n            size = int(size_prefix)\n        except ValueError:\n            raise NetstringInvalidSize('netstring message size must be valid'\n                                       ' integer, not %r' % size_prefix)\n\n        if size > maxsize:\n            raise NetstringMessageTooLong(size, maxsize)\n        payload = self.bsock.recv_size(size)\n        if self.bsock.recv(1) != b',':\n            raise NetstringProtocolError(\"expected trailing ',' after message\")\n\n        return payload\n\n    def write_ns(self, payload):\n        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)\n", "test_list": ["def test_socketutils_netstring():\n    \"\"\"A holistic feature test of BufferedSocket via the NetstringSocket\n    wrapper. Runs\n    \"\"\"\n    print('running self tests')\n    server_socket = socket.socket()\n    server_socket.bind(('127.0.0.1', 0))\n    server_socket.listen(100)\n    ip, port = server_socket.getsockname()\n    start_server = lambda: netstring_server(server_socket)\n    threading.Thread(target=start_server).start()\n\n    def client_connect():\n        clientsock = socket.create_connection((ip, port))\n        client = NetstringSocket(clientsock)\n        return client\n    client = client_connect()\n    client.write_ns(b'ping')\n    assert client.read_ns() == b'pong'\n    s = time.time()\n    for i in range(1000):\n        client.write_ns(b'ping')\n        assert client.read_ns() == b'pong'\n    dur = time.time() - s\n    print('netstring ping-pong latency', dur, 'ms')\n    s = time.time()\n    for i in range(1000):\n        client.write_ns(b'ping')\n    resps = []\n    for i in range(1000):\n        resps.append(client.read_ns())\n    e = time.time()\n    assert all([r == b'pong' for r in resps])\n    assert client.bsock.getrecvbuffer() == b''\n    dur = e - s\n    print('netstring pipelined ping-pong latency', dur, 'ms')\n    client.write_ns(b'close')\n    try:\n        client.read_ns()\n        raise Exception('read from closed socket')\n    except ConnectionClosed:\n        print('raised ConnectionClosed correctly')\n    client = client_connect()\n    client.setmaxsize(128 * 1024)\n    client.write_ns(b'reply128k')\n    res = client.read_ns()\n    assert len(res) == 128 * 1024\n    client.write_ns(b'close')\n    client = client_connect()\n    client.settimeout(0.1)\n    try:\n        client.read_ns()\n        raise Exception('did not timeout')\n    except Timeout:\n        print('read_ns raised timeout correctly')\n    client.write_ns(b'close')\n    client = client_connect()\n    client.setmaxsize(2048)\n    client.write_ns(b'reply4k')\n    try:\n        client.read_ns()\n        raise Exception('read more than maxsize')\n    except NetstringMessageTooLong:\n        print('raised MessageTooLong correctly')\n    try:\n        client.bsock.recv_until(b'b', maxsize=4096)\n        raise Exception('recv_until did not raise MessageTooLong')\n    except MessageTooLong:\n        print('raised MessageTooLong correctly')\n    assert client.bsock.recv_size(4097) == b'a' * 4096 + b','\n    print('correctly maintained buffer after exception raised')\n    client.bsock.settimeout(0.01)\n    try:\n        client.bsock.recv_until(b'a')\n        raise Exception('recv_until did not raise Timeout')\n    except Timeout:\n        print('recv_until correctly raised Timeout')\n    try:\n        client.bsock.recv_size(1)\n        raise Exception('recv_size did not raise Timeout')\n    except Timeout:\n        print('recv_size correctly raised Timeout')\n    client.write_ns(b'shutdown')\n    print('all passed')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'setmaxsize' function should accept an integer 'maxsize' parameter and update the instance's 'maxsize' attribute accordingly. It should also update '_msgsize_maxsize' using '_calc_msgsize_maxsize'.", "unit_test": "def test_setmaxsize_updates_attributes():\n    client = client_connect()\n    new_maxsize = 1024\n    client.setmaxsize(new_maxsize)\n    assert client.maxsize == new_maxsize\n    assert client._msgsize_maxsize == len(str(new_maxsize)) + 1\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_updates_attributes"}, "Exception Handling": {"requirement": "The 'setmaxsize' function should raise a ValueError if the 'maxsize' parameter is not a positive integer or zero.", "unit_test": "def test_setmaxsize_raises_valueerror_on_invalid_maxsize():\n    client = client_connect()\n    try:\n        client.setmaxsize(-1)\n        raise Exception('setmaxsize did not raise ValueError')\n    except ValueError:\n        print('setmaxsize correctly raised ValueError for negative maxsize')\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_raises_valueerror_on_invalid_maxsize"}, "Functionality Extension": {"requirement": "Extend the 'setmaxsize' function to print a message: 'Maxsize set to {new_maxsize}' indicating the change in 'maxsize' for debugging purposes.", "unit_test": "def test_setmaxsize_logs_message():\n    client = client_connect()\n    new_maxsize = 2048\n    with patch('builtins.print') as mocked_print:\n        client.setmaxsize(new_maxsize)\n        mocked_print.assert_called_with(f'Maxsize set to {new_maxsize}')\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the 'setmaxsize' function includes type annotations for its parameters and return type.", "unit_test": "def test_setmaxsize_annotations():\n    annotations = NetstringSocket.setmaxsize.__annotations__\n    assert annotations['maxsize'] == int\n    assert annotations['return'] == None\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_annotations"}, "Code Complexity": {"requirement": "The 'setmaxsize' function should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": "def test_setmaxsize_complexity():\n    complexity = calculate_cyclomatic_complexity(NetstringSocket.setmaxsize)\n    assert complexity == 1\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_complexity"}, "Context Usage Verification": {"requirement": "The 'setmaxsize' function should utilize the '_calc_msgsize_maxsize' method to update '_msgsize_maxsize'.", "unit_test": "def test_setmaxsize_uses_calc_msgsize_maxsize():\n    client = client_connect()\n    new_maxsize = 4096\n    with patch.object(NetstringSocket, '_calc_msgsize_maxsize', wraps=client._calc_msgsize_maxsize) as mocked_method:\n        client.setmaxsize(new_maxsize)\n        mocked_method.assert_called_once_with(new_maxsize)\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_uses_calc_msgsize_maxsize"}, "Context Usage Correctness Verification": {"requirement": "Verify that the '_msgsize_maxsize' is correctly updated based on the new 'maxsize' using '_calc_msgsize_maxsize'.", "unit_test": "def test_setmaxsize_correct_msgsize_maxsize_update():\n    client = client_connect()\n    new_maxsize = 5120\n    client.setmaxsize(new_maxsize)\n    expected_msgsize_maxsize = len(str(new_maxsize)) + 1\n    assert client._msgsize_maxsize == expected_msgsize_maxsize\n    client.write_ns(b'close')", "test": "tests/test_socketutils.py::test_setmaxsize_updates_attributes"}}}
{"namespace": "gunicorn.config.Config.__str__", "type": "method", "project_path": "Utilities/gunicorn", "completion_path": "Utilities/gunicorn/gunicorn/config.py", "signature_position": [54, 54], "body_position": [55, 62], "dependency": {"intra_class": ["gunicorn.config.Config.settings"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a string representation of the Config instance. It iterates through the settings dictionary, format all callable values (\"<{qual_name}()>\"), then formats each key-value pair (\"{key:{key_max_length}} = {value}\"), and appends it to a list. Finally, it joins all the lines in the list with a newline character and returns the resulting string.", "Arguments": ":param self: Config. An instance of the Config class.\n:return: str. The string representation of the Config instance."}, "tests": ["tests/test_config.py::test_str"], "indent": 4, "domain": "Utilities", "code": "    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n", "context": "class Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n###The function: __str__###\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n    def logger_class(self):\n        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n", "prompt": "Please write a python function called '__str__' base the context. This function returns a string representation of the Config instance. It iterates through the settings dictionary, format all callable values (\"<{qual_name}()>\"), then formats each key-value pair (\"{key:{key_max_length}} = {value}\"), and appends it to a list. Finally, it joins all the lines in the list with a newline character and returns the resulting string.:param self: Config. An instance of the Config class.\n:return: str. The string representation of the Config instance..\n        The context you need to refer to is as follows: class Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n###The function: __str__###\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n    def logger_class(self):\n        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n", "test_list": ["def test_str():\n    c = config.Config()\n    o = str(c)\n    OUTPUT_MATCH = {'access_log_format': '%(h)s %(l)s %(u)s %(t)s \"%(r)s\" %(s)s %(b)s \"%(f)s\" \"%(a)s\"', 'accesslog': 'None', 'backlog': '2048', 'bind': \"['127.0.0.1:8000']\", 'capture_output': 'False', 'child_exit': '<ChildExit.child_exit()>'}\n    for i, line in enumerate(o.splitlines()):\n        m = re.match('^(\\\\w+)\\\\s+= ', line)\n        assert m, \"Line {} didn't match expected format: {!r}\".format(i, line)\n        key = m.group(1)\n        try:\n            s = OUTPUT_MATCH.pop(key)\n        except KeyError:\n            continue\n        line_re = '^{}\\\\s+= {}$'.format(key, re.escape(s))\n        assert re.match(line_re, line), '{!r} != {!r}'.format(line_re, line)\n        if not OUTPUT_MATCH:\n            break\n    else:\n        assert False, 'missing expected setting lines? {}'.format(OUTPUT_MATCH.keys())"], "requirements": {"Input-Output Conditions": {"requirement": "The __str__ method should return a string where each line represents a key-value pair from the settings dictionary, formatted as '{key:{key_max_length}} = {value}'. Callable values should be formatted as '<{qual_name}()>'.", "unit_test": "def test_str_output_format():\n    c = config.Config()\n    output = str(c)\n    for line in output.splitlines():\n        assert re.match(r'^\\w+\\s+=\\s+.+$', line), f'Line format is incorrect: {line}'", "test": "tests/test_config.py::test_str_output_format"}, "Exception Handling": {"requirement": "The __str__ method should handle cases where the settings dictionary contains non-callable objects that do not have a __str__ method, and should not raise an exception 'Error processing non-callable objects'.", "unit_test": "def test_str_exception_handling():\n    c = config.Config()\n    c.settings['non_callable'] = object()\n    try:\n        str(c)\n    except Exception as e:\n        assert False, f'__str__ raised an exception: {e}'", "test": "tests/test_config.py::test_str_exception_handling"}, "Edge Case Handling": {"requirement": "The __str__ method should correctly handle an empty settings dictionary and return an empty string.", "unit_test": "def test_str_empty_settings():\n    c = config.Config()\n    c.settings = {}\n    output = str(c)\n    assert output == '', f'Expected empty string, got: {output}'", "test": "tests/test_config.py::test_str_empty_settings"}, "Functionality Extension": {"requirement": "Extend the __str__ method to include a header line 'Config Settings:' at the beginning of the output string.", "unit_test": "def test_str_with_header():\n    c = config.Config()\n    output = str(c)\n    assert output.startswith('Config Settings:\\n'), 'Output should start with header line'", "test": "tests/test_config.py::test_str_with_header"}, "Annotation Coverage": {"requirement": "Ensure that the __str__ method is fully documented with a docstring explaining its purpose, parameters, and return value.", "unit_test": "def test_str_docstring():\n    assert hasattr(config.Config.__str__, '__doc__'), '__str__ method should have a docstring'\n    assert config.Config.__str__.__doc__, '__str__ method docstring should not be empty'", "test": "tests/test_config.py::test_str_docstring"}, "Code Standard": {"requirement": "The __str__ method should adhere to PEP 8 standards, including proper indentation, line length, and spacing.", "unit_test": "def test_str_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/config.py'])\n    assert result.total_errors == 0, f'Found PEP 8 errors: {result.total_errors}'", "test": "tests/test_config.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The __str__ method should utilize the settings attribute from the Config class context.", "unit_test": "def test_str_context_usage():\n    c = config.Config()\n    assert 'settings' in c.__dict__, 'Config instance should have a settings attribute'\n    output = str(c)\n    assert 'settings' in output, 'Output should reflect usage of settings attribute'", "test": "tests/test_config.py::test_str_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The __str__ method should correctly access and format values from the settings dictionary, ensuring that callable values are formatted as '<{qual_name}()>' and non-callable values are represented as strings.", "unit_test": "def test_str_context_correctness():\n    c = config.Config()\n    c.settings['callable'] = lambda: None\n    output = str(c)\n    assert '<lambda()>' in output, 'Callable values should be formatted correctly'\n    c.settings['non_callable'] = 'test_value'\n    output = str(c)\n    assert 'non_callable = test_value' in output, 'Non-callable values should be formatted correctly'", "test": "tests/test_config.py::test_str_context_correctness"}}}
{"namespace": "pyramid.registry.Introspector.remove", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/registry.py", "signature_position": [163, 163], "body_position": [164, 173], "dependency": {"intra_class": ["pyramid.registry.Introspector._categories", "pyramid.registry.Introspector._refs", "pyramid.registry.Introspector.get"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Remove an introspection object from the Introspector instance. It first retrieves the introspection object based on the category name and discriminator. If the object is found, it removes all references to the object and deletes it from the category dictionary.", "Arguments": ":param self: Introspector. An instance of the Introspector class.\n:param category_name: str. The name of the category where the introspection object belongs.\n:param discriminator: The discriminator of the introspection object.\n:return: No return values."}, "tests": ["tests/test_registry.py::TestIntrospector::test_remove"], "indent": 4, "domain": "Internet", "code": "    def remove(self, category_name, discriminator):\n        intr = self.get(category_name, discriminator)\n        if intr is None:\n            return\n        L = self._refs.pop(intr, [])\n        for d in L:\n            L2 = self._refs[d]\n            L2.remove(intr)\n        category = self._categories[intr.category_name]\n        del category[intr.discriminator]\n        del category[intr.discriminator_hash]\n", "context": "class Introspector:\n    def __init__(self):\n        self._refs = {}\n        self._categories = {}\n        self._counter = 0\n\n    def add(self, intr):\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n\n    def get(self, category_name, discriminator, default=None):\n        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr\n\n    def get_category(self, category_name, default=None, sort_key=None):\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]\n\n    def categorized(self, sort_key=None):\n        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L\n\n    def categories(self):\n        return sorted(self._categories.keys())\n\n###The function: remove###\n    def _get_intrs_by_pairs(self, pairs):\n        introspectables = []\n        for pair in pairs:\n            category_name, discriminator = pair\n            intr = self._categories.get(category_name, {}).get(discriminator)\n            if intr is None:\n                raise KeyError((category_name, discriminator))\n            introspectables.append(intr)\n        return introspectables\n\n    def relate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)\n\n    def unrelate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.get(x, [])\n            if y in L:\n                L.remove(y)\n\n    def related(self, intr):\n        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n", "prompt": "Please write a python function called 'remove' base the context. Remove an introspection object from the Introspector instance. It first retrieves the introspection object based on the category name and discriminator. If the object is found, it removes all references to the object and deletes it from the category dictionary.:param self: Introspector. An instance of the Introspector class.\n:param category_name: str. The name of the category where the introspection object belongs.\n:param discriminator: The discriminator of the introspection object.\n:return: No return values..\n        The context you need to refer to is as follows: class Introspector:\n    def __init__(self):\n        self._refs = {}\n        self._categories = {}\n        self._counter = 0\n\n    def add(self, intr):\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n\n    def get(self, category_name, discriminator, default=None):\n        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr\n\n    def get_category(self, category_name, default=None, sort_key=None):\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]\n\n    def categorized(self, sort_key=None):\n        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L\n\n    def categories(self):\n        return sorted(self._categories.keys())\n\n###The function: remove###\n    def _get_intrs_by_pairs(self, pairs):\n        introspectables = []\n        for pair in pairs:\n            category_name, discriminator = pair\n            intr = self._categories.get(category_name, {}).get(discriminator)\n            if intr is None:\n                raise KeyError((category_name, discriminator))\n            introspectables.append(intr)\n        return introspectables\n\n    def relate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)\n\n    def unrelate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.get(x, [])\n            if y in L:\n                L.remove(y)\n\n    def related(self, intr):\n        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n", "test_list": ["def test_remove(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    intr2 = DummyIntrospectable()\n    intr2.category_name = 'category2'\n    intr2.discriminator = 'discriminator2'\n    intr2.discriminator_hash = 'discriminator2_hash'\n    inst.add(intr)\n    inst.add(intr2)\n    inst.relate(('category', 'discriminator'), ('category2', 'discriminator2'))\n    inst.remove('category', 'discriminator')\n    self.assertEqual(inst._categories, {'category': {}, 'category2': {'discriminator2': intr2, 'discriminator2_hash': intr2}})\n    self.assertEqual(inst._refs.get(intr), None)\n    self.assertEqual(inst._refs[intr2], [])"], "requirements": {"Input-Output Conditions": {"requirement": "The 'remove' function should ensure that the inputs 'category_name' and 'discriminator' are of type 'str'. If not, it should raise a TypeError.", "unit_test": "def test_remove_input_type_check(self):\n    inst = self._makeOne()\n    with self.assertRaises(TypeError):\n        inst.remove(123, 'discriminator')\n    with self.assertRaises(TypeError):\n        inst.remove('category', 456)", "test": "tests/test_registry.py::TestIntrospector::test_remove_input_type_check"}, "Exception Handling": {"requirement": "The 'remove' function should raise a KeyError with a descriptive message if the introspection object is not found in the specified category.", "unit_test": "def test_remove_key_error(self):\n    inst = self._makeOne()\n    with self.assertRaises(KeyError) as cm:\n        inst.remove('nonexistent_category', 'nonexistent_discriminator')\n    self.assertEqual(str(cm.exception), \"('nonexistent_category', 'nonexistent_discriminator')\")", "test": "tests/test_registry.py::TestIntrospector::test_remove_key_error"}, "Edge Case Handling": {"requirement": "The 'remove' function should handle the case where the category exists but the discriminator does not, by raising a KeyError.", "unit_test": "def test_remove_nonexistent_discriminator(self):\n    inst = self._makeOne()\n    inst.add(DummyIntrospectable())\n    with self.assertRaises(KeyError):\n        inst.remove('category', 'nonexistent_discriminator')", "test": "tests/test_registry.py::TestIntrospector::test_remove_nonexistent_discriminator"}, "Functionality Extension": {"requirement": "Extend the 'remove' function to return a boolean indicating whether an introspection object was successfully removed.", "unit_test": "def test_remove_return_value(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    result = inst.remove('category', 'discriminator')\n    self.assertTrue(result)\n    result = inst.remove('category', 'discriminator')\n    self.assertFalse(result)", "test": "tests/test_registry.py::TestIntrospector::test_remove_return_value"}, "Annotation Coverage": {"requirement": "Ensure that the 'remove' function has complete docstring coverage, including parameter types and a description of the function's behavior.", "unit_test": "def test_remove_docstring(self):\n    inst = self._makeOne()\n    self.assertTrue(inst.remove.__doc__ is not None)\n    self.assertIn(':param category_name: str', inst.remove.__doc__)\n    self.assertIn(':param discriminator: str', inst.remove.__doc__)\n    self.assertIn(':return: bool', inst.remove.__doc__)", "test": "tests/test_registry.py::TestIntrospector::test_remove_docstring"}, "Code Standard": {"requirement": "The 'remove' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_remove_pep8_compliance(self):\n    # This test is a placeholder for a static analysis tool that checks PEP 8 compliance.\n    # Example: Use a tool like flake8 to verify compliance.\n    errors = check_pep8_compliance('remove')\n    self.assertEqual(errors, [])", "test": "tests/test_registry.py::TestIntrospector::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'remove' function should utilize the '_categories' and '_refs' attributes of the Introspector class.", "unit_test": "def test_remove_context_usage(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    inst.remove('category', 'discriminator')\n    self.assertNotIn('discriminator', inst._categories.get('category', {}))\n    self.assertNotIn(intr, inst._refs)", "test": "tests/test_registry.py::TestIntrospector::test_remove_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'remove' function should correctly update the '_categories' and '_refs' attributes by removing the introspection object and its references.", "unit_test": "def test_remove_context_correctness(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    inst.relate(('category', 'discriminator'), ('category2', 'discriminator2'))\n    inst.remove('category', 'discriminator')\n    self.assertNotIn('discriminator', inst._categories.get('category', {}))\n    self.assertEqual(inst._refs.get(intr), None)", "test": "tests/test_registry.py::TestIntrospector::test_remove_context_correctness"}}}
{"namespace": "mrjob.job.MRJob.set_status", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/job.py", "signature_position": [587, 587], "body_position": [594, 599], "dependency": {"intra_class": ["mrjob.job.MRJob.stderr"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function sets the job status in Hadoop streaming by printing a message to the standard error stream of the input MRJob instance. It is also used as a keepalive mechanism to prevent the job from timing out. The format of the message is \"reporter:status:{message}\\n\".", "Arguments": ":param self: MRJob. An instance of the MRJob class.\n:param msg: String. The message to set as the job status.\n:return: No return values."}, "tests": ["tests/test_job.py::CountersAndStatusTestCase::test_counters_and_status"], "indent": 4, "domain": "System", "code": "    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "context": "class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n###The function: set_status###\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "prompt": "Please write a python function called 'set_status' base the context. This function sets the job status in Hadoop streaming by printing a message to the standard error stream of the input MRJob instance. It is also used as a keepalive mechanism to prevent the job from timing out. The format of the message is \"reporter:status:{message}\\n\".:param self: MRJob. An instance of the MRJob class.\n:param msg: String. The message to set as the job status.\n:return: No return values..\n        The context you need to refer to is as follows: class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n###The function: set_status###\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "test_list": ["def test_counters_and_status(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.increment_counter('Foo', 'Bar')\n    mr_job.set_status('Initializing qux gradients...')\n    mr_job.increment_counter('Foo', 'Bar')\n    mr_job.increment_counter('Foo', 'Baz', 20)\n    mr_job.set_status('Sorting metasyntactic variables...')\n    parsed_stderr = parse_mr_job_stderr(mr_job.stderr.getvalue())\n    self.assertEqual(parsed_stderr, {'counters': {'Foo': {'Bar': 2, 'Baz': 20}}, 'statuses': ['Initializing qux gradients...', 'Sorting metasyntactic variables...'], 'other': []})"], "requirements": {"Input-Output Conditions": {"requirement": "The 'set_status' function should accept only string inputs for the 'msg' parameter and should print the status message in the format 'reporter:status:{message}\\n' to the standard error stream.", "unit_test": "def test_set_status_input_output_conditions(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_input_output_conditions"}, "Exception Handling": {"requirement": "The 'set_status' function should raise a TypeError if the 'msg' parameter is not a string.", "unit_test": "def test_set_status_exception_handling(self):\n    mr_job = MRJob([]).sandbox()\n    with self.assertRaises(TypeError):\n        mr_job.set_status(123)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_exception_handling"}, "Edge Case Handling": {"requirement": "The 'set_status' function should handle empty string inputs gracefully by printing 'reporter:status:\\n' to the standard error stream.", "unit_test": "def test_set_status_edge_case_handling(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:\\n', stderr_output)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'set_status' function to accept an optional 'timestamp' parameter that, when provided, appends the current timestamp to the status message.", "unit_test": "def test_set_status_functionality_extension(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message', timestamp=True)\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertRegex(stderr_output, r'reporter:status:Test status message \\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\]\\n')", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'set_status' function is fully documented with parameter and return type annotations.", "unit_test": "def test_set_status_annotation_coverage(self):\n    annotations = MRJob.set_status.__annotations__\n    self.assertEqual(annotations['msg'], str)\n    self.assertEqual(annotations['return'], None)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_annotation_coverage"}, "Code Standard": {"requirement": "The 'set_status' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_set_status_code_standard(self):\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['mrjob/job.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'set_status' function should utilize the 'stderr' property of the MRJob instance to print the status message.", "unit_test": "def test_set_status_context_usage_verification(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'set_status' function correctly uses the 'stderr' property to ensure messages are printed to the correct stream.", "unit_test": "def test_set_status_context_usage_correctness_verification(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)", "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_context_usage_verification"}}}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/registry.py", "signature_position": [232, 232], "body_position": [233, 234], "dependency": {"intra_class": ["pyramid.registry.Introspectable._assert_resolved", "pyramid.registry.Introspectable.discriminator"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Calculate the hash of the discriminator of the Introspectable instance.", "Arguments": ":param self: Introspectable. An instance of the Introspectable class.\n:return: int. The hash value of the discriminator in the instance."}, "tests": ["tests/test_registry.py::TestIntrospectable::test_discriminator_hash"], "indent": 4, "domain": "Internet", "code": "    def discriminator_hash(self):\n        self._assert_resolved()\n        return hash(self.discriminator)\n", "context": "class Introspectable(dict):\n\n    order = 0  # mutated by introspector.add\n    action_info = None  # mutated by self.register\n\n    def __init__(self, category_name, discriminator, title, type_name):\n        self.category_name = category_name\n        self.discriminator = discriminator\n        self.title = title\n        self.type_name = type_name\n        self._relations = []\n\n    def relate(self, category_name, discriminator):\n        self._relations.append((True, category_name, discriminator))\n\n    def unrelate(self, category_name, discriminator):\n        self._relations.append((False, category_name, discriminator))\n\n    def _assert_resolved(self):\n        assert undefer(self.discriminator) is self.discriminator\n\n    @property\n###The function: discriminator_hash###\n    def __hash__(self):\n        self._assert_resolved()\n        return hash((self.category_name,) + (self.discriminator,))\n\n    def __repr__(self):\n        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )\n\n    def __bool__(self):\n        return True\n\n    def register(self, introspector, action_info):\n        self.discriminator = undefer(self.discriminator)\n        self.action_info = action_info\n        introspector.add(self)\n        for relate, category_name, discriminator in self._relations:\n            discriminator = undefer(discriminator)\n            if relate:\n                method = introspector.relate\n            else:\n                method = introspector.unrelate\n            method(\n                (self.category_name, self.discriminator),\n                (category_name, discriminator),\n            )\n", "prompt": "Please write a python function called 'discriminator_hash' base the context. Calculate the hash of the discriminator of the Introspectable instance.:param self: Introspectable. An instance of the Introspectable class.\n:return: int. The hash value of the discriminator in the instance..\n        The context you need to refer to is as follows: class Introspectable(dict):\n\n    order = 0  # mutated by introspector.add\n    action_info = None  # mutated by self.register\n\n    def __init__(self, category_name, discriminator, title, type_name):\n        self.category_name = category_name\n        self.discriminator = discriminator\n        self.title = title\n        self.type_name = type_name\n        self._relations = []\n\n    def relate(self, category_name, discriminator):\n        self._relations.append((True, category_name, discriminator))\n\n    def unrelate(self, category_name, discriminator):\n        self._relations.append((False, category_name, discriminator))\n\n    def _assert_resolved(self):\n        assert undefer(self.discriminator) is self.discriminator\n\n    @property\n###The function: discriminator_hash###\n    def __hash__(self):\n        self._assert_resolved()\n        return hash((self.category_name,) + (self.discriminator,))\n\n    def __repr__(self):\n        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )\n\n    def __bool__(self):\n        return True\n\n    def register(self, introspector, action_info):\n        self.discriminator = undefer(self.discriminator)\n        self.action_info = action_info\n        introspector.add(self)\n        for relate, category_name, discriminator in self._relations:\n            discriminator = undefer(discriminator)\n            if relate:\n                method = introspector.relate\n            else:\n                method = introspector.unrelate\n            method(\n                (self.category_name, self.discriminator),\n                (category_name, discriminator),\n            )\n", "test_list": ["def test_discriminator_hash(self):\n    inst = self._makeOnePopulated()\n    self.assertEqual(inst.discriminator_hash, hash(inst.discriminator))"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'discriminator_hash' should return an integer hash value for the discriminator attribute of the Introspectable instance.", "unit_test": "def test_discriminator_hash_output_type(self):\n    inst = self._makeOnePopulated()\n    self.assertIsInstance(inst.discriminator_hash, int)", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash"}, "Exception Handling": {"requirement": "The function 'discriminator_hash' should raise a TypeError if the discriminator is not hashable.", "unit_test": "def test_discriminator_hash_non_hashable(self):\n    inst = self._makeOnePopulated()\n    inst.discriminator = ['non-hashable']\n    with self.assertRaises(TypeError):\n        _ = inst.discriminator_hash", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_non_hashable"}, "Edge Case Handling": {"requirement": "The function 'discriminator_hash' should correctly handle the case where the discriminator is an empty string.", "unit_test": "def test_discriminator_hash_empty_discriminator(self):\n    inst = self._makeOnePopulated()\n    inst.discriminator = ''\n    self.assertEqual(inst.discriminator_hash, hash(''))", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_empty_discriminator"}, "Functionality Extension": {"requirement": "Extend the 'discriminator_hash' function to include the category_name in the hash calculation.", "unit_test": "def test_discriminator_hash_with_category(self):\n    inst = self._makeOnePopulated()\n    expected_hash = hash((inst.category_name, inst.discriminator))\n    self.assertEqual(inst.discriminator_hash, expected_hash)", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_with_category"}, "Annotation Coverage": {"requirement": "Ensure that the 'discriminator_hash' function is properly documented with a docstring explaining its purpose : 'Calculate the hash'.", "unit_test": "def test_discriminator_hash_docstring(self):\n    self.assertIsNotNone(discriminator_hash.__doc__)\n    self.assertIn('Calculate the hash', discriminator_hash.__doc__)", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_docstring"}, "Context Usage Verification": {"requirement": "The 'discriminator_hash' function should utilize the 'discriminator' attribute from the Introspectable class.", "unit_test": "def test_discriminator_hash_uses_discriminator(self):\n    inst = self._makeOnePopulated()\n    self.assertIn('discriminator', inst.__dict__)\n    _ = inst.discriminator_hash", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_uses_discriminator"}, "Context Usage Correctness Verification": {"requirement": "The 'discriminator_hash' function should correctly use the 'discriminator' attribute to compute the hash value.", "unit_test": "def test_discriminator_hash_correct_usage(self):\n    inst = self._makeOnePopulated()\n    expected_hash = hash(inst.discriminator)\n    self.assertEqual(inst.discriminator_hash, expected_hash)", "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_correct_usage"}}}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/job.py", "signature_position": [1187, 1187], "body_position": [1205, 1207], "dependency": {"intra_class": ["mrjob.job.MRJob._passthru_arg_dests", "mrjob.job.MRJob.arg_parser"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is used to add a command-line argument that both the job runner and the job itself will respect. It creates options that can be used by the job to configure its behavior. The options are added to the argument parser of the job.", "Arguments": ":param self: MRJob. An instance of the MRJob class.\n:param *args: Variable length argument list. The arguments to be passed to  the argument parser.\n:param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the argument parser.\n:return: No return values."}, "tests": ["tests/test_job.py::CommandLineArgsTestCase::test_bad_option_types"], "indent": 4, "domain": "System", "code": "    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "context": "class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n###The function: add_passthru_arg###\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "prompt": "Please write a python function called 'add_passthru_arg' base the context. This function is used to add a command-line argument that both the job runner and the job itself will respect. It creates options that can be used by the job to configure its behavior. The options are added to the argument parser of the job.:param self: MRJob. An instance of the MRJob class.\n:param *args: Variable length argument list. The arguments to be passed to  the argument parser.\n:param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the argument parser.\n:return: No return values..\n        The context you need to refer to is as follows: class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n###The function: add_passthru_arg###\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "test_list": ["def test_bad_option_types(self):\n    mr_job = MRJob(args=[])\n    self.assertRaises(ValueError, mr_job.add_passthru_arg, '--stop-words', dest='stop_words', type='set', default=None)\n    self.assertRaises(ValueError, mr_job.add_passthru_arg, '--leave-a-msg', dest='leave_a_msg', action='callback', default=None)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add_passthru_arg' function should only accept arguments that are valid for the argument parser, ensuring that the input types for options are either 'str', 'int', 'float', or 'bool'.", "unit_test": "def test_valid_option_types(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--valid-str', dest='valid_str', type=str, default='default')\n    mr_job.add_passthru_arg('--valid-int', dest='valid_int', type=int, default=0)\n    mr_job.add_passthru_arg('--valid-float', dest='valid_float', type=float, default=0.0)\n    mr_job.add_passthru_arg('--valid-bool', dest='valid_bool', type=bool, default=False)\n    self.assertIn('valid_str', mr_job._passthru_arg_dests)\n    self.assertIn('valid_int', mr_job._passthru_arg_dests)\n    self.assertIn('valid_float', mr_job._passthru_arg_dests)\n    self.assertIn('valid_bool', mr_job._passthru_arg_dests)", "test": "tests/test_job.py::CommandLineArgsTestCase::test_valid_option_types"}, "Exception Handling": {"requirement": "The 'add_passthru_arg' function should raise a ValueError with a descriptive message: 'Unsupported argument type: dict' if an unsupported type is provided for the argument.", "unit_test": "def test_unsupported_option_type(self):\n    mr_job = MRJob(args=[])\n    with self.assertRaises(ValueError) as cm:\n        mr_job.add_passthru_arg('--unsupported-type', dest='unsupported', type=dict, default=None)\n    self.assertEqual(str(cm.exception), 'Unsupported argument type: dict')", "test": "tests/test_job.py::CommandLineArgsTestCase::test_unsupported_option_type"}, "Edge Case Handling": {"requirement": "The 'add_passthru_arg' function should handle edge cases where no type is specified by defaulting to 'str'.", "unit_test": "def test_default_type(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--default-type', dest='default_type')\n    self.assertIn('default_type', mr_job._passthru_arg_dests)\n    self.assertEqual(mr_job.arg_parser.get_default('default_type'), None)", "test": "tests/test_job.py::CommandLineArgsTestCase::test_default_type"}, "Functionality Extension": {"requirement": "Extend the 'add_passthru_arg' function to support a 'choices' parameter, allowing only specific values for an argument.", "unit_test": "def test_choices_parameter(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--color', dest='color', choices=['red', 'green', 'blue'], default='red')\n    self.assertIn('color', mr_job._passthru_arg_dests)\n    self.assertEqual(mr_job.arg_parser.get_default('color'), 'red')", "test": "tests/test_job.py::CommandLineArgsTestCase::test_default_type"}, "Annotation Coverage": {"requirement": "Ensure that the 'add_passthru_arg' function is fully annotated with parameter and return types.", "unit_test": "def test_function_annotations(self):\n    annotations = MRJob.add_passthru_arg.__annotations__\n    self.assertEqual(annotations['self'], 'MRJob')\n    self.assertEqual(annotations['args'], 'tuple')\n    self.assertEqual(annotations['kwargs'], 'dict')\n    self.assertEqual(annotations['return'], 'None')", "test": "tests/test_job.py::CommandLineArgsTestCase::test_function_annotations"}, "Code Standard": {"requirement": "Ensure that the 'add_passthru_arg' function follows PEP 8 style guidelines, including proper indentation and spacing.", "unit_test": "def test_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['mrjob/job.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/test_job.py::CommandLineArgsTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'add_passthru_arg' function utilizes the 'arg_parser' attribute of the MRJob class to add arguments.", "unit_test": "def test_arg_parser_usage(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--test-arg', dest='test_arg', type=str)\n    self.assertTrue(any('--test-arg' in action.option_strings for action in mr_job.arg_parser._actions))", "test": "tests/test_job.py::CommandLineArgsTestCase::test_arg_parser_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'add_passthru_arg' function correctly adds the destination of the argument to the '_passthru_arg_dests' set.", "unit_test": "def test_passthru_arg_dests_update(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--test-arg', dest='test_arg', type=str)\n    self.assertIn('test_arg', mr_job._passthru_arg_dests)", "test": "tests/test_job.py::CommandLineArgsTestCase::test_passthru_arg_dests_update"}}}
{"namespace": "mingus.containers.note.Note.to_hertz", "type": "method", "project_path": "Multimedia/mingus", "completion_path": "Multimedia/mingus/mingus/containers/note.py", "signature_position": [226, 226], "body_position": [233, 234], "dependency": {"intra_class": ["mingus.containers.note.Note.__int__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function converts a given Note instance to Hertz (frequency in cycles per second).\n", "Arguments": ":param self: Note. An instance of the Note class.\n:param standard_pitch: float. The pitch of A-4, from which the rest of the notes are calculated. It defaults to 440 if not specified.\n:return: float. The frequency of the Note in Hertz.\n"}, "tests": ["tests/unit/containers/test_note.py::test_Note::test_to_hertz"], "indent": 4, "domain": "Multimedia", "code": "    def to_hertz(self, standard_pitch=440):\n        \"\"\"Return the Note in Hz.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        # int(Note(\"A\")) == 57\n        diff = self.__int__() - 57\n        return 2 ** (diff / 12.0) * standard_pitch\n", "context": "class Note(object):\n\n    \"\"\"A note object.\n\n    In the mingus.core module, notes are generally represented by strings.\n    Most of the times, this is not enough. We want to set the octave and\n    maybe the amplitude, vibrato or other dynamics. Then we want to store\n    the notes in bars, the bars in tracks, the tracks in compositions, etc.\n\n    We could do this with a number of lists, but ultimately it is a lot\n    easier to use objects. The Note class provides an easy way to deal with\n    notes in an object oriented matter.\n\n    You can use the class NoteContainer to group Notes together in intervals\n    and chords.\n    \"\"\"\n\n    name = _DEFAULT_NAME\n    octave = _DEFAULT_OCTAVE\n    channel = _DEFAULT_CHANNEL\n    velocity = _DEFAULT_VELOCITY\n\n    def __init__(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            dynamics[\"velocity\"] = velocity\n        if channel is not None:\n            dynamics[\"channel\"] = channel\n\n        if isinstance(name, six.string_types):\n            self.set_note(name, octave, dynamics)\n        elif hasattr(name, \"name\"):\n            # Hardcopy Note object\n            self.set_note(name.name, name.octave, name.dynamics)\n        elif isinstance(name, int):\n            self.from_int(name)\n        else:\n            raise NoteFormatError(\"Don't know what to do with name object: %r\" % name)\n\n    @property\n    def dynamics(self):\n        \"\"\"\n        .. deprecated:: Provided only for compatibility with existing code.\n        \"\"\"\n        return {\n            \"channel\": self.channel,\n            \"velocity\": self.velocity,\n        }\n\n    def set_channel(self, channel):\n        if not 0 <= channel < 16:\n            raise ValueError(\"MIDI channel must be 0-15\")\n        self.channel = channel\n\n    def set_velocity(self, velocity):\n        if not 0 <= velocity < 128:\n            raise ValueError(\"MIDI velocity must be 0-127\")\n        self.velocity = velocity\n\n    def set_note(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"Set the note to name in octave with dynamics.\n\n        Return the objects if it succeeded, raise an NoteFormatError\n        otherwise.\n\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        :return:\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            self.set_velocity(velocity)\n        elif \"velocity\" in dynamics:\n            self.set_velocity(dynamics[\"velocity\"])\n\n        if channel is not None:\n            self.set_channel(channel)\n        if \"channel\" in dynamics:\n            self.set_channel(dynamics[\"channel\"])\n\n        dash_index = name.split(\"-\")\n        if len(dash_index) == 1:\n            if notes.is_valid_note(name):\n                self.name = name\n                self.octave = octave\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        elif len(dash_index) == 2:\n            note, octave = dash_index\n            if notes.is_valid_note(note):\n                self.name = note\n                self.octave = int(octave)\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        else:\n            raise NoteFormatError(\"Invalid note representation: %r\" % name)\n\n    def empty(self):\n        \"\"\"Remove the data in the instance.\"\"\"\n        # TODO: Review these two. This seems to leave the object in an invalid state\n        self.name = \"\"\n        self.octave = 0\n\n        self.channel = _DEFAULT_CHANNEL\n        self.velocity = _DEFAULT_VELOCITY\n\n    def augment(self):\n        \"\"\"Call notes.augment with this note as argument.\"\"\"\n        self.name = notes.augment(self.name)\n\n    def diminish(self):\n        \"\"\"Call notes.diminish with this note as argument.\"\"\"\n        self.name = notes.diminish(self.name)\n\n    def change_octave(self, diff):\n        \"\"\"Change the octave of the note to the current octave + diff.\"\"\"\n        self.octave += diff\n        if self.octave < 0:\n            self.octave = 0\n\n    def octave_up(self):\n        \"\"\"Increment the current octave with 1.\"\"\"\n        self.change_octave(1)\n\n    def octave_down(self):\n        \"\"\"Decrement the current octave with 1.\"\"\"\n        self.change_octave(-1)\n\n    def remove_redundant_accidentals(self):\n        \"\"\"Call notes.remove_redundant_accidentals on this note's name.\"\"\"\n        self.name = notes.remove_redundant_accidentals(self.name)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose the note up or down the interval.\n\n        Examples:\n        >>> a = Note('A')\n        >>> a.transpose('3')\n        >>> a\n        'C#-5'\n        >>> a.transpose('3', False)\n        >>> a\n        'A-4'\n        \"\"\"\n        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1\n\n    def from_int(self, integer):\n        \"\"\"Set the Note corresponding to the integer.\n\n        0 is a C on octave 0, 12 is a C on octave 1, etc.\n\n        Example:\n        >>> Note().from_int(12)\n        'C-1'\n        \"\"\"\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self\n\n    def measure(self, other):\n        \"\"\"Return the number of semitones between this Note and the other.\n\n        Examples:\n        >>> Note('C').measure(Note('D'))\n        2\n        >>> Note('D').measure(Note('C'))\n        -2\n        \"\"\"\n        return int(other) - int(self)\n\n###The function: to_hertz###\n    def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self\n\n    def to_shorthand(self):\n        \"\"\"Give the traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note('C-4').to_shorthand()\n        \"c'\"\n        >>> Note('C-3').to_shorthand()\n        'c'\n        >>> Note('C-2').to_shorthand()\n        'C'\n        >>> Note('C-1').to_shorthand()\n        'C,'\n        \"\"\"\n        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res\n\n    def from_shorthand(self, shorthand):\n        \"\"\"Convert from traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note().from_shorthand(\"C,,\")\n        'C-0'\n        >>> Note().from_shorthand(\"C\")\n        'C-2'\n        >>> Note().from_shorthand(\"c'\")\n        'C-4'\n        \"\"\"\n        name = \"\"\n        octave = 0\n        for x in shorthand:\n            if x in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n                name = str.upper(x)\n                octave = 3\n            elif x in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                name = x\n                octave = 2\n            elif x in [\"#\", \"b\"]:\n                name += x\n            elif x == \",\":\n                octave -= 1\n            elif x == \"'\":\n                octave += 1\n        return self.set_note(name, octave, {})\n\n    def __int__(self):\n        \"\"\"Return the current octave multiplied by twelve and add\n        notes.note_to_int to it.\n        \n        This means a C-0 returns 0, C-1 returns 12, etc. This method allows\n        you to use int() on Notes.\n        \"\"\"\n        res = self.octave * 12 + notes.note_to_int(self.name[0])\n        for n in self.name[1:]:\n            if n == \"#\":\n                res += 1\n            elif n == \"b\":\n                res -= 1\n        return res\n\n    def __lt__(self, other):\n        \"\"\"Enable the comparing operators on Notes (>, <, \\ ==, !=, >= and <=).\n\n        So we can sort() Intervals, etc.\n\n        Examples:\n        >>> Note('C', 4) < Note('B', 4)\n        True\n        >>> Note('C', 4) > Note('B', 4)\n        False\n        \"\"\"\n        if other is None:\n            return False\n        return int(self) < int(other)\n\n    def __eq__(self, other):\n        \"\"\"Compare Notes for equality by comparing their note values.\"\"\"\n        if other is None:\n            return False\n        return int(self) == int(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        return not (self < other or self == other)\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __ge__(self, other):\n        return not self < other\n\n    def __repr__(self):\n        \"\"\"Return a helpful representation for printing Note classes.\"\"\"\n        return \"'%s-%d'\" % (self.name, self.octave)\n", "prompt": "Please write a python function called 'to_hertz' base the context. This function converts a given Note instance to Hertz (frequency in cycles per second).\n:param self: Note. An instance of the Note class.\n:param standard_pitch: float. The pitch of A-4, from which the rest of the notes are calculated. It defaults to 440 if not specified.\n:return: float. The frequency of the Note in Hertz.\n.\n        The context you need to refer to is as follows: class Note(object):\n\n    \"\"\"A note object.\n\n    In the mingus.core module, notes are generally represented by strings.\n    Most of the times, this is not enough. We want to set the octave and\n    maybe the amplitude, vibrato or other dynamics. Then we want to store\n    the notes in bars, the bars in tracks, the tracks in compositions, etc.\n\n    We could do this with a number of lists, but ultimately it is a lot\n    easier to use objects. The Note class provides an easy way to deal with\n    notes in an object oriented matter.\n\n    You can use the class NoteContainer to group Notes together in intervals\n    and chords.\n    \"\"\"\n\n    name = _DEFAULT_NAME\n    octave = _DEFAULT_OCTAVE\n    channel = _DEFAULT_CHANNEL\n    velocity = _DEFAULT_VELOCITY\n\n    def __init__(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            dynamics[\"velocity\"] = velocity\n        if channel is not None:\n            dynamics[\"channel\"] = channel\n\n        if isinstance(name, six.string_types):\n            self.set_note(name, octave, dynamics)\n        elif hasattr(name, \"name\"):\n            # Hardcopy Note object\n            self.set_note(name.name, name.octave, name.dynamics)\n        elif isinstance(name, int):\n            self.from_int(name)\n        else:\n            raise NoteFormatError(\"Don't know what to do with name object: %r\" % name)\n\n    @property\n    def dynamics(self):\n        \"\"\"\n        .. deprecated:: Provided only for compatibility with existing code.\n        \"\"\"\n        return {\n            \"channel\": self.channel,\n            \"velocity\": self.velocity,\n        }\n\n    def set_channel(self, channel):\n        if not 0 <= channel < 16:\n            raise ValueError(\"MIDI channel must be 0-15\")\n        self.channel = channel\n\n    def set_velocity(self, velocity):\n        if not 0 <= velocity < 128:\n            raise ValueError(\"MIDI velocity must be 0-127\")\n        self.velocity = velocity\n\n    def set_note(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"Set the note to name in octave with dynamics.\n\n        Return the objects if it succeeded, raise an NoteFormatError\n        otherwise.\n\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        :return:\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            self.set_velocity(velocity)\n        elif \"velocity\" in dynamics:\n            self.set_velocity(dynamics[\"velocity\"])\n\n        if channel is not None:\n            self.set_channel(channel)\n        if \"channel\" in dynamics:\n            self.set_channel(dynamics[\"channel\"])\n\n        dash_index = name.split(\"-\")\n        if len(dash_index) == 1:\n            if notes.is_valid_note(name):\n                self.name = name\n                self.octave = octave\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        elif len(dash_index) == 2:\n            note, octave = dash_index\n            if notes.is_valid_note(note):\n                self.name = note\n                self.octave = int(octave)\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        else:\n            raise NoteFormatError(\"Invalid note representation: %r\" % name)\n\n    def empty(self):\n        \"\"\"Remove the data in the instance.\"\"\"\n        # TODO: Review these two. This seems to leave the object in an invalid state\n        self.name = \"\"\n        self.octave = 0\n\n        self.channel = _DEFAULT_CHANNEL\n        self.velocity = _DEFAULT_VELOCITY\n\n    def augment(self):\n        \"\"\"Call notes.augment with this note as argument.\"\"\"\n        self.name = notes.augment(self.name)\n\n    def diminish(self):\n        \"\"\"Call notes.diminish with this note as argument.\"\"\"\n        self.name = notes.diminish(self.name)\n\n    def change_octave(self, diff):\n        \"\"\"Change the octave of the note to the current octave + diff.\"\"\"\n        self.octave += diff\n        if self.octave < 0:\n            self.octave = 0\n\n    def octave_up(self):\n        \"\"\"Increment the current octave with 1.\"\"\"\n        self.change_octave(1)\n\n    def octave_down(self):\n        \"\"\"Decrement the current octave with 1.\"\"\"\n        self.change_octave(-1)\n\n    def remove_redundant_accidentals(self):\n        \"\"\"Call notes.remove_redundant_accidentals on this note's name.\"\"\"\n        self.name = notes.remove_redundant_accidentals(self.name)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose the note up or down the interval.\n\n        Examples:\n        >>> a = Note('A')\n        >>> a.transpose('3')\n        >>> a\n        'C#-5'\n        >>> a.transpose('3', False)\n        >>> a\n        'A-4'\n        \"\"\"\n        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1\n\n    def from_int(self, integer):\n        \"\"\"Set the Note corresponding to the integer.\n\n        0 is a C on octave 0, 12 is a C on octave 1, etc.\n\n        Example:\n        >>> Note().from_int(12)\n        'C-1'\n        \"\"\"\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self\n\n    def measure(self, other):\n        \"\"\"Return the number of semitones between this Note and the other.\n\n        Examples:\n        >>> Note('C').measure(Note('D'))\n        2\n        >>> Note('D').measure(Note('C'))\n        -2\n        \"\"\"\n        return int(other) - int(self)\n\n###The function: to_hertz###\n    def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self\n\n    def to_shorthand(self):\n        \"\"\"Give the traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note('C-4').to_shorthand()\n        \"c'\"\n        >>> Note('C-3').to_shorthand()\n        'c'\n        >>> Note('C-2').to_shorthand()\n        'C'\n        >>> Note('C-1').to_shorthand()\n        'C,'\n        \"\"\"\n        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res\n\n    def from_shorthand(self, shorthand):\n        \"\"\"Convert from traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note().from_shorthand(\"C,,\")\n        'C-0'\n        >>> Note().from_shorthand(\"C\")\n        'C-2'\n        >>> Note().from_shorthand(\"c'\")\n        'C-4'\n        \"\"\"\n        name = \"\"\n        octave = 0\n        for x in shorthand:\n            if x in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n                name = str.upper(x)\n                octave = 3\n            elif x in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                name = x\n                octave = 2\n            elif x in [\"#\", \"b\"]:\n                name += x\n            elif x == \",\":\n                octave -= 1\n            elif x == \"'\":\n                octave += 1\n        return self.set_note(name, octave, {})\n\n    def __int__(self):\n        \"\"\"Return the current octave multiplied by twelve and add\n        notes.note_to_int to it.\n        \n        This means a C-0 returns 0, C-1 returns 12, etc. This method allows\n        you to use int() on Notes.\n        \"\"\"\n        res = self.octave * 12 + notes.note_to_int(self.name[0])\n        for n in self.name[1:]:\n            if n == \"#\":\n                res += 1\n            elif n == \"b\":\n                res -= 1\n        return res\n\n    def __lt__(self, other):\n        \"\"\"Enable the comparing operators on Notes (>, <, \\ ==, !=, >= and <=).\n\n        So we can sort() Intervals, etc.\n\n        Examples:\n        >>> Note('C', 4) < Note('B', 4)\n        True\n        >>> Note('C', 4) > Note('B', 4)\n        False\n        \"\"\"\n        if other is None:\n            return False\n        return int(self) < int(other)\n\n    def __eq__(self, other):\n        \"\"\"Compare Notes for equality by comparing their note values.\"\"\"\n        if other is None:\n            return False\n        return int(self) == int(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        return not (self < other or self == other)\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __ge__(self, other):\n        return not self < other\n\n    def __repr__(self):\n        \"\"\"Return a helpful representation for printing Note classes.\"\"\"\n        return \"'%s-%d'\" % (self.name, self.octave)\n", "test_list": ["def test_to_hertz(self):\n    self.assertEqual(Note('A', 0).to_hertz(), 27.5)\n    self.assertEqual(Note('A', 1).to_hertz(), 55)\n    self.assertEqual(Note('A', 2).to_hertz(), 110)\n    self.assertEqual(Note('A', 3).to_hertz(), 220)\n    self.assertEqual(Note('A', 4).to_hertz(), 440)\n    self.assertEqual(Note('A', 5).to_hertz(), 880)\n    self.assertEqual(Note('A', 6).to_hertz(), 1760)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_hertz' function should return a float value representing the frequency in Hertz for a valid Note instance. The input Note instance should have valid 'name' and 'octave' attributes.", "unit_test": "def test_to_hertz_input_output(self):\n    note = Note('C', 4)\n    result = note.to_hertz()\n    self.assertIsInstance(result, float)\n    self.assertGreater(result, 0)", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_input_output"}, "Exception Handling": {"requirement": "The 'to_hertz' function should raise a ValueError if the Note instance has an invalid note name.", "unit_test": "def test_to_hertz_invalid_note_name(self):\n    with self.assertRaises(ValueError):\n        Note('H', 4).to_hertz()", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_invalid_note_name"}, "Edge Case Handling": {"requirement": "The 'to_hertz' function should correctly handle edge cases such as the lowest and highest possible octaves for a note.", "unit_test": "def test_to_hertz_edge_cases(self):\n    self.assertEqual(Note('C', 0).to_hertz(), 16.35)\n    self.assertEqual(Note('B', 8).to_hertz(), 7902.13)", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'to_hertz' function to accept an optional 'standard_pitch' parameter to allow conversion based on different tuning standards.", "unit_test": "def test_to_hertz_with_standard_pitch(self):\n    self.assertAlmostEqual(Note('A', 4).to_hertz(standard_pitch=432), 432.0, places=1)", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_with_standard_pitch"}, "Annotation Coverage": {"requirement": "Ensure that the 'to_hertz' function has complete docstring coverage, including parameter types and return type.", "unit_test": "def test_to_hertz_docstring(self):\n    self.assertIn(':param self: Note', Note.to_hertz.__doc__)\n    self.assertIn(':param standard_pitch: float', Note.to_hertz.__doc__)\n    self.assertIn(':return: float', Note.to_hertz.__doc__)", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_docstring"}, "Code Standard": {"requirement": "The 'to_hertz' function should adhere to PEP 8 standards, including proper naming conventions and spacing.", "unit_test": "def test_to_hertz_pep8(self):\n    import pep8\n    style = pep8.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/note.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/unit/containers/test_note.py::test_Note::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'to_hertz' function should utilize the '__int__' method of the Note class to convert the note to an integer representation for frequency calculation.", "unit_test": "def test_to_hertz_context_usage(self):\n    note = Note()\n    with unittest.mock.patch.object(Note, '__int__', return_value=57) as mock_int:\n        note.to_hertz()\n        mock_int.assert_called_once()", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'to_hertz' function should correctly use the '__int__' method to determine the semitone distance from A4 for frequency calculation.", "unit_test": "def test_to_hertz_context_correctness(self):\n    note = Note()\n    with unittest.mock.patch.object(Note, '__int__', return_value=57):\n        frequency = note.to_hertz()\n        self.assertAlmostEqual(frequency, 440 * 2**((57 - 69) / 12), places=2)", "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_context_correctness"}}}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "type": "method", "project_path": "Communications/Wikipedia-API", "completion_path": "Communications/Wikipedia-API/wikipediaapi/__init__.py", "signature_position": [1068, 1068], "body_position": [1069, 1071], "dependency": {"intra_class": ["wikipediaapi.WikipediaPage._called", "wikipediaapi.WikipediaPage.pageid", "wikipediaapi.WikipediaPage.title"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a string representation of a WikipediaPage object. It checks if any recorded methods have been called, and if so, it includes the title, pageid, and ns in the string: \"{title} (id: {page id}, ns: {ns})\". Otherwise, it includes only the title and ns attributes in the string: \"{title} (id: ??, ns: {ns})\"", "Arguments": ":param self: WikipediaPage. An instance of the WikipediaPage class.\n:return: String. The string representation of the WikipediaPage object."}, "tests": ["tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_after_fetching", "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_before_fetching"], "indent": 4, "domain": "Communications", "code": "    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "context": "class WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n###The function: __repr__###", "prompt": "Please write a python function called '__repr__' base the context. This function returns a string representation of a WikipediaPage object. It checks if any recorded methods have been called, and if so, it includes the title, pageid, and ns in the string: \"{title} (id: {page id}, ns: {ns})\". Otherwise, it includes only the title and ns attributes in the string: \"{title} (id: ??, ns: {ns})\":param self: WikipediaPage. An instance of the WikipediaPage class.\n:return: String. The string representation of the WikipediaPage object..\n        The context you need to refer to is as follows: class WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n###The function: __repr__###", "test_list": ["def test_repr_after_fetching(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')\n    self.assertEqual(page.pageid, 4)\n    self.assertEqual(repr(page), 'Test 1 (id: 4, ns: 0)')", "def test_repr_before_fetching(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')"], "requirements": {"Input-Output Conditions": {"requirement": "The __repr__ method should return a string with the title, pageid, and ns attributes formatted correctly. If pageid is not available, it should be represented as '??'.", "unit_test": "def test_repr_output_format(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')\n    page.pageid = 4\n    self.assertEqual(repr(page), 'Test_1 (id: 4, ns: 0)')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_output_format"}, "Exception Handling": {"requirement": "The __repr__ method should handle exceptions gracefully if any attribute is missing or invalid, and it should return a default string representation.", "unit_test": "def test_repr_exception_handling(self):\n    page = self.wiki.page('Test_1')\n    del page._attributes['title']\n    try:\n        repr(page)\n    except Exception as e:\n        self.fail(f'__repr__ raised an exception {e}')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_exception_handling"}, "Edge Case Handling": {"requirement": "The __repr__ method should handle edge cases where the title or ns is an empty string or None, and it should still return a valid string representation.", "unit_test": "def test_repr_edge_cases(self):\n    page = self.wiki.page('')\n    page._attributes['ns'] = None\n    self.assertEqual(repr(page), ' (id: ??, ns: None)')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_edge_cases"}, "Functionality Extension": {"requirement": "Extend the __repr__ method to include the language attribute in the string representation if it is different from the default 'en'.", "unit_test": "def test_repr_with_language(self):\n    page = self.wiki.page('Test_1')\n    page._attributes['language'] = 'fr'\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0, lang: fr)')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_with_language"}, "Annotation Coverage": {"requirement": "Ensure that the __repr__ method has complete annotation coverage, including parameter types and return types.", "unit_test": "def test_repr_annotation_coverage(self):\n    annotations = WikipediaPage.__repr__.__annotations__\n    self.assertIn('return', annotations)\n    self.assertEqual(annotations['return'], str)", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_annotation_coverage"}, "Code Standard": {"requirement": "The __repr__ method should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_repr_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['wikipediaapi.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the __repr__ method uses the necessary context attributes such as title, pageid, and ns from the WikipediaPage class.", "unit_test": "def test_repr_context_usage(self):\n    page = self.wiki.page('Test_1')\n    self.assertTrue(hasattr(page, '_attributes'))\n    self.assertIn('title', page._attributes)\n    self.assertIn('ns', page._attributes)", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the __repr__ method correctly uses the context attributes to construct the string representation, particularly checking the _called dictionary for method invocation.", "unit_test": "def test_repr_context_correctness(self):\n    page = self.wiki.page('Test_1')\n    page._called['info'] = True\n    page.pageid = 4\n    self.assertEqual(repr(page), 'Test_1 (id: 4, ns: 0)')", "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_context_correctness"}}}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "type": "method", "project_path": "Security/msticpy", "completion_path": "Security/msticpy/msticpy/config/query_editor.py", "signature_position": [299, 299], "body_position": [301, 305], "dependency": {"intra_class": ["msticpy.config.query_editor.QueryParameterEditWidget._blank_parameter", "msticpy.config.query_editor.QueryParameterEditWidget._changed_data", "msticpy.config.query_editor.QueryParameterEditWidget.param_container", "msticpy.config.query_editor.QueryParameterEditWidget.parameter_dropdown"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function deletes a parameter item from the QueryParameterEditWidget instance. It removes the parameter from the parameters dictionary and clears the input widgets. It also sets the changed data flag to True.", "Arguments": ":param self: QueryParameterEditWidget. An instance of the QueryParameterEditWidget class.\n:param button: The button that triggered the delete action. It is not used in the function.\n:return: No return values."}, "tests": ["tests/config/test_query_editor.py::test_remove_parameter"], "indent": 4, "domain": "Security", "code": "    def delete_parameter(self, button):\n        \"\"\"Delete parameter item.\"\"\"\n        del button\n        del self.param_container.parameters[self.parameter_dropdown.value]\n        # Clear the input widgets\n        self._blank_parameter()\n        self._changed_data = True\n", "context": "class QueryParameterEditWidget(IPyDisplayMixin):\n    \"\"\"Class to manage editing of query parameters.\n\n    This class provides a graphical user interface for editing query parameters.\n    It allows users to add, modify, and delete parameters, as well as specify\n    their names, descriptions, types, default values, and whether they are mandatory.\n\n    Attributes\n    ----------\n    _changed_data : bool\n        A flag indicating whether the data has been changed.\n    param_container : Union[Query, QueryDefaults]\n        The container for the query parameters.\n    parameter_dropdown : ipywidgets.Select\n        A widget for selecting a parameter to edit.\n    parameter_name_widget : ipywidgets.Text\n        A widget for editing the name of a parameter.\n    description_widget : ipywidgets.Text\n        A widget for editing the description of a parameter.\n    type_widget : ipywidgets.Dropdown\n        A widget for selecting the type of a parameter.\n    default_reqd_widget : ipywidgets.Checkbox\n        A widget for indicating whether a default value is required for a parameter.\n\n    \"\"\"\n\n    def __init__(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Initialize the class.\"\"\"\n        self._changed_data = False\n        self.param_container = container\n        self.parameter_dropdown = widgets.Select(\n            description=\"Parameters\",\n            size=5,\n            options=list(\n                self.param_container.parameters.keys()\n                if self.param_container.parameters\n                else []\n            ),\n            **sel_fmt(height=\"100px\"),\n        )\n        # Create widgets for the Parameter fields\n        self.parameter_name_widget = widgets.Text(description=\"Name\", **txt_fmt())\n        self.description_widget = widgets.Text(description=\"Description\", **txt_fmt())\n        self.type_widget = widgets.Dropdown(\n            description=\"Type\", options=_PARAM_OPTIONS, **sel_fmt(height=\"30px\")\n        )\n        self.default_reqd_widget = widgets.Checkbox(description=\"Use a default value\")\n        self.default_widget = widgets.Text(description=\"Default Value\", **txt_fmt())\n\n        # Create buttons\n        self.add_parameter_button = widgets.Button(description=\"New Parameter\")\n        self.save_parameter_button = widgets.Button(description=\"Save Parameter\")\n        self.delete_parameter_button = widgets.Button(description=\"Delete Parameter\")\n\n        # Attach the functions to buttons\n        self.add_parameter_button.on_click(self.add_parameter)\n        self.save_parameter_button.on_click(self.save_parameter)\n        self.delete_parameter_button.on_click(self.delete_parameter)\n        self.parameter_dropdown.observe(self.populate_widgets, names=\"value\")\n\n        # Create a widget for adding, editing, and deleting Parameters\n        self.layout = widgets.VBox(\n            [\n                widgets.HBox(\n                    [\n                        self.parameter_dropdown,\n                        widgets.VBox(\n                            [\n                                self.add_parameter_button,\n                                self.delete_parameter_button,\n                            ]\n                        ),\n                    ]\n                ),\n                widgets.VBox(\n                    children=[\n                        self.parameter_name_widget,\n                        self.description_widget,\n                        self.type_widget,\n                        widgets.HBox([self.default_reqd_widget, self.default_widget]),\n                        self.save_parameter_button,\n                    ],\n                    **box_layout(),\n                ),\n            ]\n        )\n        if self.param_container and self.param_container.parameters:\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n\n    @property\n    def changed_data(self):\n        \"\"\"Return True if data has changed.\"\"\"\n        return self._changed_data\n\n    def reset_changed_data(self):\n        \"\"\"Reset changed data flag.\"\"\"\n        self._changed_data = False\n\n    def set_param_container(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Set the parameter container.\"\"\"\n        self.param_container = container\n        if self.param_container and self.param_container.parameters:\n            self.parameter_dropdown.options = list(\n                self.param_container.parameters.keys()\n            )\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n        else:\n            self.parameter_dropdown.options = []\n            self._blank_parameter()\n\n    # Define a function to add a new Parameter to the selected Query\n    def add_parameter(self, button):\n        \"\"\"Add a new parameter.\"\"\"\n        del button\n        # Clear the input widgets\n        self._blank_parameter()\n        self.parameter_name_widget.value = \"new_parameter\"\n\n    def _blank_parameter(self):\n        \"\"\"Clear the parameter widgets.\"\"\"\n        self.parameter_name_widget.value = \"\"\n        self.description_widget.value = \"\"\n        self.type_widget.value = _PARAM_OPTIONS[0]\n        self.default_widget.value = \"\"\n        self.default_reqd_widget.value = False\n\n    # Define a function to populate the Parameter widgets with the values of the selected Parameter\n    def populate_widgets(self, change):\n        \"\"\"Populate parameter value in widgets.\"\"\"\n        parameter = self.param_container.parameters[change.new]\n        self.parameter_name_widget.value = change.new\n        self.description_widget.value = parameter.description\n        self.type_widget.value = parameter.datatype\n        self.default_reqd_widget.value = parameter.default is not None\n        self.default_widget.value = parameter.default or \"\"\n\n    # Define a function to edit the selected Parameter with the values from the widgets\n    def save_parameter(self, button):\n        \"\"\"Save currently edited parameter.\"\"\"\n        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        parameter = QueryParameter(\n            description=self.description_widget.value,\n            datatype=self.type_widget.value or _PARAM_OPTIONS[0],\n        )\n        parameter.default = (\n            self.default_widget.value if self.default_reqd_widget.value else None\n        )\n        self.param_container.parameters[param_name] = parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n\n    # Define a function to delete the selected Parameter from the selected Query\n###The function: delete_parameter###", "prompt": "Please write a python function called 'delete_parameter' base the context. This function deletes a parameter item from the QueryParameterEditWidget instance. It removes the parameter from the parameters dictionary and clears the input widgets. It also sets the changed data flag to True.:param self: QueryParameterEditWidget. An instance of the QueryParameterEditWidget class.\n:param button: The button that triggered the delete action. It is not used in the function.\n:return: No return values..\n        The context you need to refer to is as follows: class QueryParameterEditWidget(IPyDisplayMixin):\n    \"\"\"Class to manage editing of query parameters.\n\n    This class provides a graphical user interface for editing query parameters.\n    It allows users to add, modify, and delete parameters, as well as specify\n    their names, descriptions, types, default values, and whether they are mandatory.\n\n    Attributes\n    ----------\n    _changed_data : bool\n        A flag indicating whether the data has been changed.\n    param_container : Union[Query, QueryDefaults]\n        The container for the query parameters.\n    parameter_dropdown : ipywidgets.Select\n        A widget for selecting a parameter to edit.\n    parameter_name_widget : ipywidgets.Text\n        A widget for editing the name of a parameter.\n    description_widget : ipywidgets.Text\n        A widget for editing the description of a parameter.\n    type_widget : ipywidgets.Dropdown\n        A widget for selecting the type of a parameter.\n    default_reqd_widget : ipywidgets.Checkbox\n        A widget for indicating whether a default value is required for a parameter.\n\n    \"\"\"\n\n    def __init__(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Initialize the class.\"\"\"\n        self._changed_data = False\n        self.param_container = container\n        self.parameter_dropdown = widgets.Select(\n            description=\"Parameters\",\n            size=5,\n            options=list(\n                self.param_container.parameters.keys()\n                if self.param_container.parameters\n                else []\n            ),\n            **sel_fmt(height=\"100px\"),\n        )\n        # Create widgets for the Parameter fields\n        self.parameter_name_widget = widgets.Text(description=\"Name\", **txt_fmt())\n        self.description_widget = widgets.Text(description=\"Description\", **txt_fmt())\n        self.type_widget = widgets.Dropdown(\n            description=\"Type\", options=_PARAM_OPTIONS, **sel_fmt(height=\"30px\")\n        )\n        self.default_reqd_widget = widgets.Checkbox(description=\"Use a default value\")\n        self.default_widget = widgets.Text(description=\"Default Value\", **txt_fmt())\n\n        # Create buttons\n        self.add_parameter_button = widgets.Button(description=\"New Parameter\")\n        self.save_parameter_button = widgets.Button(description=\"Save Parameter\")\n        self.delete_parameter_button = widgets.Button(description=\"Delete Parameter\")\n\n        # Attach the functions to buttons\n        self.add_parameter_button.on_click(self.add_parameter)\n        self.save_parameter_button.on_click(self.save_parameter)\n        self.delete_parameter_button.on_click(self.delete_parameter)\n        self.parameter_dropdown.observe(self.populate_widgets, names=\"value\")\n\n        # Create a widget for adding, editing, and deleting Parameters\n        self.layout = widgets.VBox(\n            [\n                widgets.HBox(\n                    [\n                        self.parameter_dropdown,\n                        widgets.VBox(\n                            [\n                                self.add_parameter_button,\n                                self.delete_parameter_button,\n                            ]\n                        ),\n                    ]\n                ),\n                widgets.VBox(\n                    children=[\n                        self.parameter_name_widget,\n                        self.description_widget,\n                        self.type_widget,\n                        widgets.HBox([self.default_reqd_widget, self.default_widget]),\n                        self.save_parameter_button,\n                    ],\n                    **box_layout(),\n                ),\n            ]\n        )\n        if self.param_container and self.param_container.parameters:\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n\n    @property\n    def changed_data(self):\n        \"\"\"Return True if data has changed.\"\"\"\n        return self._changed_data\n\n    def reset_changed_data(self):\n        \"\"\"Reset changed data flag.\"\"\"\n        self._changed_data = False\n\n    def set_param_container(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Set the parameter container.\"\"\"\n        self.param_container = container\n        if self.param_container and self.param_container.parameters:\n            self.parameter_dropdown.options = list(\n                self.param_container.parameters.keys()\n            )\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n        else:\n            self.parameter_dropdown.options = []\n            self._blank_parameter()\n\n    # Define a function to add a new Parameter to the selected Query\n    def add_parameter(self, button):\n        \"\"\"Add a new parameter.\"\"\"\n        del button\n        # Clear the input widgets\n        self._blank_parameter()\n        self.parameter_name_widget.value = \"new_parameter\"\n\n    def _blank_parameter(self):\n        \"\"\"Clear the parameter widgets.\"\"\"\n        self.parameter_name_widget.value = \"\"\n        self.description_widget.value = \"\"\n        self.type_widget.value = _PARAM_OPTIONS[0]\n        self.default_widget.value = \"\"\n        self.default_reqd_widget.value = False\n\n    # Define a function to populate the Parameter widgets with the values of the selected Parameter\n    def populate_widgets(self, change):\n        \"\"\"Populate parameter value in widgets.\"\"\"\n        parameter = self.param_container.parameters[change.new]\n        self.parameter_name_widget.value = change.new\n        self.description_widget.value = parameter.description\n        self.type_widget.value = parameter.datatype\n        self.default_reqd_widget.value = parameter.default is not None\n        self.default_widget.value = parameter.default or \"\"\n\n    # Define a function to edit the selected Parameter with the values from the widgets\n    def save_parameter(self, button):\n        \"\"\"Save currently edited parameter.\"\"\"\n        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        parameter = QueryParameter(\n            description=self.description_widget.value,\n            datatype=self.type_widget.value or _PARAM_OPTIONS[0],\n        )\n        parameter.default = (\n            self.default_widget.value if self.default_reqd_widget.value else None\n        )\n        self.param_container.parameters[param_name] = parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n\n    # Define a function to delete the selected Parameter from the selected Query\n###The function: delete_parameter###", "test_list": ["def test_remove_parameter(query):\n    \"\"\"Test removing a parameter.\"\"\"\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert len(editor.param_container.parameters) == 1\n    editor.delete_parameter(None)\n    assert len(editor.param_container.parameters) == 0"], "requirements": {"Input-Output Conditions": {"requirement": "The delete_parameter function should remove the valid parameters. If parameters are not valid, please raise TypeError.", "unit_test": "def test_delete_parameter_updates_dropdown(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert 'new_parameter' in editor.parameter_dropdown.options\n    editor.delete_parameter(None)\n    assert 'new_parameter' not in editor.parameter_dropdown.options", "test": "tests/config/test_query_editor.py::test_remove_parameter"}, "Exception Handling": {"requirement": "The delete_parameter function should handle cases where no parameter is selected gracefully, without raising an exception.", "unit_test": "def test_delete_parameter_no_selection(query):\n    editor = QueryParameterEditWidget(query)\n    try:\n        editor.delete_parameter(None)\n        assert True  # No exception should be raised\n    except Exception:\n        assert False, 'Exception was raised when no parameter was selected'", "test": "tests/config/test_query_editor.py::test_delete_parameter_no_selection"}, "Edge Case Handling": {"requirement": "The delete_parameter function should handle the case where the parameters dictionary is empty without errors.", "unit_test": "def test_delete_parameter_empty_dict(query):\n    editor = QueryParameterEditWidget(query)\n    editor.param_container.parameters.clear()\n    try:\n        editor.delete_parameter(None)\n        assert True  # No exception should be raised\n    except Exception:\n        assert False, 'Exception was raised with empty parameters dictionary'", "test": "tests/config/test_query_editor.py::test_delete_parameter_empty_dict"}, "Functionality Extension": {"requirement": "Extend the delete_parameter function to log a message:'Deleted parameter: new_parameter' in caplog.text' indicating which parameter was deleted.", "unit_test": "def test_delete_parameter_logs_message(query, caplog):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    with caplog.at_level(logging.INFO):\n        editor.delete_parameter(None)\n    assert 'Deleted parameter: new_parameter' in caplog.text", "test": "tests/config/test_query_editor.py::test_delete_parameter_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the delete_parameter function includes type annotations for its parameters.", "unit_test": "def test_delete_parameter_annotations():\n    import inspect\n    sig = inspect.signature(QueryParameterEditWidget.delete_parameter)\n    assert sig.parameters['self'].annotation == 'QueryParameterEditWidget'\n    assert sig.parameters['button'].annotation == 'Any'", "test": "tests/config/test_query_editor.py::test_delete_parameter_annotations"}, "Code Standard": {"requirement": "The delete_parameter function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_delete_parameter_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_delete_parameter.py'])\n    assert result.total_errors == 0, 'PEP 8 style errors: {}'.format(result.total_errors)", "test": "tests/config/test_query_editor.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The delete_parameter function should utilize the parameter_dropdown and param_container attributes of the QueryParameterEditWidget class.", "unit_test": "def test_delete_parameter_context_usage(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert 'new_parameter' in editor.param_container.parameters\n    editor.delete_parameter(None)\n    assert 'new_parameter' not in editor.param_container.parameters", "test": "tests/config/test_query_editor.py::test_delete_parameter_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The delete_parameter function should correctly update the _changed_data flag to True after a parameter is deleted.", "unit_test": "def test_delete_parameter_changed_data_flag(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert not editor.changed_data\n    editor.delete_parameter(None)\n    assert editor.changed_data", "test": "tests/config/test_query_editor.py::test_delete_parameter_changed_data_flag"}}}
{"namespace": "dash.development.base_component.Component._traverse", "type": "method", "project_path": "Software-Development/dash", "completion_path": "Software-Development/dash/dash/development/base_component.py", "signature_position": [319, 319], "body_position": [321, 322], "dependency": {"intra_class": ["dash.development.base_component.Component._traverse_with_paths"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function traverses the tree structure of a Component instance and yields the second value in each item in the tree.", "Arguments": ":param self: Component. An instance of the Component class.\n:return: Yields each item in the tree."}, "tests": ["tests/unit/development/test_base_component.py::test_debc011_traverse_with_tuples", "tests/unit/development/test_base_component.py::test_debc010_traverse_full_tree"], "indent": 4, "domain": "Software-Development", "code": "    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n", "context": "class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n###The function: _traverse###\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "prompt": "Please write a python function called '_traverse' base the context. This function traverses the tree structure of a Component instance and yields the second value in each item in the tree.:param self: Component. An instance of the Component class.\n:return: Yields each item in the tree..\n        The context you need to refer to is as follows: class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n###The function: _traverse###\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "test_list": ["def test_debc011_traverse_with_tuples():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = tuple(c2.children)\n    c.children = tuple(c.children)\n    elements = [i for i in c._traverse()]\n    assert elements == list(c.children) + [c3] + [c2] + list(c2.children)", "def test_debc010_traverse_full_tree():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    elements = [i for i in c._traverse()]\n    assert elements == c.children + [c3] + [c2] + c2.children"], "requirements": {"Exception Handling": {"requirement": "The '_traverse' function should raise a TypeError if any item in the tree is not a tuple or does not have at least two elements.", "unit_test": "def test_traverse_raises_type_error():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2,)]  # Second item is not a tuple with two elements\n    c.children = [(4, 'd'), 'not a tuple']\n    try:\n        elements = [i for i in c._traverse()]\n    except TypeError as e:\n        assert str(e) == 'Each item in the tree must be a tuple with at least two elements.'", "test": "tests/unit/development/test_base_component.py::test_traverse_correct_order"}, "Edge Case Handling": {"requirement": "The '_traverse' function should handle an empty tree gracefully by yielding nothing.", "unit_test": "def test_traverse_empty_tree():\n    c = Component()\n    c.children = []\n    elements = [i for i in c._traverse()]\n    assert elements == []", "test": "tests/unit/development/test_base_component.py::test_traverse_empty_tree"}, "Functionality Extension": {"requirement": "Extend the '_traverse' function to accept an optional parameter that specifies which index of the tuple to yield.", "unit_test": "def test_traverse_with_index_parameter():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a', 'x'), (2, 'b', 'y'), (3, 'c', 'z')]\n    c.children = [(4, 'd', 'w'), (5, 'e', 'v')]\n    elements = [i for i in c._traverse(index=2)]\n    assert elements == ['w', 'v', 'x', 'y', 'z']", "test": "tests/unit/development/test_base_component.py::test_traverse_with_index_parameter"}, "Annotation Coverage": {"requirement": "Ensure that the '_traverse' function has complete type annotations for all parameters and return types.", "unit_test": "def test_traverse_annotations():\n    from typing import Generator\n    assert '_traverse' in Component.__dict__\n    assert Component._traverse.__annotations__ == {'return': Generator}", "test": "tests/unit/development/test_base_component.py::test_traverse_annotations"}, "Code Complexity": {"requirement": "The '_traverse' function should have a cyclomatic complexity of no more than 2.", "unit_test": "def test_traverse_cyclomatic_complexity():\n    import radon.complexity as cc\n    complexity = cc.cc_visit(Component._traverse)\n    assert complexity[0].complexity <= 5", "test": "tests/unit/development/test_base_component.py::test_traverse_cyclomatic_complexity"}, "Code Standard": {"requirement": "The '_traverse' function should adhere to PEP 8 standards, including proper indentation and line length.", "unit_test": "def test_traverse_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_component_file.py'])\n    assert result.total_errors == 0", "test": "tests/unit/development/test_base_component.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The '_traverse' function should utilize the '_traverse_with_paths' method from the Component class context.", "unit_test": "def test_traverse_uses_traverse_with_paths():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2, 'b')]\n    c.children = [(3, 'c'), (4, 'd')]\n    paths = [p for p, _ in c._traverse_with_paths()]\n    assert all(isinstance(p, str) for p in paths)", "test": "tests/unit/development/test_base_component.py::test_traverse_uses_traverse_with_paths"}, "Context Usage Correctness Verification": {"requirement": "The '_traverse' function should correctly use the '_traverse_with_paths' method to ensure the correct traversal order.", "unit_test": "def test_traverse_correct_order():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2, 'b')]\n    c.children = [(3, 'c'), (4, 'd')]\n    elements = [i for i in c._traverse()]\n    assert elements == ['c', 'd', 'a', 'b']", "test": "tests/unit/development/test_base_component.py::test_traverse_correct_order"}}}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "type": "method", "project_path": "Security/pycoin", "completion_path": "Security/pycoin/pycoin/blockchain/BlockChain.py", "signature_position": [61, 61], "body_position": [62, 73], "dependency": {"intra_class": ["pycoin.blockchain.BlockChain.BlockChain._locked_chain", "pycoin.blockchain.BlockChain.BlockChain._longest_chain_cache", "pycoin.blockchain.BlockChain.BlockChain._longest_local_block_chain", "pycoin.blockchain.BlockChain.BlockChain.length", "pycoin.blockchain.BlockChain.BlockChain.parent_hash", "pycoin.blockchain.BlockChain.BlockChain.weight_lookup"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a tuple containing information about a block in the blockchain at the given index. It first checks if the index is negative, and if so, it adjusts it to be a positive index relative to the end of the blockchain. Then, it checks if the index is within the range of the locked chain. If it is, it returns the corresponding block from the locked chain. If the index is outside the range of the locked chain, it retrieves the block from the longest local block chain or the longest chain cache, depending on the index value. Finally, it looks up the weight of the block using the weight lookup dictionary and returns a tuple containing the block's hash, parent hash, and weight.", "Arguments": ":param self: BlockChain. An instance of the BlockChain class.\n:param index: Integer. The index of the block to retrieve.\n:return: Tuple. A tuple containing the block's hash, parent hash, and weight."}, "tests": ["tests/blockchain_test.py::BlockchainTestCase::test_large", "tests/blockchain_test.py::BlockchainTestCase::test_chain_locking", "tests/blockchain_test.py::BlockchainTestCase::test_basic"], "indent": 4, "domain": "Security", "code": "    def tuple_for_index(self, index):\n        if index < 0:\n            index = self.length() + index\n        size = len(self._locked_chain)\n        if index < size:\n            return self._locked_chain[index]\n        index -= size\n\n        longest_chain = self._longest_local_block_chain()\n        the_hash = longest_chain[-index-1]\n        parent_hash = self.parent_hash if index <= 0 else self._longest_chain_cache[-index]\n        weight = self.weight_lookup.get(the_hash)\n        return (the_hash, parent_hash, weight)\n", "context": "class BlockChain(object):\n    def __init__(self, parent_hash=ZERO_HASH, unlocked_block_storage={}, did_lock_to_index_f=None):\n        self.parent_hash = parent_hash\n        self.hash_to_index_lookup = {}\n        self.weight_lookup = {}\n        self.chain_finder = ChainFinder()\n        self.change_callbacks = weakref.WeakSet()\n        self._longest_chain_cache = None\n        self.did_lock_to_index_f = did_lock_to_index_f\n        self.unlocked_block_storage = unlocked_block_storage\n\n        self._locked_chain = []\n\n    def preload_locked_blocks(self, headers_iter):\n        self._locked_chain = []\n        the_hash = self.parent_hash\n        for idx, h in enumerate(headers_iter):\n            the_hash = h.hash()\n            self._locked_chain.append((the_hash, h.previous_block_hash, h.difficulty))\n            self.hash_to_index_lookup[the_hash] = idx\n        self.parent_hash = the_hash\n\n    def is_hash_known(self, the_hash):\n        return the_hash in self.hash_to_index_lookup\n\n    def length(self):\n        return len(self._longest_local_block_chain()) + len(self._locked_chain)\n\n    def locked_length(self):\n        return len(self._locked_chain)\n\n    def unlocked_length(self):\n        return len(self._longest_local_block_chain())\n\n###The function: tuple_for_index###\n    def last_block_hash(self):\n        if self.length() == 0:\n            return self.parent_hash\n        return self.hash_for_index(-1)\n\n    def hash_for_index(self, index):\n        return self.tuple_for_index(index)[0]\n\n    def index_for_hash(self, the_hash):\n        return self.hash_to_index_lookup.get(the_hash)\n\n    def add_change_callback(self, callback):\n        self.change_callbacks.add(callback)\n\n    def lock_to_index(self, index):\n        old_length = len(self._locked_chain)\n        index -= old_length\n        longest_chain = self._longest_local_block_chain()\n        if index < 1:\n            return\n        excluded = set()\n        for idx in range(index):\n            the_hash = longest_chain[-idx-1]\n            parent_hash = self.parent_hash if idx <= 0 else self._longest_chain_cache[-idx]\n            weight = self.weight_lookup.get(the_hash)\n            item = (the_hash, parent_hash, weight)\n            self._locked_chain.append(item)\n            excluded.add(the_hash)\n        if self.did_lock_to_index_f:\n            self.did_lock_to_index_f(self._locked_chain[old_length:old_length+index], old_length)\n        old_chain_finder = self.chain_finder\n        self.chain_finder = ChainFinder()\n        self._longest_chain_cache = None\n\n        def iterate():\n            for tree in old_chain_finder.trees_from_bottom.values():\n                for c in tree:\n                    if c in excluded:\n                        break\n                    excluded.add(c)\n                    if c in old_chain_finder.parent_lookup:\n                        yield (c, old_chain_finder.parent_lookup[c])\n        self.chain_finder.load_nodes(iterate())\n        self.parent_hash = the_hash\n\n    def _longest_local_block_chain(self):\n        if self._longest_chain_cache is None:\n            max_weight = 0\n            longest = []\n            for chain in self.chain_finder.all_chains_ending_at(self.parent_hash):\n                weight = sum(self.weight_lookup.get(h, 0) for h in chain)\n                if weight > max_weight:\n                    longest = chain\n                    max_weight = weight\n            self._longest_chain_cache = longest[:-1]\n        return self._longest_chain_cache\n\n    def block_for_hash(self, h):\n        return self.unlocked_block_storage.get(h)\n\n    def add_headers(self, header_iter):\n        def iterate():\n            for header in header_iter:\n                h = header.hash()\n                self.weight_lookup[h] = header.difficulty\n                self.unlocked_block_storage[h] = header\n                yield h, header.previous_block_hash\n\n        old_longest_chain = self._longest_local_block_chain()\n\n        self.chain_finder.load_nodes(iterate())\n\n        self._longest_chain_cache = None\n        new_longest_chain = self._longest_local_block_chain()\n\n        if old_longest_chain and new_longest_chain:\n            old_path, new_path = self.chain_finder.find_ancestral_path(\n                old_longest_chain[0],\n                new_longest_chain[0]\n            )\n            old_path = old_path[:-1]\n            new_path = new_path[:-1]\n        else:\n            old_path = old_longest_chain\n            new_path = new_longest_chain\n        if old_path:\n            logger.debug(\"old_path is %r-%r\", old_path[0], old_path[-1])\n        if new_path:\n            logger.debug(\"new_path is %r-%r\", new_path[0], new_path[-1])\n            logger.debug(\"block chain now has %d elements\", self.length())\n\n        # return a list of operations:\n        # (\"add\"/\"remove\", the_hash, the_index)\n        ops = []\n        size = len(old_longest_chain) + len(self._locked_chain)\n        for idx, h in enumerate(old_path):\n            op = (\"remove\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            del self.hash_to_index_lookup[h]\n        size = len(new_longest_chain) + len(self._locked_chain)\n        for idx, h in reversed(list(enumerate(new_path))):\n            op = (\"add\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            self.hash_to_index_lookup[h] = size-idx-1\n        for callback in self.change_callbacks:\n            callback(self, ops)\n\n        return ops\n\n    def __repr__(self):\n        local_block_chain = self._longest_local_block_chain()\n        if local_block_chain:\n            finish = b2h_rev(local_block_chain[0])\n            start = b2h_rev(local_block_chain[-1])\n            longest_chain = \"longest chain %s to %s of size %d\" % (start, finish, self.unlocked_length())\n        else:\n            longest_chain = \"no unlocked elements\"\n        return \"<BlockChain with %d locked elements and %s>\" % (self.locked_length(), longest_chain)\n", "prompt": "Please write a python function called 'tuple_for_index' base the context. This function returns a tuple containing information about a block in the blockchain at the given index. It first checks if the index is negative, and if so, it adjusts it to be a positive index relative to the end of the blockchain. Then, it checks if the index is within the range of the locked chain. If it is, it returns the corresponding block from the locked chain. If the index is outside the range of the locked chain, it retrieves the block from the longest local block chain or the longest chain cache, depending on the index value. Finally, it looks up the weight of the block using the weight lookup dictionary and returns a tuple containing the block's hash, parent hash, and weight.:param self: BlockChain. An instance of the BlockChain class.\n:param index: Integer. The index of the block to retrieve.\n:return: Tuple. A tuple containing the block's hash, parent hash, and weight..\n        The context you need to refer to is as follows: class BlockChain(object):\n    def __init__(self, parent_hash=ZERO_HASH, unlocked_block_storage={}, did_lock_to_index_f=None):\n        self.parent_hash = parent_hash\n        self.hash_to_index_lookup = {}\n        self.weight_lookup = {}\n        self.chain_finder = ChainFinder()\n        self.change_callbacks = weakref.WeakSet()\n        self._longest_chain_cache = None\n        self.did_lock_to_index_f = did_lock_to_index_f\n        self.unlocked_block_storage = unlocked_block_storage\n\n        self._locked_chain = []\n\n    def preload_locked_blocks(self, headers_iter):\n        self._locked_chain = []\n        the_hash = self.parent_hash\n        for idx, h in enumerate(headers_iter):\n            the_hash = h.hash()\n            self._locked_chain.append((the_hash, h.previous_block_hash, h.difficulty))\n            self.hash_to_index_lookup[the_hash] = idx\n        self.parent_hash = the_hash\n\n    def is_hash_known(self, the_hash):\n        return the_hash in self.hash_to_index_lookup\n\n    def length(self):\n        return len(self._longest_local_block_chain()) + len(self._locked_chain)\n\n    def locked_length(self):\n        return len(self._locked_chain)\n\n    def unlocked_length(self):\n        return len(self._longest_local_block_chain())\n\n###The function: tuple_for_index###\n    def last_block_hash(self):\n        if self.length() == 0:\n            return self.parent_hash\n        return self.hash_for_index(-1)\n\n    def hash_for_index(self, index):\n        return self.tuple_for_index(index)[0]\n\n    def index_for_hash(self, the_hash):\n        return self.hash_to_index_lookup.get(the_hash)\n\n    def add_change_callback(self, callback):\n        self.change_callbacks.add(callback)\n\n    def lock_to_index(self, index):\n        old_length = len(self._locked_chain)\n        index -= old_length\n        longest_chain = self._longest_local_block_chain()\n        if index < 1:\n            return\n        excluded = set()\n        for idx in range(index):\n            the_hash = longest_chain[-idx-1]\n            parent_hash = self.parent_hash if idx <= 0 else self._longest_chain_cache[-idx]\n            weight = self.weight_lookup.get(the_hash)\n            item = (the_hash, parent_hash, weight)\n            self._locked_chain.append(item)\n            excluded.add(the_hash)\n        if self.did_lock_to_index_f:\n            self.did_lock_to_index_f(self._locked_chain[old_length:old_length+index], old_length)\n        old_chain_finder = self.chain_finder\n        self.chain_finder = ChainFinder()\n        self._longest_chain_cache = None\n\n        def iterate():\n            for tree in old_chain_finder.trees_from_bottom.values():\n                for c in tree:\n                    if c in excluded:\n                        break\n                    excluded.add(c)\n                    if c in old_chain_finder.parent_lookup:\n                        yield (c, old_chain_finder.parent_lookup[c])\n        self.chain_finder.load_nodes(iterate())\n        self.parent_hash = the_hash\n\n    def _longest_local_block_chain(self):\n        if self._longest_chain_cache is None:\n            max_weight = 0\n            longest = []\n            for chain in self.chain_finder.all_chains_ending_at(self.parent_hash):\n                weight = sum(self.weight_lookup.get(h, 0) for h in chain)\n                if weight > max_weight:\n                    longest = chain\n                    max_weight = weight\n            self._longest_chain_cache = longest[:-1]\n        return self._longest_chain_cache\n\n    def block_for_hash(self, h):\n        return self.unlocked_block_storage.get(h)\n\n    def add_headers(self, header_iter):\n        def iterate():\n            for header in header_iter:\n                h = header.hash()\n                self.weight_lookup[h] = header.difficulty\n                self.unlocked_block_storage[h] = header\n                yield h, header.previous_block_hash\n\n        old_longest_chain = self._longest_local_block_chain()\n\n        self.chain_finder.load_nodes(iterate())\n\n        self._longest_chain_cache = None\n        new_longest_chain = self._longest_local_block_chain()\n\n        if old_longest_chain and new_longest_chain:\n            old_path, new_path = self.chain_finder.find_ancestral_path(\n                old_longest_chain[0],\n                new_longest_chain[0]\n            )\n            old_path = old_path[:-1]\n            new_path = new_path[:-1]\n        else:\n            old_path = old_longest_chain\n            new_path = new_longest_chain\n        if old_path:\n            logger.debug(\"old_path is %r-%r\", old_path[0], old_path[-1])\n        if new_path:\n            logger.debug(\"new_path is %r-%r\", new_path[0], new_path[-1])\n            logger.debug(\"block chain now has %d elements\", self.length())\n\n        # return a list of operations:\n        # (\"add\"/\"remove\", the_hash, the_index)\n        ops = []\n        size = len(old_longest_chain) + len(self._locked_chain)\n        for idx, h in enumerate(old_path):\n            op = (\"remove\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            del self.hash_to_index_lookup[h]\n        size = len(new_longest_chain) + len(self._locked_chain)\n        for idx, h in reversed(list(enumerate(new_path))):\n            op = (\"add\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            self.hash_to_index_lookup[h] = size-idx-1\n        for callback in self.change_callbacks:\n            callback(self, ops)\n\n        return ops\n\n    def __repr__(self):\n        local_block_chain = self._longest_local_block_chain()\n        if local_block_chain:\n            finish = b2h_rev(local_block_chain[0])\n            start = b2h_rev(local_block_chain[-1])\n            longest_chain = \"longest chain %s to %s of size %d\" % (start, finish, self.unlocked_length())\n        else:\n            longest_chain = \"no unlocked elements\"\n        return \"<BlockChain with %d locked elements and %s>\" % (self.locked_length(), longest_chain)\n", "test_list": ["def test_large(self):\n    SIZE = 3000\n    ITEMS = [FakeBlock(i) for i in range(SIZE)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    BC = BlockChain(parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.locked_length() == 0\n    assert BC.length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    ops = BC.add_headers(ITEMS)\n    assert ops == [('add', ITEMS[i], i) for i in range(SIZE)]\n    assert longest_block_chain(BC) == list(range(SIZE))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == SIZE\n    for i in range(SIZE):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None", "def test_chain_locking(self):\n    SIZE = 2000\n    COUNT = 200\n    ITEMS = [FakeBlock(i, i - 1) for i in range(SIZE * COUNT)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    BC = BlockChain(parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.locked_length() == 0\n    assert BC.length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    for i in range(COUNT):\n        start, end = (i * SIZE, (i + 1) * SIZE)\n        lock_start = max(0, start - 10)\n        expected_parent = lock_start - 1 if lock_start else parent_for_0\n        assert BC.length() == start\n        assert BC.locked_length() == lock_start\n        ops = BC.add_headers(ITEMS[start:end])\n        assert ops == [('add', ITEMS[i], i) for i in range(start, end)]\n        assert longest_locked_block_chain(BC) == list(range(lock_start, end))\n        assert set(BC.chain_finder.missing_parents()) == {expected_parent}\n        assert BC.parent_hash == expected_parent\n        assert BC.locked_length() == lock_start\n        assert BC.length() == end\n        for i in range(start, end):\n            v = BC.tuple_for_index(i)\n            assert v[0] == i\n            assert v[1] == parent_for_0 if i == 0 else i\n        assert BC.index_for_hash(-1) is None\n        assert BC.locked_length() == max(0, lock_start)\n        BC.lock_to_index(end - 10)\n        assert BC.locked_length() == end - 10", "def test_basic(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(100)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.length() == 0\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    assert BC.parent_hash == parent_for_0\n    assert BC.index_for_hash(0) is None\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[:5])\n    assert ops == [('add', ITEMS[i], i) for i in range(5)]\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == list(range(5))\n    assert BC.length() == 5\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    for i in range(5):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[:7])\n    assert ops == [('add', ITEMS[i], i) for i in range(5, 7)]\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == list(range(7))\n    assert BC.length() == 7\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    for i in range(7):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[10:14])\n    assert ops == []\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == [0, 1, 2, 3, 4, 5, 6]\n    assert BC.locked_length() == 0\n    assert BC.locked_length() == 0\n    assert BC.length() == 7\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0, 9}\n    for i in range(7):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[7:10])\n    assert ops == [('add', ITEMS[i], i) for i in range(7, 14)]\n    assert longest_block_chain(BC) == list(range(14))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 14\n    for i in range(14):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[90:])\n    assert ops == []\n    assert longest_block_chain(BC) == list(range(14))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0, 89}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 14\n    for i in range(14):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[14:90])\n    assert ops == [('add', ITEMS[i], i) for i in range(14, 100)]\n    assert longest_block_chain(BC) == list(range(100))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 100\n    for i in range(100):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'tuple_for_index' should return a tuple of three elements: block's hash, parent hash, and weight. The input 'index' should be an integer, and the function should handle both positive and negative indices correctly.", "unit_test": "def test_tuple_for_index_output_format(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(10)]\n    BC.add_headers(ITEMS)\n    for i in range(10):\n        result = BC.tuple_for_index(i)\n        assert isinstance(result, tuple)\n        assert len(result) == 3\n        assert isinstance(result[0], int)\n        assert isinstance(result[1], int)\n        assert isinstance(result[2], int)", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_output_format"}, "Exception Handling": {"requirement": "The function 'tuple_for_index' should raise an IndexError with a descriptive message: 'Index out of range for blockchain.'' if the index is out of range of the blockchain.", "unit_test": "def test_tuple_for_index_out_of_range(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    try:\n        BC.tuple_for_index(10)\n    except IndexError as e:\n        assert str(e) == 'Index out of range for blockchain.'", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_out_of_range"}, "Edge Case Handling": {"requirement": "The function 'tuple_for_index' should correctly handle the edge case where the blockchain is empty and return a tuple with None values.", "unit_test": "def test_tuple_for_index_empty_blockchain(self):\n    BC = BlockChain(parent_for_0)\n    result = BC.tuple_for_index(0)\n    assert result == (None, None, None)", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_empty_blockchain"}, "Functionality Extension": {"requirement": "Extend the 'tuple_for_index' function to accept an optional parameter 'include_timestamp' which, if set to True, includes the block's timestamp in the returned tuple.", "unit_test": "def test_tuple_for_index_with_timestamp(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i, timestamp=i*1000) for i in range(5)]\n    BC.add_headers(ITEMS)\n    result = BC.tuple_for_index(2, include_timestamp=True)\n    assert len(result) == 4\n    assert result[3] == 2000", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_with_timestamp"}, "Annotation Coverage": {"requirement": "Ensure that the 'tuple_for_index' function has complete type annotations for its parameters and return type.", "unit_test": "def test_tuple_for_index_annotations(self):\n    annotations = BlockChain.tuple_for_index.__annotations__\n    assert annotations['index'] == int\n    assert annotations['return'] == tuple", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'tuple_for_index' function should not exceed 5.", "unit_test": "def test_tuple_for_index_complexity(self):\n    from radon.complexity import cc_visit\n    source = inspect.getsource(BlockChain.tuple_for_index)\n    complexity = cc_visit(source)\n    assert complexity[0].complexity <= 5", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_complexity"}, "Code Standard": {"requirement": "The 'tuple_for_index' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_tuple_for_index_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/blockchain.py'])\n    assert result.total_errors == 0", "test": "tests/blockchain_test.py::BlockchainTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'tuple_for_index' function should utilize the 'length', '_longest_local_block_chain' method of the BlockChain class.", "unit_test": "def test_tuple_for_index_context_usage(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    BC.tuple_for_index(2)\n    assert '_locked_chain' in BC.__dict__\n    assert '_longest_chain_cache' in BC.__dict__\n    assert 'weight_lookup' in BC.__dict__", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'tuple_for_index' function should correctly use the '_locked_chain' for indices within its range and '_longest_chain_cache' for indices outside of it.", "unit_test": "def test_tuple_for_index_correct_context_usage(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    BC.lock_to_index(2)\n    result_locked = BC.tuple_for_index(1)\n    result_unlocked = BC.tuple_for_index(3)\n    assert result_locked[0] in [block[0] for block in BC._locked_chain]\n    assert result_unlocked[0] in BC._longest_local_block_chain()", "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_correct_context_usage"}}}
{"namespace": "datasette.utils.asgi.Response.redirect", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/utils/asgi.py", "signature_position": [411, 411], "body_position": [412, 414], "dependency": {"intra_class": ["datasette.utils.asgi.Response.__init__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function creates a redirect response. It sets the \"Location\" header to the specified path and returns a Response instance with the given status code and headers.", "Arguments": ":param cls: Class. The class of the Response instance.\n:param path: String. The path to redirect to.\n:param status: Integer. The status code for the response. It defaults to 302 if not specified.\n:param headers: Dictionary. Additional headers to include in the response. It defaults to an empty dictionary if not specified.\n:return: Response. The created redirect response instance."}, "tests": ["tests/test_internals_response.py::test_response_redirect", "tests/test_internals_response.py::test_response_set_cookie"], "indent": 4, "domain": "Database", "code": "\n    @classmethod\n    def redirect(cls, path, status=302, headers=None):\n        headers = headers or {}\n", "context": "class Response:\n    def __init__(self, body=None, status=200, headers=None, content_type=\"text/plain\"):\n        self.body = body\n        self.status = status\n        self.headers = headers or {}\n        self._set_cookie_headers = []\n        self.content_type = content_type\n\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):\n            body = body.encode(\"utf-8\")\n        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:\n                cookie[key][prop_name] = True\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n\n    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n###The function: redirect###        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n", "prompt": "Please write a python function called 'redirect' base the context. This function creates a redirect response. It sets the \"Location\" header to the specified path and returns a Response instance with the given status code and headers.:param cls: Class. The class of the Response instance.\n:param path: String. The path to redirect to.\n:param status: Integer. The status code for the response. It defaults to 302 if not specified.\n:param headers: Dictionary. Additional headers to include in the response. It defaults to an empty dictionary if not specified.\n:return: Response. The created redirect response instance..\n        The context you need to refer to is as follows: class Response:\n    def __init__(self, body=None, status=200, headers=None, content_type=\"text/plain\"):\n        self.body = body\n        self.status = status\n        self.headers = headers or {}\n        self._set_cookie_headers = []\n        self.content_type = content_type\n\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):\n            body = body.encode(\"utf-8\")\n        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:\n                cookie[key][prop_name] = True\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n\n    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n###The function: redirect###        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n", "test_list": ["def test_response_redirect():\n    response = Response.redirect('/foo')\n    assert 302 == response.status\n    assert '/foo' == response.headers['Location']", "@pytest.mark.asyncio\nasync def test_response_set_cookie():\n    events = []\n\n    async def send(event):\n        events.append(event)\n    response = Response.redirect('/foo')\n    response.set_cookie('foo', 'bar', max_age=10, httponly=True)\n    await response.asgi_send(send)\n    assert [{'type': 'http.response.start', 'status': 302, 'headers': [[b'Location', b'/foo'], [b'content-type', b'text/plain'], [b'set-cookie', b'foo=bar; HttpOnly; Max-Age=10; Path=/; SameSite=lax']]}, {'type': 'http.response.body', 'body': b''}] == events"], "requirements": {"Input-Output Conditions": {"requirement": "The 'redirect' function should accept a string for 'path', an integer for 'status', and a dictionary for 'headers'. It should return a Response instance with the 'Location' header set to the specified path and the status code set to the specified status.", "unit_test": "def test_redirect_input_output_conditions():\n    response = Response.redirect('/test', status=301, headers={'X-Test': 'value'})\n    assert isinstance(response, Response)\n    assert response.headers['Location'] == '/test'\n    assert response.status == 301\n    assert response.headers['X-Test'] == 'value'", "test": "tests/test_internals_response.py::test_redirect_input_output_conditions"}, "Exception Handling": {"requirement": "The 'redirect' function should raise a TypeError if 'path' is not a string, 'status' is not an integer, or 'headers' is not a dictionary.", "unit_test": "def test_redirect_exception_handling():\n    with pytest.raises(TypeError):\n        Response.redirect(123)\n    with pytest.raises(TypeError):\n        Response.redirect('/test', status='301')\n    with pytest.raises(TypeError):\n        Response.redirect('/test', headers='not-a-dict')", "test": "tests/test_internals_response.py::test_redirect_exception_handling"}, "Edge Case Handling": {"requirement": "The 'redirect' function should handle edge cases where 'path' is an empty string or 'headers' is None.", "unit_test": "def test_redirect_edge_case_handling():\n    response = Response.redirect('')\n    assert response.headers['Location'] == ''\n    response = Response.redirect('/test', headers=None)\n    assert response.headers['Location'] == '/test'", "test": "tests/test_internals_response.py::test_redirect_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'redirect' function to allow setting a default content type of 'text/html' for the redirect response.", "unit_test": "def test_redirect_functionality_extension():\n    response = Response.redirect('/test', status=301)\n    assert response.content_type == 'text/html; charset=utf-8'", "test": "tests/test_internals_response.py::test_redirect_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'redirect' function has complete type annotations for all parameters and the return type.", "unit_test": "def test_redirect_annotation_coverage():\n    from typing import get_type_hints\n    hints = get_type_hints(Response.redirect)\n    assert hints['path'] == str\n    assert hints['status'] == int\n    assert hints['headers'] == dict\n    assert hints['return'] == Response", "test": "tests/test_internals_response.py::test_redirect_annotation_coverage"}, "Code Standard": {"requirement": "The 'redirect' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": "def test_redirect_code_standard():\n    import subprocess\n    result = subprocess.run(['flake8', '--select=E,W', '--max-line-length=79'], capture_output=True, text=True)\n    assert result.returncode == 0, f'PEP 8 violations: {result.stdout}'", "test": "tests/test_internals_response.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'redirect' function utilizes the 'Response' class from the provided context.", "unit_test": "def test_redirect_context_usage_verification():\n    response = Response.redirect('/test')\n    assert isinstance(response, Response)", "test": "tests/test_internals_response.py::test_redirect_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'redirect' function correctly uses the 'Response' class to set the 'Location' header and status code.", "unit_test": "def test_redirect_context_usage_correctness_verification():\n    response = Response.redirect('/test', status=301)\n    assert response.headers['Location'] == '/test'\n    assert response.status == 301", "test": "tests/test_internals_response.py::test_redirect_context_usage_correctness_verification"}}}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "type": "method", "project_path": "Communications/PySimpleSOAP", "completion_path": "Communications/PySimpleSOAP/pysimplesoap/simplexml.py", "signature_position": [116, 116], "body_position": [118, 121], "dependency": {"intra_class": ["pysimplesoap.simplexml.SimpleXMLElement.__document"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns the XML representation of the document. If the \"pretty\" parameter is set to False, it returns the XML representation without any formatting. If \"pretty\" is set to True, it returns the XML representation with indentation and line breaks for better readability.", "Arguments": ":param self: SimpleXMLElement. An instance of the SimpleXMLElement class.\n:param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n:param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n:return: String. The XML representation of the document."}, "tests": ["tests/simplexmlelement_test.py::TestSimpleXMLElement::test_marshall_cdata", "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_to_xml"], "indent": 4, "domain": "Communications", "code": "    def as_xml(self, filename=None, pretty=False):\n        \"\"\"Return the XML representation of the document\"\"\"\n        if not pretty:\n            return self.__document.toxml('UTF-8')\n        else:\n            return self.__document.toprettyxml(encoding='UTF-8')\n", "context": "class SimpleXMLElement(object):\n    \"\"\"Simple XML manipulation (simil PHP)\"\"\"\n\n    def __init__(self, text=None, elements=None, document=None,\n                 namespace=None, prefix=None, namespaces_map={}, jetty=False):\n        \"\"\"\n        :param namespaces_map: How to map our namespace prefix to that given by the client;\n          {prefix: received_prefix}\n        \"\"\"\n        self.__namespaces_map = namespaces_map\n        _rx = \"|\".join(namespaces_map.keys())  # {'external': 'ext', 'model': 'mod'} -> 'external|model'\n        self.__ns_rx = re.compile(r\"^(%s):.*$\" % _rx)  # And now we build an expression ^(external|model):.*$\n                                                       # to find prefixes in all xml nodes i.e.: <model:code>1</model:code>\n                                                       # and later change that to <mod:code>1</mod:code>\n        self.__ns = namespace\n        self.__prefix = prefix\n        self.__jetty = jetty                           # special list support\n\n        if text is not None:\n            try:\n                self.__document = xml.dom.minidom.parseString(text)\n            except:\n                log.error(text)\n                raise\n            self.__elements = [self.__document.documentElement]\n        else:\n            self.__elements = elements\n            self.__document = document\n\n    def add_child(self, name, text=None, ns=True):\n        \"\"\"Adding a child tag to a node\"\"\"\n        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __setattr__(self, tag, text):\n        \"\"\"Add text child tag node (short form)\"\"\"\n        if tag.startswith(\"_\"):\n            object.__setattr__(self, tag, text)\n        else:\n            ##log.debug('__setattr__(%s, %s)', tag, text)\n            self.add_child(tag, text)\n\n    def __delattr__(self, tag):\n        \"\"\"Remove a child tag (non recursive!)\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        for element in elements:\n            self._element.removeChild(element)\n\n    def add_comment(self, data):\n        \"\"\"Add an xml comment to this child\"\"\"\n        comment = self.__document.createComment(data)\n        self._element.appendChild(comment)\n\n###The function: as_xml###\n    if sys.version > '3':\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            return self._element.toxml()\n    else:\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            # NOTE: do not use self.as_xml('UTF-8') as it returns the whole xml doc\n            return self._element.toxml('UTF-8')\n\n    def get_name(self):\n        \"\"\"Return the tag name of this node\"\"\"\n        return self._element.tagName\n\n    def get_local_name(self):\n        \"\"\"Return the tag local name (prefix:name) of this node\"\"\"\n        return self._element.localName\n\n    def get_prefix(self):\n        \"\"\"Return the namespace prefix of this node\"\"\"\n        return self._element.prefix\n\n    def get_namespace_uri(self, ns):\n        \"\"\"Return the namespace uri for a prefix\"\"\"\n        element = self._element\n        while element is not None and element.attributes is not None:\n            try:\n                return element.attributes['xmlns:%s' % ns].value\n            except KeyError:\n                element = element.parentNode\n\n    def attributes(self):\n        \"\"\"Return a dict of attributes for this tag\"\"\"\n        #TODO: use slice syntax [:]?\n        return self._element.attributes\n\n    def __getitem__(self, item):\n        \"\"\"Return xml tag attribute value or a slice of attributes (iter)\"\"\"\n        ##log.debug('__getitem__(%s)', item)\n        if isinstance(item, basestring):\n            if self._element.hasAttribute(item):\n                return self._element.attributes[item].value\n        elif isinstance(item, slice):\n            # return a list with name:values\n            return list(self._element.attributes.items())[item]\n        else:\n            # return element by index (position)\n            element = self.__elements[item]\n            return SimpleXMLElement(\n                elements=[element],\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map\n            )\n\n    def add_attribute(self, name, value):\n        \"\"\"Set an attribute value from a string\"\"\"\n        self._element.setAttribute(name, value)\n\n    def __setitem__(self, item, value):\n        \"\"\"Set an attribute value\"\"\"\n        if isinstance(item, basestring):\n            self.add_attribute(item, value)\n        elif isinstance(item, slice):\n            # set multiple attributes at once\n            for k, v in value.items():\n                self.add_attribute(k, v)\n\n    def __delitem__(self, item):\n        \"Remove an attribute\"\n        self._element.removeAttribute(item)\n\n    def __call__(self, tag=None, ns=None, children=False, root=False,\n                 error=True, ):\n        \"\"\"Search (even in child nodes) and return a child tag by name\"\"\"\n        try:\n            if root:\n                # return entire document\n                return SimpleXMLElement(\n                    elements=[self.__document.documentElement],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map\n                )\n            if tag is None:\n                # if no name given, iterate over siblings (same level)\n                return self.__iter__()\n            if children:\n                # future: filter children? by ns?\n                return self.children()\n            elements = None\n            if isinstance(tag, int):\n                # return tag by index\n                elements = [self.__elements[tag]]\n            if ns and not elements:\n                for ns_uri in isinstance(ns, (tuple, list)) and ns or (ns, ):\n                    ##log.debug('searching %s by ns=%s', tag, ns_uri)\n                    elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                    if elements:\n                        break\n            if self.__ns and not elements:\n                ##log.debug('searching %s by ns=%s', tag, self.__ns)\n                elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n            if not elements:\n                ##log.debug('searching %s', tag)\n                elements = self._element.getElementsByTagName(tag)\n            if not elements:\n                ##log.debug(self._element.toxml())\n                if error:\n                    raise AttributeError(\"No elements found\")\n                else:\n                    return\n            return SimpleXMLElement(\n                elements=elements,\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map)\n        except AttributeError as e:\n            raise AttributeError(\"Tag not found: %s (%s)\" % (tag, e))\n\n    def __getattr__(self, tag):\n        \"\"\"Shortcut for __call__\"\"\"\n        return self.__call__(tag)\n\n    def __iter__(self):\n        \"\"\"Iterate over xml tags at this level\"\"\"\n        try:\n            for __element in self.__elements:\n                yield SimpleXMLElement(\n                    elements=[__element],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map)\n        except:\n            raise\n\n    def __dir__(self):\n        \"\"\"List xml children tags names\"\"\"\n        return [node.tagName for node\n                in self._element.childNodes\n                if node.nodeType != node.TEXT_NODE]\n\n    def children(self):\n        \"\"\"Return xml children tags element\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        if not elements:\n            return None\n            #raise IndexError(\"Tag %s has no children\" % self._element.tagName)\n        return SimpleXMLElement(\n            elements=elements,\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __len__(self):\n        \"\"\"Return element count\"\"\"\n        return len(self.__elements)\n\n    def __contains__(self, item):\n        \"\"\"Search for a tag name in this element or child nodes\"\"\"\n        return self._element.getElementsByTagName(item)\n\n    def __unicode__(self):\n        \"\"\"Returns the unicode text nodes of the current element\"\"\"\n        rc = ''\n        for node in self._element.childNodes:\n            if node.nodeType == node.TEXT_NODE or node.nodeType == node.CDATA_SECTION_NODE:\n                rc = rc + node.data\n        return rc\n\n    if sys.version > '3':\n        __str__ = __unicode__\n    else:\n        def __str__(self):\n            return self.__unicode__().encode('utf-8')\n\n    def __int__(self):\n        \"\"\"Returns the integer value of the current element\"\"\"\n        return int(self.__str__())\n\n    def __float__(self):\n        \"\"\"Returns the float value of the current element\"\"\"\n        try:\n            return float(self.__str__())\n        except:\n            raise IndexError(self._element.toxml())\n\n    _element = property(lambda self: self.__elements[0])\n\n    def unmarshall(self, types, strict=True):\n        #import pdb; pdb.set_trace()\n\n        \"\"\"Convert to python values the current serialized xml element\"\"\"\n        # types is a dict of {tag name: convertion function}\n        # strict=False to use default type conversion if not specified\n        # example: types={'p': {'a': int,'b': int}, 'c': [{'d':str}]}\n        #   expected xml: <p><a>1</a><b>2</b></p><c><d>hola</d><d>chau</d>\n        #   returnde value: {'p': {'a':1,'b':2}, `'c':[{'d':'hola'},{'d':'chau'}]}\n        d = {}\n        for node in self():\n            name = str(node.get_local_name())\n            ref_name_type = None\n            # handle multirefs: href=\"#id0\"\n            if 'href' in node.attributes().keys():\n                href = node['href'][1:]\n                for ref_node in self(root=True)(\"multiRef\"):\n                    if ref_node['id'] == href:\n                        node = ref_node\n                        ref_name_type = ref_node['xsi:type'].split(\":\")[1]\n                        break\n\n            try:\n                if isinstance(types, dict):\n                    fn = types[name]\n                    # custom array only in the response (not defined in the WSDL):\n                    # <results soapenc:arrayType=\"xsd:string[199]>\n                    if any([k for k,v in node[:] if 'arrayType' in k]) and not isinstance(fn, list):\n                        fn = [fn]\n                else:\n                    fn = types\n            except (KeyError, ) as e:\n                xmlns = node['xmlns'] or node.get_namespace_uri(node.get_prefix())\n                if 'xsi:type' in node.attributes().keys():\n                    xsd_type = node['xsi:type'].split(\":\")[1]\n                    try:\n                        # get fn type from SOAP-ENC:arrayType=\"xsd:string[28]\"\n                        if xsd_type == 'Array':\n                            array_type = [k for k,v in node[:] if 'arrayType' in k][0]\n                            xsd_type = node[array_type].split(\":\")[1]\n                            if \"[\" in xsd_type:\n                                xsd_type = xsd_type[:xsd_type.index(\"[\")]\n                            fn = [REVERSE_TYPE_MAP[xsd_type]]\n                        else:\n                            fn = REVERSE_TYPE_MAP[xsd_type]\n                    except:\n                        fn = None  # ignore multirefs!\n                elif xmlns == \"http://www.w3.org/2001/XMLSchema\":\n                    # self-defined schema, return the SimpleXMLElement\n                    # TODO: parse to python types if <s:element ref=\"s:schema\"/>\n                    fn = None\n                elif None in types:\n                    # <s:any/>, return the SimpleXMLElement\n                    # TODO: check position of None if inside <s:sequence>\n                    fn = None\n                elif strict:\n                    raise TypeError(\"Tag: %s invalid (type not found)\" % (name,))\n                else:\n                    # if not strict, use default type conversion\n                    fn = str\n\n            if isinstance(fn, list):\n                # append to existing list (if any) - unnested dict arrays -\n                value = d.setdefault(name, [])\n                children = node.children()\n                # TODO: check if this was really needed (get first child only)\n                ##if len(fn[0]) == 1 and children:\n                ##    children = children()\n                if fn and not isinstance(fn[0], dict):\n                    # simple arrays []\n                    for child in (children or []):\n                        tmp_dict = child.unmarshall(fn[0], strict)\n                        value.extend(tmp_dict.values())\n                elif (self.__jetty and len(fn[0]) > 1):\n                    # Jetty array style support [{k, v}]\n                    for parent in node:\n                        tmp_dict = {}    # unmarshall each value & mix\n                        for child in (node.children() or []):\n                            tmp_dict.update(child.unmarshall(fn[0], strict))\n                        value.append(tmp_dict)\n                else:  # .Net / Java\n                    for child in (children or []):\n                        value.append(child.unmarshall(fn[0], strict))\n\n            elif isinstance(fn, tuple):\n                value = []\n                _d = {}\n                children = node.children()\n                as_dict = len(fn) == 1 and isinstance(fn[0], dict)\n\n                for child in (children and children() or []):  # Readability counts\n                    if as_dict:\n                        _d.update(child.unmarshall(fn[0], strict))  # Merging pairs\n                    else:\n                        value.append(child.unmarshall(fn[0], strict))\n                if as_dict:\n                    value.append(_d)\n\n                if name in d:\n                    _tmp = list(d[name])\n                    _tmp.extend(value)\n                    value = tuple(_tmp)\n                else:\n                    value = tuple(value)\n\n            elif isinstance(fn, dict):\n                ##if ref_name_type is not None:\n                ##    fn = fn[ref_name_type]\n                children = node.children()\n                value = children and children.unmarshall(fn, strict)\n            else:\n                if fn is None:  # xsd:anyType not unmarshalled\n                    value = node\n                elif unicode(node) or (fn == str and unicode(node) != ''):\n                    try:\n                        # get special deserialization function (if any)\n                        fn = TYPE_UNMARSHAL_FN.get(fn, fn)\n                        if fn == str:\n                            # always return an unicode object:\n                            # (avoid encoding errors in py<3!)\n                            value = unicode(node)\n                        else:\n                            value = fn(unicode(node))\n                    except (ValueError, TypeError) as e:\n                        raise ValueError(\"Tag: %s: %s\" % (name, e))\n                else:\n                    value = None\n            d[name] = value\n        return d\n\n    def _update_ns(self, name):\n        \"\"\"Replace the defined namespace alias with tohse used by the client.\"\"\"\n        pref = self.__ns_rx.search(name)\n        if pref:\n            pref = pref.groups()[0]\n            try:\n                name = name.replace(pref, self.__namespaces_map[pref])\n            except KeyError:\n                log.warning('Unknown namespace alias %s' % name)\n        return name\n\n    def marshall(self, name, value, add_child=True, add_comments=False,\n                 ns=False, add_children_ns=True):\n        \"\"\"Analyze python value and add the serialized XML element using tag name\"\"\"\n        # Change node name to that used by a client\n        name = self._update_ns(name)\n\n        if isinstance(value, dict):  # serialize dict (<key>value</key>)\n            # for the first parent node, use the document target namespace\n            # (ns==True) or use the namespace string uri if passed (elements)\n            child = add_child and self.add_child(name, ns=ns) or self\n            for k, v in value.items():\n                if not add_children_ns:\n                    ns = False\n                elif hasattr(value, 'namespaces'):\n                    # for children, use the wsdl element target namespace:\n                    ns = value.namespaces.get(k)\n                else:\n                    # simple type\n                    ns = None\n                child.marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, tuple):  # serialize tuple (<key>value</key>)\n            child = add_child and self.add_child(name, ns=ns) or self\n            if not add_children_ns:\n                ns = False\n            for k, v in value:\n                getattr(self, name).marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, list):  # serialize lists\n            child = self.add_child(name, ns=ns)\n            if not add_children_ns:\n                ns = False\n            if add_comments:\n                child.add_comment(\"Repetitive array of:\")\n            for t in value:\n                child.marshall(name, t, False, add_comments=add_comments, ns=ns)\n        elif isinstance(value, (xml.dom.minidom.CDATASection, basestring)):  # do not convert strings or unicodes\n            self.add_child(name, value, ns=ns)\n        elif value is None:  # sent a empty tag?\n            self.add_child(name, ns=ns)\n        elif value in TYPE_MAP.keys():\n            # add commented placeholders for simple tipes (for examples/help only)\n            child = self.add_child(name, ns=ns)\n            child.add_comment(TYPE_MAP[value])\n        else:  # the rest of object types are converted to string\n            # get special serialization function (if any)\n            fn = TYPE_MARSHAL_FN.get(type(value), str)\n            self.add_child(name, fn(value), ns=ns)\n\n    def import_node(self, other):\n        x = self.__document.importNode(other._element, True)  # deep copy\n        self._element.appendChild(x)\n\n    def write_c14n(self, output=None, exclusive=True):\n        \"Generate the canonical version of the XML node\"\n        from . import c14n\n        xml = c14n.Canonicalize(self._element, output,\n                                unsuppressedPrefixes=[] if exclusive else None)\n        return xml\n", "prompt": "Please write a python function called 'as_xml' base the context. This function returns the XML representation of the document. If the \"pretty\" parameter is set to False, it returns the XML representation without any formatting. If \"pretty\" is set to True, it returns the XML representation with indentation and line breaks for better readability.:param self: SimpleXMLElement. An instance of the SimpleXMLElement class.\n:param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n:param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n:return: String. The XML representation of the document..\n        The context you need to refer to is as follows: class SimpleXMLElement(object):\n    \"\"\"Simple XML manipulation (simil PHP)\"\"\"\n\n    def __init__(self, text=None, elements=None, document=None,\n                 namespace=None, prefix=None, namespaces_map={}, jetty=False):\n        \"\"\"\n        :param namespaces_map: How to map our namespace prefix to that given by the client;\n          {prefix: received_prefix}\n        \"\"\"\n        self.__namespaces_map = namespaces_map\n        _rx = \"|\".join(namespaces_map.keys())  # {'external': 'ext', 'model': 'mod'} -> 'external|model'\n        self.__ns_rx = re.compile(r\"^(%s):.*$\" % _rx)  # And now we build an expression ^(external|model):.*$\n                                                       # to find prefixes in all xml nodes i.e.: <model:code>1</model:code>\n                                                       # and later change that to <mod:code>1</mod:code>\n        self.__ns = namespace\n        self.__prefix = prefix\n        self.__jetty = jetty                           # special list support\n\n        if text is not None:\n            try:\n                self.__document = xml.dom.minidom.parseString(text)\n            except:\n                log.error(text)\n                raise\n            self.__elements = [self.__document.documentElement]\n        else:\n            self.__elements = elements\n            self.__document = document\n\n    def add_child(self, name, text=None, ns=True):\n        \"\"\"Adding a child tag to a node\"\"\"\n        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __setattr__(self, tag, text):\n        \"\"\"Add text child tag node (short form)\"\"\"\n        if tag.startswith(\"_\"):\n            object.__setattr__(self, tag, text)\n        else:\n            ##log.debug('__setattr__(%s, %s)', tag, text)\n            self.add_child(tag, text)\n\n    def __delattr__(self, tag):\n        \"\"\"Remove a child tag (non recursive!)\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        for element in elements:\n            self._element.removeChild(element)\n\n    def add_comment(self, data):\n        \"\"\"Add an xml comment to this child\"\"\"\n        comment = self.__document.createComment(data)\n        self._element.appendChild(comment)\n\n###The function: as_xml###\n    if sys.version > '3':\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            return self._element.toxml()\n    else:\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            # NOTE: do not use self.as_xml('UTF-8') as it returns the whole xml doc\n            return self._element.toxml('UTF-8')\n\n    def get_name(self):\n        \"\"\"Return the tag name of this node\"\"\"\n        return self._element.tagName\n\n    def get_local_name(self):\n        \"\"\"Return the tag local name (prefix:name) of this node\"\"\"\n        return self._element.localName\n\n    def get_prefix(self):\n        \"\"\"Return the namespace prefix of this node\"\"\"\n        return self._element.prefix\n\n    def get_namespace_uri(self, ns):\n        \"\"\"Return the namespace uri for a prefix\"\"\"\n        element = self._element\n        while element is not None and element.attributes is not None:\n            try:\n                return element.attributes['xmlns:%s' % ns].value\n            except KeyError:\n                element = element.parentNode\n\n    def attributes(self):\n        \"\"\"Return a dict of attributes for this tag\"\"\"\n        #TODO: use slice syntax [:]?\n        return self._element.attributes\n\n    def __getitem__(self, item):\n        \"\"\"Return xml tag attribute value or a slice of attributes (iter)\"\"\"\n        ##log.debug('__getitem__(%s)', item)\n        if isinstance(item, basestring):\n            if self._element.hasAttribute(item):\n                return self._element.attributes[item].value\n        elif isinstance(item, slice):\n            # return a list with name:values\n            return list(self._element.attributes.items())[item]\n        else:\n            # return element by index (position)\n            element = self.__elements[item]\n            return SimpleXMLElement(\n                elements=[element],\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map\n            )\n\n    def add_attribute(self, name, value):\n        \"\"\"Set an attribute value from a string\"\"\"\n        self._element.setAttribute(name, value)\n\n    def __setitem__(self, item, value):\n        \"\"\"Set an attribute value\"\"\"\n        if isinstance(item, basestring):\n            self.add_attribute(item, value)\n        elif isinstance(item, slice):\n            # set multiple attributes at once\n            for k, v in value.items():\n                self.add_attribute(k, v)\n\n    def __delitem__(self, item):\n        \"Remove an attribute\"\n        self._element.removeAttribute(item)\n\n    def __call__(self, tag=None, ns=None, children=False, root=False,\n                 error=True, ):\n        \"\"\"Search (even in child nodes) and return a child tag by name\"\"\"\n        try:\n            if root:\n                # return entire document\n                return SimpleXMLElement(\n                    elements=[self.__document.documentElement],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map\n                )\n            if tag is None:\n                # if no name given, iterate over siblings (same level)\n                return self.__iter__()\n            if children:\n                # future: filter children? by ns?\n                return self.children()\n            elements = None\n            if isinstance(tag, int):\n                # return tag by index\n                elements = [self.__elements[tag]]\n            if ns and not elements:\n                for ns_uri in isinstance(ns, (tuple, list)) and ns or (ns, ):\n                    ##log.debug('searching %s by ns=%s', tag, ns_uri)\n                    elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                    if elements:\n                        break\n            if self.__ns and not elements:\n                ##log.debug('searching %s by ns=%s', tag, self.__ns)\n                elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n            if not elements:\n                ##log.debug('searching %s', tag)\n                elements = self._element.getElementsByTagName(tag)\n            if not elements:\n                ##log.debug(self._element.toxml())\n                if error:\n                    raise AttributeError(\"No elements found\")\n                else:\n                    return\n            return SimpleXMLElement(\n                elements=elements,\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map)\n        except AttributeError as e:\n            raise AttributeError(\"Tag not found: %s (%s)\" % (tag, e))\n\n    def __getattr__(self, tag):\n        \"\"\"Shortcut for __call__\"\"\"\n        return self.__call__(tag)\n\n    def __iter__(self):\n        \"\"\"Iterate over xml tags at this level\"\"\"\n        try:\n            for __element in self.__elements:\n                yield SimpleXMLElement(\n                    elements=[__element],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map)\n        except:\n            raise\n\n    def __dir__(self):\n        \"\"\"List xml children tags names\"\"\"\n        return [node.tagName for node\n                in self._element.childNodes\n                if node.nodeType != node.TEXT_NODE]\n\n    def children(self):\n        \"\"\"Return xml children tags element\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        if not elements:\n            return None\n            #raise IndexError(\"Tag %s has no children\" % self._element.tagName)\n        return SimpleXMLElement(\n            elements=elements,\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __len__(self):\n        \"\"\"Return element count\"\"\"\n        return len(self.__elements)\n\n    def __contains__(self, item):\n        \"\"\"Search for a tag name in this element or child nodes\"\"\"\n        return self._element.getElementsByTagName(item)\n\n    def __unicode__(self):\n        \"\"\"Returns the unicode text nodes of the current element\"\"\"\n        rc = ''\n        for node in self._element.childNodes:\n            if node.nodeType == node.TEXT_NODE or node.nodeType == node.CDATA_SECTION_NODE:\n                rc = rc + node.data\n        return rc\n\n    if sys.version > '3':\n        __str__ = __unicode__\n    else:\n        def __str__(self):\n            return self.__unicode__().encode('utf-8')\n\n    def __int__(self):\n        \"\"\"Returns the integer value of the current element\"\"\"\n        return int(self.__str__())\n\n    def __float__(self):\n        \"\"\"Returns the float value of the current element\"\"\"\n        try:\n            return float(self.__str__())\n        except:\n            raise IndexError(self._element.toxml())\n\n    _element = property(lambda self: self.__elements[0])\n\n    def unmarshall(self, types, strict=True):\n        #import pdb; pdb.set_trace()\n\n        \"\"\"Convert to python values the current serialized xml element\"\"\"\n        # types is a dict of {tag name: convertion function}\n        # strict=False to use default type conversion if not specified\n        # example: types={'p': {'a': int,'b': int}, 'c': [{'d':str}]}\n        #   expected xml: <p><a>1</a><b>2</b></p><c><d>hola</d><d>chau</d>\n        #   returnde value: {'p': {'a':1,'b':2}, `'c':[{'d':'hola'},{'d':'chau'}]}\n        d = {}\n        for node in self():\n            name = str(node.get_local_name())\n            ref_name_type = None\n            # handle multirefs: href=\"#id0\"\n            if 'href' in node.attributes().keys():\n                href = node['href'][1:]\n                for ref_node in self(root=True)(\"multiRef\"):\n                    if ref_node['id'] == href:\n                        node = ref_node\n                        ref_name_type = ref_node['xsi:type'].split(\":\")[1]\n                        break\n\n            try:\n                if isinstance(types, dict):\n                    fn = types[name]\n                    # custom array only in the response (not defined in the WSDL):\n                    # <results soapenc:arrayType=\"xsd:string[199]>\n                    if any([k for k,v in node[:] if 'arrayType' in k]) and not isinstance(fn, list):\n                        fn = [fn]\n                else:\n                    fn = types\n            except (KeyError, ) as e:\n                xmlns = node['xmlns'] or node.get_namespace_uri(node.get_prefix())\n                if 'xsi:type' in node.attributes().keys():\n                    xsd_type = node['xsi:type'].split(\":\")[1]\n                    try:\n                        # get fn type from SOAP-ENC:arrayType=\"xsd:string[28]\"\n                        if xsd_type == 'Array':\n                            array_type = [k for k,v in node[:] if 'arrayType' in k][0]\n                            xsd_type = node[array_type].split(\":\")[1]\n                            if \"[\" in xsd_type:\n                                xsd_type = xsd_type[:xsd_type.index(\"[\")]\n                            fn = [REVERSE_TYPE_MAP[xsd_type]]\n                        else:\n                            fn = REVERSE_TYPE_MAP[xsd_type]\n                    except:\n                        fn = None  # ignore multirefs!\n                elif xmlns == \"http://www.w3.org/2001/XMLSchema\":\n                    # self-defined schema, return the SimpleXMLElement\n                    # TODO: parse to python types if <s:element ref=\"s:schema\"/>\n                    fn = None\n                elif None in types:\n                    # <s:any/>, return the SimpleXMLElement\n                    # TODO: check position of None if inside <s:sequence>\n                    fn = None\n                elif strict:\n                    raise TypeError(\"Tag: %s invalid (type not found)\" % (name,))\n                else:\n                    # if not strict, use default type conversion\n                    fn = str\n\n            if isinstance(fn, list):\n                # append to existing list (if any) - unnested dict arrays -\n                value = d.setdefault(name, [])\n                children = node.children()\n                # TODO: check if this was really needed (get first child only)\n                ##if len(fn[0]) == 1 and children:\n                ##    children = children()\n                if fn and not isinstance(fn[0], dict):\n                    # simple arrays []\n                    for child in (children or []):\n                        tmp_dict = child.unmarshall(fn[0], strict)\n                        value.extend(tmp_dict.values())\n                elif (self.__jetty and len(fn[0]) > 1):\n                    # Jetty array style support [{k, v}]\n                    for parent in node:\n                        tmp_dict = {}    # unmarshall each value & mix\n                        for child in (node.children() or []):\n                            tmp_dict.update(child.unmarshall(fn[0], strict))\n                        value.append(tmp_dict)\n                else:  # .Net / Java\n                    for child in (children or []):\n                        value.append(child.unmarshall(fn[0], strict))\n\n            elif isinstance(fn, tuple):\n                value = []\n                _d = {}\n                children = node.children()\n                as_dict = len(fn) == 1 and isinstance(fn[0], dict)\n\n                for child in (children and children() or []):  # Readability counts\n                    if as_dict:\n                        _d.update(child.unmarshall(fn[0], strict))  # Merging pairs\n                    else:\n                        value.append(child.unmarshall(fn[0], strict))\n                if as_dict:\n                    value.append(_d)\n\n                if name in d:\n                    _tmp = list(d[name])\n                    _tmp.extend(value)\n                    value = tuple(_tmp)\n                else:\n                    value = tuple(value)\n\n            elif isinstance(fn, dict):\n                ##if ref_name_type is not None:\n                ##    fn = fn[ref_name_type]\n                children = node.children()\n                value = children and children.unmarshall(fn, strict)\n            else:\n                if fn is None:  # xsd:anyType not unmarshalled\n                    value = node\n                elif unicode(node) or (fn == str and unicode(node) != ''):\n                    try:\n                        # get special deserialization function (if any)\n                        fn = TYPE_UNMARSHAL_FN.get(fn, fn)\n                        if fn == str:\n                            # always return an unicode object:\n                            # (avoid encoding errors in py<3!)\n                            value = unicode(node)\n                        else:\n                            value = fn(unicode(node))\n                    except (ValueError, TypeError) as e:\n                        raise ValueError(\"Tag: %s: %s\" % (name, e))\n                else:\n                    value = None\n            d[name] = value\n        return d\n\n    def _update_ns(self, name):\n        \"\"\"Replace the defined namespace alias with tohse used by the client.\"\"\"\n        pref = self.__ns_rx.search(name)\n        if pref:\n            pref = pref.groups()[0]\n            try:\n                name = name.replace(pref, self.__namespaces_map[pref])\n            except KeyError:\n                log.warning('Unknown namespace alias %s' % name)\n        return name\n\n    def marshall(self, name, value, add_child=True, add_comments=False,\n                 ns=False, add_children_ns=True):\n        \"\"\"Analyze python value and add the serialized XML element using tag name\"\"\"\n        # Change node name to that used by a client\n        name = self._update_ns(name)\n\n        if isinstance(value, dict):  # serialize dict (<key>value</key>)\n            # for the first parent node, use the document target namespace\n            # (ns==True) or use the namespace string uri if passed (elements)\n            child = add_child and self.add_child(name, ns=ns) or self\n            for k, v in value.items():\n                if not add_children_ns:\n                    ns = False\n                elif hasattr(value, 'namespaces'):\n                    # for children, use the wsdl element target namespace:\n                    ns = value.namespaces.get(k)\n                else:\n                    # simple type\n                    ns = None\n                child.marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, tuple):  # serialize tuple (<key>value</key>)\n            child = add_child and self.add_child(name, ns=ns) or self\n            if not add_children_ns:\n                ns = False\n            for k, v in value:\n                getattr(self, name).marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, list):  # serialize lists\n            child = self.add_child(name, ns=ns)\n            if not add_children_ns:\n                ns = False\n            if add_comments:\n                child.add_comment(\"Repetitive array of:\")\n            for t in value:\n                child.marshall(name, t, False, add_comments=add_comments, ns=ns)\n        elif isinstance(value, (xml.dom.minidom.CDATASection, basestring)):  # do not convert strings or unicodes\n            self.add_child(name, value, ns=ns)\n        elif value is None:  # sent a empty tag?\n            self.add_child(name, ns=ns)\n        elif value in TYPE_MAP.keys():\n            # add commented placeholders for simple tipes (for examples/help only)\n            child = self.add_child(name, ns=ns)\n            child.add_comment(TYPE_MAP[value])\n        else:  # the rest of object types are converted to string\n            # get special serialization function (if any)\n            fn = TYPE_MARSHAL_FN.get(type(value), str)\n            self.add_child(name, fn(value), ns=ns)\n\n    def import_node(self, other):\n        x = self.__document.importNode(other._element, True)  # deep copy\n        self._element.appendChild(x)\n\n    def write_c14n(self, output=None, exclusive=True):\n        \"Generate the canonical version of the XML node\"\n        from . import c14n\n        xml = c14n.Canonicalize(self._element, output,\n                                unsuppressedPrefixes=[] if exclusive else None)\n        return xml\n", "test_list": ["def test_marshall_cdata(self):\n    span = SimpleXMLElement('<span/>')\n    cdata = CDATASection()\n    cdata.data = 'python'\n    span.add_child('a', cdata)\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a><![CDATA[python]]></a></span>'\n    self.eq(span.as_xml(), xml if PY2 else xml.encode('utf-8'))", "def test_to_xml(self):\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a href=\"python.org.ar\">pyar</a><prueba><i>1</i><float>1.5</float></prueba></span>'\n    self.eq(SimpleXMLElement(xml).as_xml(), xml if PY2 else xml.encode('utf-8'))\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a href=\"google.com\">google</a><a>yahoo</a><a>hotmail</a></span>'\n    self.eq(SimpleXMLElement(xml).as_xml(), xml if PY2 else xml.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'as_xml' function should correctly return the XML representation of the document as a string. If the 'pretty' parameter is set to True, the XML should be formatted with indentation and line breaks.", "unit_test": "def test_as_xml_pretty_formatting():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    pretty_xml = '<?xml version=\"1.0\" ?>\\n<root>\\n  <child>value</child>\\n</root>\\n'\n    assert element.as_xml(pretty=True) == pretty_xml", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_pretty_formatting"}, "Exception Handling": {"requirement": "The 'as_xml' function should raise a ValueError if the XML document is malformed or cannot be parsed.", "unit_test": "def test_as_xml_malformed_xml():\n    malformed_xml = '<root><child>value</child>'  # Missing closing tag\n    try:\n        element = SimpleXMLElement(malformed_xml)\n        element.as_xml()\n    except ValueError as e:\n        assert str(e) == 'Malformed XML document'", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_malformed_xml"}, "Edge Case Handling": {"requirement": "The 'as_xml' function should handle empty XML documents gracefully and return an empty string.", "unit_test": "def test_as_xml_empty_document():\n    empty_xml = ''\n    element = SimpleXMLElement(empty_xml)\n    assert element.as_xml() == ''", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_empty_document"}, "Functionality Extension": {"requirement": "Extend the 'as_xml' function to accept an optional 'encoding' parameter that specifies the character encoding of the XML output.", "unit_test": "def test_as_xml_with_encoding():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    encoded_xml = '<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?><root><child>value</child></root>'\n    assert element.as_xml(encoding='ISO-8859-1') == encoded_xml", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_with_encoding"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'as_xml' function are annotated with type hints.", "unit_test": "def test_as_xml_annotations():\n    from typing import get_type_hints\n    hints = get_type_hints(SimpleXMLElement.as_xml)\n    assert hints == {'pretty': bool, 'filename': str, 'return': str}", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_annotations"}, "Code Standard": {"requirement": "The 'as_xml' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_as_xml_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/your/module.py'])\n    assert result.total_errors == 0", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'as_xml' function should utilize the '__document' attribute of the SimpleXMLElement class to generate the XML representation.", "unit_test": "def test_as_xml_uses_document():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    assert hasattr(element, '_SimpleXMLElement__document')\n    assert element.as_xml() == element._SimpleXMLElement__document.toxml()", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_uses_document"}, "Context Usage Correctness Verification": {"requirement": "The 'as_xml' function should correctly use the '__document' attribute to ensure the XML representation is accurate and complete.", "unit_test": "def test_as_xml_correct_document_usage():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    assert element.as_xml() == element._SimpleXMLElement__document.toxml()", "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_correct_document_usage"}}}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "type": "method", "project_path": "Multimedia/mingus", "completion_path": "Multimedia/mingus/mingus/containers/note_container.py", "signature_position": [293, 293], "body_position": [298, 302], "dependency": {"intra_class": ["mingus.containers.note_container.NoteContainer.notes"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a list of unique note names from the current note container.\n", "Arguments": ":param self: NoteContainer, an instance of the NoteContainer class.\n:return: List. A list containing the unique note names from the current note container.\n"}, "tests": ["tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names"], "indent": 4, "domain": "Multimedia", "code": "    def get_note_names(self):\n        \"\"\"Return a list with all the note names in the current container.\n\n        Every name will only be mentioned once.\n        \"\"\"\n        res = []\n        for n in self.notes:\n            if n.name not in res:\n                res.append(n.name)\n        return res\n", "context": "class NoteContainer(object):\n\n    \"\"\"A container for notes.\n\n    The NoteContainer provides a container for the mingus.containers.Note\n    objects.\n\n    It can be used to store single and multiple notes and is required for\n    working with Bars.\n    \"\"\"\n\n    notes = []\n\n    def __init__(self, notes=None):\n        if notes is None:\n            notes = []\n        self.empty()\n        self.add_notes(notes)\n\n    def empty(self):\n        \"\"\"Empty the container.\"\"\"\n        self.notes = []\n\n    def add_note(self, note, octave=None, dynamics=None):\n        \"\"\"Add a note to the container and sorts the notes from low to high.\n\n        The note can either be a string, in which case you could also use\n        the octave and dynamics arguments, or a Note object.\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n        if isinstance(note, six.string_types):\n            if octave is not None:\n                note = Note(note, octave, dynamics)\n            elif len(self.notes) == 0:\n                note = Note(note, 4, dynamics)\n            else:\n                if Note(note, self.notes[-1].octave) < self.notes[-1]:\n                    note = Note(note, self.notes[-1].octave + 1, dynamics)\n                else:\n                    note = Note(note, self.notes[-1].octave, dynamics)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % note\n            )\n        if note not in self.notes:\n            self.notes.append(note)\n            self.notes.sort()\n        return self.notes\n\n    def add_notes(self, notes):\n        \"\"\"Feed notes to self.add_note.\n\n        The notes can either be an other NoteContainer, a list of Note\n        objects or strings or a list of lists formatted like this:\n        >>> notes = [['C', 5], ['E', 5], ['G', 6]]\n\n        or even:\n        >>> notes = [['C', 5, {'velocity': 20}], ['E', 6, {'velocity': 20}]]\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            for x in notes.notes:\n                self.add_note(x)\n            return self.notes\n        elif hasattr(notes, \"name\"):\n            self.add_note(notes)\n            return self.notes\n        elif isinstance(notes, six.string_types):\n            self.add_note(notes)\n            return self.notes\n        for x in notes:\n            if isinstance(x, list) and len(x) != 1:\n                if len(x) == 2:\n                    self.add_note(x[0], x[1])\n                else:\n                    self.add_note(x[0], x[1], x[2])\n            else:\n                self.add_note(x)\n        return self.notes\n\n    def from_chord(self, shorthand):\n        \"\"\"Shortcut to from_chord_shorthand.\"\"\"\n        return self.from_chord_shorthand(shorthand)\n\n    def from_chord_shorthand(self, shorthand):\n        \"\"\"Empty the container and add the notes in the shorthand.\n\n        See mingus.core.chords.from_shorthand for an up to date list of\n        recognized format.\n\n        Example:\n        >>> NoteContainer().from_chord_shorthand('Am')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n\n    def from_interval(self, startnote, shorthand, up=True):\n        \"\"\"Shortcut to from_interval_shorthand.\"\"\"\n        return self.from_interval_shorthand(startnote, shorthand, up)\n\n    def from_interval_shorthand(self, startnote, shorthand, up=True):\n        \"\"\"Empty the container and add the note described in the startnote and\n        shorthand.\n\n        See core.intervals for the recognized format.\n\n        Examples:\n        >>> nc = NoteContainer()\n        >>> nc.from_interval_shorthand('C', '5')\n        ['C-4', 'G-4']\n        >>> nc.from_interval_shorthand('C', '5', False)\n        ['F-3', 'C-4']\n        \"\"\"\n        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self\n\n    def from_progression(self, shorthand, key=\"C\"):\n        \"\"\"Shortcut to from_progression_shorthand.\"\"\"\n        return self.from_progression_shorthand(shorthand, key)\n\n    def from_progression_shorthand(self, shorthand, key=\"C\"):\n        \"\"\"Empty the container and add the notes described in the progressions\n        shorthand (eg. 'IIm6', 'V7', etc).\n\n        See mingus.core.progressions for all the recognized format.\n\n        Example:\n        >>> NoteContainer().from_progression_shorthand('VI')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self\n\n    def _consonance_test(self, testfunc, param=None):\n        \"\"\"Private function used for testing consonance/dissonance.\"\"\"\n        n = list(self.notes)\n        while len(n) > 1:\n            first = n[0]\n            for second in n[1:]:\n                if param is None:\n                    if not testfunc(first.name, second.name):\n                        return False\n                else:\n                    if not testfunc(first.name, second.name, param):\n                        return False\n            n = n[1:]\n        return True\n\n    def is_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_consonant, include_fourths)\n\n    def is_perfect_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are perfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_perfect_consonant, include_fourths)\n\n    def is_imperfect_consonant(self):\n        \"\"\"Test whether the notes are imperfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_imperfect_consonant)\n\n    def is_dissonant(self, include_fourths=False):\n        \"\"\"Test whether the notes are dissonants.\n\n        See the core.intervals module for a longer description.\n        \"\"\"\n        return not self.is_consonant(not include_fourths)\n\n    def remove_note(self, note, octave=-1):\n        \"\"\"Remove note from container.\n\n        The note can either be a Note object or a string representing the\n        note's name. If no specific octave is given, the note gets removed\n        in every octave.\n        \"\"\"\n        res = []\n        for x in self.notes:\n            if isinstance(note, six.string_types):\n                if x.name != note:\n                    res.append(x)\n                else:\n                    if x.octave != octave and octave != -1:\n                        res.append(x)\n            else:\n                if x != note:\n                    res.append(x)\n        self.notes = res\n        return res\n\n    def remove_notes(self, notes):\n        \"\"\"Remove notes from the containers.\n\n        This function accepts a list of Note objects or notes as strings and\n        also single strings or Note objects.\n        \"\"\"\n        if isinstance(notes, six.string_types):\n            return self.remove_note(notes)\n        elif hasattr(notes, \"name\"):\n            return self.remove_note(notes)\n        else:\n            for x in notes:\n                self.remove_note(x)\n            return self.notes\n\n    def remove_duplicate_notes(self):\n        \"\"\"Remove duplicate and enharmonic notes from the container.\"\"\"\n        res = []\n        for x in self.notes:\n            if x not in res:\n                res.append(x)\n        self.notes = res\n        return res\n\n    def sort(self):\n        \"\"\"Sort the notes in the container from low to high.\"\"\"\n        self.notes.sort()\n\n    def augment(self):\n        \"\"\"Augment all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.augment()\n\n    def diminish(self):\n        \"\"\"Diminish all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.diminish()\n\n    def determine(self, shorthand=False):\n        \"\"\"Determine the type of chord or interval currently in the\n        container.\"\"\"\n        return chords.determine(self.get_note_names(), shorthand)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose all the notes in the container up or down the given\n        interval.\"\"\"\n        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n\n###The function: get_note_names###\n    def __repr__(self):\n        \"\"\"Return a nice and clean string representing the note container.\"\"\"\n        return str(self.notes)\n\n    def __getitem__(self, item):\n        \"\"\"Enable the use of the container as a simple array.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0]\n        'C-4'\n        \"\"\"\n        return self.notes[item]\n\n    def __setitem__(self, item, value):\n        \"\"\"Enable the use of the [] notation on NoteContainers.\n\n        This function accepts Notes and notes as string.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0] = 'B'\n        >>> n\n        ['B-4', 'E-4', 'G-4']\n        \"\"\"\n        if isinstance(value, six.string_types):\n            n = Note(value)\n            self.notes[item] = n\n        else:\n            self.notes[item] = value\n        return self.notes\n\n    def __add__(self, notes):\n        \"\"\"Enable the use of the '+' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n + 'B'\n        ['C-4', 'E-4', 'G-4', 'B-4']\n        \"\"\"\n        self.add_notes(notes)\n        return self\n\n    def __sub__(self, notes):\n        \"\"\"Enable the use of the '-' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n - 'E'\n        ['C-4', 'G-4']\n        \"\"\"\n        self.remove_notes(notes)\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of notes in the container.\"\"\"\n        return len(self.notes)\n\n    def __eq__(self, other):\n        \"\"\"Enable the '==' operator for NoteContainer instances.\"\"\"\n        for x in self:\n            if x not in other:\n                return False\n        return True\n", "prompt": "Please write a python function called 'get_note_names' base the context. This function returns a list of unique note names from the current note container.\n:param self: NoteContainer, an instance of the NoteContainer class.\n:return: List. A list containing the unique note names from the current note container.\n.\n        The context you need to refer to is as follows: class NoteContainer(object):\n\n    \"\"\"A container for notes.\n\n    The NoteContainer provides a container for the mingus.containers.Note\n    objects.\n\n    It can be used to store single and multiple notes and is required for\n    working with Bars.\n    \"\"\"\n\n    notes = []\n\n    def __init__(self, notes=None):\n        if notes is None:\n            notes = []\n        self.empty()\n        self.add_notes(notes)\n\n    def empty(self):\n        \"\"\"Empty the container.\"\"\"\n        self.notes = []\n\n    def add_note(self, note, octave=None, dynamics=None):\n        \"\"\"Add a note to the container and sorts the notes from low to high.\n\n        The note can either be a string, in which case you could also use\n        the octave and dynamics arguments, or a Note object.\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n        if isinstance(note, six.string_types):\n            if octave is not None:\n                note = Note(note, octave, dynamics)\n            elif len(self.notes) == 0:\n                note = Note(note, 4, dynamics)\n            else:\n                if Note(note, self.notes[-1].octave) < self.notes[-1]:\n                    note = Note(note, self.notes[-1].octave + 1, dynamics)\n                else:\n                    note = Note(note, self.notes[-1].octave, dynamics)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % note\n            )\n        if note not in self.notes:\n            self.notes.append(note)\n            self.notes.sort()\n        return self.notes\n\n    def add_notes(self, notes):\n        \"\"\"Feed notes to self.add_note.\n\n        The notes can either be an other NoteContainer, a list of Note\n        objects or strings or a list of lists formatted like this:\n        >>> notes = [['C', 5], ['E', 5], ['G', 6]]\n\n        or even:\n        >>> notes = [['C', 5, {'velocity': 20}], ['E', 6, {'velocity': 20}]]\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            for x in notes.notes:\n                self.add_note(x)\n            return self.notes\n        elif hasattr(notes, \"name\"):\n            self.add_note(notes)\n            return self.notes\n        elif isinstance(notes, six.string_types):\n            self.add_note(notes)\n            return self.notes\n        for x in notes:\n            if isinstance(x, list) and len(x) != 1:\n                if len(x) == 2:\n                    self.add_note(x[0], x[1])\n                else:\n                    self.add_note(x[0], x[1], x[2])\n            else:\n                self.add_note(x)\n        return self.notes\n\n    def from_chord(self, shorthand):\n        \"\"\"Shortcut to from_chord_shorthand.\"\"\"\n        return self.from_chord_shorthand(shorthand)\n\n    def from_chord_shorthand(self, shorthand):\n        \"\"\"Empty the container and add the notes in the shorthand.\n\n        See mingus.core.chords.from_shorthand for an up to date list of\n        recognized format.\n\n        Example:\n        >>> NoteContainer().from_chord_shorthand('Am')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n\n    def from_interval(self, startnote, shorthand, up=True):\n        \"\"\"Shortcut to from_interval_shorthand.\"\"\"\n        return self.from_interval_shorthand(startnote, shorthand, up)\n\n    def from_interval_shorthand(self, startnote, shorthand, up=True):\n        \"\"\"Empty the container and add the note described in the startnote and\n        shorthand.\n\n        See core.intervals for the recognized format.\n\n        Examples:\n        >>> nc = NoteContainer()\n        >>> nc.from_interval_shorthand('C', '5')\n        ['C-4', 'G-4']\n        >>> nc.from_interval_shorthand('C', '5', False)\n        ['F-3', 'C-4']\n        \"\"\"\n        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self\n\n    def from_progression(self, shorthand, key=\"C\"):\n        \"\"\"Shortcut to from_progression_shorthand.\"\"\"\n        return self.from_progression_shorthand(shorthand, key)\n\n    def from_progression_shorthand(self, shorthand, key=\"C\"):\n        \"\"\"Empty the container and add the notes described in the progressions\n        shorthand (eg. 'IIm6', 'V7', etc).\n\n        See mingus.core.progressions for all the recognized format.\n\n        Example:\n        >>> NoteContainer().from_progression_shorthand('VI')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self\n\n    def _consonance_test(self, testfunc, param=None):\n        \"\"\"Private function used for testing consonance/dissonance.\"\"\"\n        n = list(self.notes)\n        while len(n) > 1:\n            first = n[0]\n            for second in n[1:]:\n                if param is None:\n                    if not testfunc(first.name, second.name):\n                        return False\n                else:\n                    if not testfunc(first.name, second.name, param):\n                        return False\n            n = n[1:]\n        return True\n\n    def is_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_consonant, include_fourths)\n\n    def is_perfect_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are perfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_perfect_consonant, include_fourths)\n\n    def is_imperfect_consonant(self):\n        \"\"\"Test whether the notes are imperfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_imperfect_consonant)\n\n    def is_dissonant(self, include_fourths=False):\n        \"\"\"Test whether the notes are dissonants.\n\n        See the core.intervals module for a longer description.\n        \"\"\"\n        return not self.is_consonant(not include_fourths)\n\n    def remove_note(self, note, octave=-1):\n        \"\"\"Remove note from container.\n\n        The note can either be a Note object or a string representing the\n        note's name. If no specific octave is given, the note gets removed\n        in every octave.\n        \"\"\"\n        res = []\n        for x in self.notes:\n            if isinstance(note, six.string_types):\n                if x.name != note:\n                    res.append(x)\n                else:\n                    if x.octave != octave and octave != -1:\n                        res.append(x)\n            else:\n                if x != note:\n                    res.append(x)\n        self.notes = res\n        return res\n\n    def remove_notes(self, notes):\n        \"\"\"Remove notes from the containers.\n\n        This function accepts a list of Note objects or notes as strings and\n        also single strings or Note objects.\n        \"\"\"\n        if isinstance(notes, six.string_types):\n            return self.remove_note(notes)\n        elif hasattr(notes, \"name\"):\n            return self.remove_note(notes)\n        else:\n            for x in notes:\n                self.remove_note(x)\n            return self.notes\n\n    def remove_duplicate_notes(self):\n        \"\"\"Remove duplicate and enharmonic notes from the container.\"\"\"\n        res = []\n        for x in self.notes:\n            if x not in res:\n                res.append(x)\n        self.notes = res\n        return res\n\n    def sort(self):\n        \"\"\"Sort the notes in the container from low to high.\"\"\"\n        self.notes.sort()\n\n    def augment(self):\n        \"\"\"Augment all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.augment()\n\n    def diminish(self):\n        \"\"\"Diminish all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.diminish()\n\n    def determine(self, shorthand=False):\n        \"\"\"Determine the type of chord or interval currently in the\n        container.\"\"\"\n        return chords.determine(self.get_note_names(), shorthand)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose all the notes in the container up or down the given\n        interval.\"\"\"\n        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n\n###The function: get_note_names###\n    def __repr__(self):\n        \"\"\"Return a nice and clean string representing the note container.\"\"\"\n        return str(self.notes)\n\n    def __getitem__(self, item):\n        \"\"\"Enable the use of the container as a simple array.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0]\n        'C-4'\n        \"\"\"\n        return self.notes[item]\n\n    def __setitem__(self, item, value):\n        \"\"\"Enable the use of the [] notation on NoteContainers.\n\n        This function accepts Notes and notes as string.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0] = 'B'\n        >>> n\n        ['B-4', 'E-4', 'G-4']\n        \"\"\"\n        if isinstance(value, six.string_types):\n            n = Note(value)\n            self.notes[item] = n\n        else:\n            self.notes[item] = value\n        return self.notes\n\n    def __add__(self, notes):\n        \"\"\"Enable the use of the '+' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n + 'B'\n        ['C-4', 'E-4', 'G-4', 'B-4']\n        \"\"\"\n        self.add_notes(notes)\n        return self\n\n    def __sub__(self, notes):\n        \"\"\"Enable the use of the '-' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n - 'E'\n        ['C-4', 'G-4']\n        \"\"\"\n        self.remove_notes(notes)\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of notes in the container.\"\"\"\n        return len(self.notes)\n\n    def __eq__(self, other):\n        \"\"\"Enable the '==' operator for NoteContainer instances.\"\"\"\n        for x in self:\n            if x not in other:\n                return False\n        return True\n", "test_list": ["def test_get_note_names(self):\n    self.assertEqual(['A', 'C', 'E'], self.n3.get_note_names())\n    self.assertEqual(['A', 'C', 'E', 'F', 'G'], self.n4.get_note_names())\n    self.assertEqual(['A', 'C', 'E', 'F', 'G'], self.n5.get_note_names())"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'get_note_names' should return a list of unique note names as strings, ensuring no duplicates and maintaining the order of first appearance.", "unit_test": "def test_get_note_names_unique(self):\n    self.n6 = NoteContainer(['A', 'C', 'A', 'E'])\n    self.assertEqual(['A', 'C', 'E'], self.n6.get_note_names())", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_unique"}, "Exception Handling": {"requirement": "The function 'get_note_names' should handle cases where the notes attribute is not a list and raise a TypeError with a descriptive message.", "unit_test": "def test_get_note_names_type_error(self):\n    self.n7 = NoteContainer()\n    self.n7.notes = 'Not a list'\n    with self.assertRaises(TypeError):\n        self.n7.get_note_names()", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_type_error"}, "Edge Case Handling": {"requirement": "The function 'get_note_names' should return an empty list when the NoteContainer is empty.", "unit_test": "def test_get_note_names_empty_container(self):\n    self.n8 = NoteContainer()\n    self.assertEqual([], self.n8.get_note_names())", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_empty_container"}, "Functionality Extension": {"requirement": "Extend the 'get_note_names' function to accept an optional parameter 'sort' that, when set to True, returns the note names sorted alphabetically.", "unit_test": "def test_get_note_names_sorted(self):\n    self.n9 = NoteContainer(['E', 'C', 'A'])\n    self.assertEqual(['A', 'C', 'E'], self.n9.get_note_names(sort=True))", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_sorted"}, "Annotation Coverage": {"requirement": "Ensure that the 'get_note_names' function has complete type annotations for parameters and return types.", "unit_test": "def test_get_note_names_annotations(self):\n    annotations = self.n10.get_note_names.__annotations__\n    self.assertEqual(annotations['return'], list)\n    self.assertEqual(annotations.get('sort', None), bool)", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_annotations"}, "Code Complexity": {"requirement": "The 'get_note_names' function should have a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": "def test_get_note_names_complexity(self):\n    complexity = calculate_cyclomatic_complexity(self.n11.get_note_names)\n    self.assertEqual(complexity, 1)", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_correct_extraction"}, "Code Standard": {"requirement": "The 'get_note_names' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_get_note_names_pep8(self):\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['note_container.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'get_note_names' function should utilize the 'notes' attribute from the NoteContainer class to retrieve note names.", "unit_test": "def test_get_note_names_context_usage(self):\n    self.n12 = NoteContainer(['A', 'B', 'C'])\n    self.assertIn('notes', dir(self.n12))\n    self.assertEqual(['A', 'B', 'C'], self.n12.get_note_names())", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'get_note_names' function should correctly extract the 'name' attribute from each Note object in the 'notes' list.", "unit_test": "def test_get_note_names_correct_extraction(self):\n    self.n13 = NoteContainer([Note('A'), Note('B'), Note('C')])\n    self.assertEqual(['A', 'B', 'C'], self.n13.get_note_names())", "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_correct_extraction"}}}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "type": "method", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/en_spell.py", "signature_position": [99, 99], "body_position": [106, 109], "dependency": {"intra_class": ["pycorrector.en_spell.EnSpell.candidates", "pycorrector.en_spell.EnSpell.check_init"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function corrects the spelling of a given word by finding the most probable spelling correction. It first checks if the EnSpell instance has been initialized. Then, it calculates the probability of each candidate spelling correction for the word and sorts them in ascending order. Finally, it returns the correction with the highest probability.", "Arguments": ":param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word to be corrected.\n:return: String. The most probable spelling correction for the word."}, "tests": ["tests/en_spell_bug_fix_test.py::EnBugTestCase::test_en_bug_correct2"], "indent": 4, "domain": "Text-Processing", "code": "    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n", "context": "class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n###The function: correct_word###\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "prompt": "Please write a python function called 'correct_word' base the context. This function corrects the spelling of a given word by finding the most probable spelling correction. It first checks if the EnSpell instance has been initialized. Then, it calculates the probability of each candidate spelling correction for the word and sorts them in ascending order. Finally, it returns the correction with the highest probability.:param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word to be corrected.\n:return: String. The most probable spelling correction for the word..\n        The context you need to refer to is as follows: class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n###The function: correct_word###\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "test_list": ["def test_en_bug_correct2(self):\n    \"\"\"\u6d4b\u8bd5\u82f1\u6587\u7ea0\u9519bug\"\"\"\n    print(spell.word_freq_dict.get('whould'))\n    print(spell.candidates('whould'))\n    a = spell.correct_word('whould')\n    print(a)\n    r = spell.correct('contend proble poety adress whould niether  quaties')\n    print(r)\n    assert spell.correct('whould')[0] == 'would'"], "requirements": {"Input-Output Conditions": {"requirement": "The 'correct_word' function should accept a string as input and return a string as output.", "unit_test": "def test_correct_word_input_output():\n    assert isinstance(spell.correct_word('example'), str)\n    try:\n        spell.correct_word(123)\n    except TypeError:\n        assert True\n    else:\n        assert False", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_input_output"}, "Exception Handling": {"requirement": "The 'correct_word' function should raise a ValueError with a descriptive message: 'Input word cannot be an empty string.' if the input word is an empty string.", "unit_test": "def test_correct_word_exception_handling():\n    try:\n        spell.correct_word('')\n    except ValueError as e:\n        assert str(e) == 'Input word cannot be an empty string.'\n    else:\n        assert False", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_exception_handling"}, "Edge Case Handling": {"requirement": "The 'correct_word' function should handle edge cases such as very short words (e.g., single-letter words) and return them unchanged if no correction is found.", "unit_test": "def test_correct_word_edge_cases():\n    assert spell.correct_word('a') == 'a'\n    assert spell.correct_word('I') == 'I'", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'correct_word' function to accept an optional parameter 'max_candidates' to limit the number of candidate corrections considered.", "unit_test": "def test_correct_word_functionality_extension():\n    assert len(spell.candidates('whould')) > 1\n    assert len(spell.candidates('whould', max_candidates=1)) == 1", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'correct_word' function has complete parameter and return type annotations.", "unit_test": "def test_correct_word_annotation_coverage():\n    assert 'word: str' in spell.correct_word.__annotations__\n    assert 'return: str' in spell.correct_word.__annotations__", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_annotation_coverage"}, "Code Complexity": {"requirement": "The 'correct_word' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": "def test_correct_word_code_complexity():\n    from radon.complexity import cc_visit\n    complexity = cc_visit(spell.correct_word)\n    assert complexity[0].complexity <= 5", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_code_complexity"}, "Code Standard": {"requirement": "The 'correct_word' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_correct_word_code_standard():\n    import subprocess\n    result = subprocess.run(['flake8', 'path_to_correct_word_function.py'], capture_output=True, text=True)\n    assert result.stdout == ''", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'correct_word' function utilizes the 'candidates' method from the EnSpell class to generate possible corrections.", "unit_test": "def test_correct_word_context_usage():\n    original_candidates = spell.candidates\n    spell.candidates = lambda x: {'mock_candidate'}\n    assert spell.correct_word('whould') == 'mock_candidate'\n    spell.candidates = original_candidates", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'correct_word' function correctly uses the 'check_init' method to initialize the word frequency dictionary if needed.", "unit_test": "def test_correct_word_context_usage_correctness():\n    spell.word_freq_dict = {}\n    spell.correct_word('whould')\n    assert spell.word_freq_dict != {}", "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_context_usage_correctness"}}}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "type": "method", "project_path": "Software-Development/dash", "completion_path": "Software-Development/dash/dash/development/base_component.py", "signature_position": [203, 204], "body_position": [205, 228], "dependency": {"intra_class": ["dash.development.base_component.Component._namespace", "dash.development.base_component.Component._prop_names", "dash.development.base_component.Component._type"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function converts a Component instance into a JSON object that can be used by Plotly. It extracts the normal properties of the Component instance and adds them to the JSON object. It also adds any wildcard properties (properties starting with \"data-\" or \"aria-\") to the JSON object. Finally, it includes the properties, type and namespace of the Component instance in the JSON object.", "Arguments": ":param self: Component. An instance of the Component class.\n:return: JSON. The JSON representation of the Component instance."}, "tests": ["tests/unit/development/test_base_component.py::test_debc022_to_plotly_json_with_children", "tests/unit/development/test_base_component.py::test_debc012_to_plotly_json_full_tree", "tests/unit/development/test_base_component.py::test_debc020_to_plotly_json_without_children", "tests/unit/development/test_base_component.py::test_debc023_to_plotly_json_with_wildcards", "tests/unit/development/test_base_component.py::test_debc021_to_plotly_json_with_null_arguments"], "indent": 4, "domain": "Software-Development", "code": "    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n", "context": "class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n###The function: to_plotly_json###\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "prompt": "Please write a python function called 'to_plotly_json' base the context. This function converts a Component instance into a JSON object that can be used by Plotly. It extracts the normal properties of the Component instance and adds them to the JSON object. It also adds any wildcard properties (properties starting with \"data-\" or \"aria-\") to the JSON object. Finally, it includes the properties, type and namespace of the Component instance in the JSON object.:param self: Component. An instance of the Component class.\n:return: JSON. The JSON representation of the Component instance..\n        The context you need to refer to is as follows: class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n###The function: to_plotly_json###\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "test_list": ["def test_debc022_to_plotly_json_with_children():\n    c = Component(id='a', children='Hello World')\n    c._prop_names = ('id', 'children')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a', 'children': 'Hello World'}, 'type': 'MyComponent'}", "def test_debc012_to_plotly_json_full_tree():\n    c = nested_tree()[0]\n    Component._namespace\n    Component._type\n    expected = {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': [{'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'id': '0.0'}}, {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': [10, None, 'wrap string', {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': 'string', 'id': '0.1.x.x.0'}}, 'another string', 4.51], 'id': '0.1.x.x'}}, 'id': '0.1.x'}}, 'id': '0.1'}}], 'id': '0'}}\n    res = json.loads(json.dumps(c.to_plotly_json(), cls=plotly.utils.PlotlyJSONEncoder))\n    assert res == expected", "def test_debc020_to_plotly_json_without_children():\n    c = Component(id='a')\n    c._prop_names = ('id',)\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a'}, 'type': 'MyComponent'}", "def test_debc023_to_plotly_json_with_wildcards():\n    c = Component(id='a', **{'aria-expanded': 'true', 'data-toggle': 'toggled', 'data-none': None})\n    c._prop_names = ('id',)\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'aria-expanded': 'true', 'data-toggle': 'toggled', 'data-none': None, 'id': 'a'}, 'type': 'MyComponent'}", "def test_debc021_to_plotly_json_with_null_arguments():\n    c = Component(id='a')\n    c._prop_names = ('id', 'style')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a'}, 'type': 'MyComponent'}\n    c = Component(id='a', style=None)\n    c._prop_names = ('id', 'style')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a', 'style': None}, 'type': 'MyComponent'}"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_plotly_json' function should correctly convert a Component instance to a JSON object, ensuring that all properties, including wildcard properties, are accurately represented in the output.", "unit_test": "def test_to_plotly_json_output_structure():\n    c = Component(id='test', **{'data-test': 'value'})\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert 'namespace' in result\n    assert 'props' in result\n    assert 'type' in result\n    assert result['props']['id'] == 'test'\n    assert result['props']['data-test'] == 'value'", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_output_structure"}, "Exception Handling": {"requirement": "The 'to_plotly_json' function should raise a TypeError if a Component instance has properties not defined in '_prop_names' or not matching wildcard attributes.", "unit_test": "def test_to_plotly_json_unexpected_property():\n    c = Component(id='test', unexpected_prop='value')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    try:\n        c.to_plotly_json()\n    except TypeError as e:\n        assert 'unexpected keyword argument' in str(e)", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_unexpected_property"}, "Edge Case Handling": {"requirement": "The 'to_plotly_json' function should handle cases where the Component instance has no properties set, returning a JSON object with only 'type' and 'namespace'.", "unit_test": "def test_to_plotly_json_no_properties():\n    c = Component()\n    c._prop_names = ()\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result == {'namespace': 'test_namespace', 'props': {}, 'type': 'TestComponent'}", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_no_properties"}, "Functionality Extension": {"requirement": "Extend the 'to_plotly_json' function to include a 'version' key in the JSON object, representing the version of the component's namespace.", "unit_test": "def test_to_plotly_json_with_version():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    c._version = '1.0.0'\n    result = c.to_plotly_json()\n    assert result['version'] == '1.0.0'", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_with_version"}, "Annotation Coverage": {"requirement": "Ensure that the 'to_plotly_json' function includes comprehensive docstrings and type annotations for all parameters and return types.", "unit_test": "def test_to_plotly_json_annotations():\n    import inspect\n    annotations = inspect.getfullargspec(Component.to_plotly_json).annotations\n    assert 'self' in annotations\n    assert annotations['return'] == dict", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_annotations"}, "Code Complexity": {"requirement": "The 'to_plotly_json' function should maintain a cyclomatic complexity of 10 or lower to ensure readability and maintainability.", "unit_test": "def test_to_plotly_json_complexity():\n    from radon.complexity import cc_visit\n    with open('component.py', 'r') as file:\n        code = file.read()\n    complexity = cc_visit(code)\n    for func in complexity:\n        if func.name == 'to_plotly_json':\n            assert func.complexity <= 10", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_complexity"}, "Code Standard": {"requirement": "The 'to_plotly_json' function should adhere to PEP 8 standards, ensuring proper formatting and style.", "unit_test": "def test_to_plotly_json_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['component.py'])\n    assert result.total_errors == 0", "test": "tests/unit/development/test_base_component.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'to_plotly_json' function should utilize the '_namespace', '_prop_names', and '_type' attributes of the Component class.", "unit_test": "def test_to_plotly_json_context_usage():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result['namespace'] == 'test_namespace'\n    assert result['type'] == 'TestComponent'", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'to_plotly_json' function correctly uses the '_namespace', '_prop_names', and '_type' attributes to construct the JSON object.", "unit_test": "def test_to_plotly_json_context_correctness():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result['namespace'] == c._namespace\n    assert result['type'] == c._type", "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_context_correctness"}}}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "type": "method", "project_path": "Scientific-Engineering/lux", "completion_path": "Scientific-Engineering/lux/lux/vis/Vis.py", "signature_position": [142, 142], "body_position": [143, 149], "dependency": {"intra_class": ["lux.vis.Vis.Vis._inferred_intent"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.", "Arguments": ":param self: Vis. An instance of the Vis class.\n:param channel: The channel to filter the inferred intent list.\n:return: List. The filtered list of objects from the inferred intent list."}, "tests": ["tests/test_dates.py::test_refresh_inplace", "tests/test_columns.py::test_special_char"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def get_attr_by_channel(self, channel):\n        spec_obj = list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\" if hasattr(x, \"channel\") else False,\n                self._inferred_intent,\n            )\n        )\n        return spec_obj\n", "context": "class Vis:\n    \"\"\"\n    Vis Object represents a collection of fully fleshed out specifications required for data fetching and visualization.\n    \"\"\"\n\n    def __init__(self, intent, source=None, title=\"\", score=0.0):\n        self._intent = intent  # user's original intent to Vis\n        self._inferred_intent = intent  # re-written, expanded version of user's original intent\n        self._source = source  # original data attached to the Vis\n        self._vis_data = None  # processed data for Vis (e.g., selected, aggregated, binned)\n        self._code = None\n        self._mark = \"\"\n        self._min_max = {}\n        self._postbin = None\n        self.title = title\n        self.score = score\n        self._all_column = False\n        self.approx = False\n        self.refresh_source(self._source)\n\n    def __repr__(self):\n        all_clause = all([isinstance(unit, lux.Clause) for unit in self._inferred_intent])\n        if all_clause:\n            filter_intents = None\n            channels, additional_channels = [], []\n            for clause in self._inferred_intent:\n\n                if hasattr(clause, \"value\"):\n                    if clause.value != \"\":\n                        filter_intents = clause\n                if hasattr(clause, \"attribute\"):\n                    if clause.attribute != \"\":\n                        if clause.aggregation != \"\" and clause.aggregation is not None:\n                            attribute = f\"{clause._aggregation_name.upper()}({clause.attribute})\"\n                        elif clause.bin_size > 0:\n                            attribute = f\"BIN({clause.attribute})\"\n                        else:\n                            attribute = clause.attribute\n                        if clause.channel == \"x\":\n                            channels.insert(0, [clause.channel, attribute])\n                        elif clause.channel == \"y\":\n                            channels.insert(1, [clause.channel, attribute])\n                        elif clause.channel != \"\":\n                            additional_channels.append([clause.channel, attribute])\n\n            channels.extend(additional_channels)\n            str_channels = \"\"\n            for channel in channels:\n                str_channels += f\"{channel[0]}: {channel[1]}, \"\n\n            if filter_intents:\n                return f\"<Vis  ({str_channels[:-2]} -- [{filter_intents.attribute}{filter_intents.filter_op}{filter_intents.value}]) mark: {self._mark}, score: {self.score} >\"\n            else:\n                return f\"<Vis  ({str_channels[:-2]}) mark: {self._mark}, score: {self.score} >\"\n        else:\n            # When Vis not compiled (e.g., when self._source not populated), print original intent\n            return f\"<Vis  ({str(self._intent)}) mark: {self._mark}, score: {self.score} >\"\n\n    @property\n    def data(self):\n        return self._vis_data\n\n    @property\n    def code(self):\n        return self._code\n\n    @property\n    def mark(self):\n        return self._mark\n\n    @property\n    def min_max(self):\n        return self._min_max\n\n    @property\n    def intent(self):\n        return self._intent\n\n    @intent.setter\n    def intent(self, intent: List[Clause]) -> None:\n        self.set_intent(intent)\n\n    def set_intent(self, intent: List[Clause]) -> None:\n        \"\"\"\n        Sets the intent of the Vis and refresh the source based on the new intent\n\n        Parameters\n        ----------\n        intent : List[Clause]\n                Query specifying the desired VisList\n        \"\"\"\n        self._intent = intent\n        self.refresh_source(self._source)\n\n    def _ipython_display_(self):\n        from lux.utils.utils import check_import_lux_widget\n        from IPython.display import display\n\n        check_import_lux_widget()\n        import luxwidget\n\n        if self.data is None:\n            raise Exception(\n                \"No data is populated in Vis. In order to generate data required for the vis, use the 'refresh_source' function to populate the Vis with a data source (e.g., vis.refresh_source(df)).\"\n            )\n        else:\n            from lux.core.frame import LuxDataFrame\n\n            widget = luxwidget.LuxWidget(\n                currentVis=LuxDataFrame.current_vis_to_JSON([self]),\n                recommendations=[],\n                intent=\"\",\n                message=\"\",\n                config={\"plottingScale\": lux.config.plotting_scale},\n            )\n            display(widget)\n\n    def get_attr_by_attr_name(self, attr_name):\n        return list(filter(lambda x: x.attribute == attr_name, self._inferred_intent))\n\n###The function: get_attr_by_channel###\n    def get_attr_by_data_model(self, dmodel, exclude_record=False):\n        if exclude_record:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if x.attribute != \"Record\" and hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n        else:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n\n    def get_attr_by_data_type(self, dtype):\n        return list(\n            filter(\n                lambda x: x.data_type == dtype and x.value == \"\" if hasattr(x, \"data_type\") else False,\n                self._inferred_intent,\n            )\n        )\n\n    def remove_filter_from_spec(self, value):\n        new_intent = list(filter(lambda x: x.value != value, self._inferred_intent))\n        self.set_intent(new_intent)\n\n    def remove_column_from_spec(self, attribute, remove_first: bool = False):\n        \"\"\"\n        Removes an attribute from the Vis's clause\n\n        Parameters\n        ----------\n        attribute : str\n                attribute to be removed\n        remove_first : bool, optional\n                Boolean flag to determine whether to remove all instances of the attribute or only one (first) instance, by default False\n        \"\"\"\n        if not remove_first:\n            new_inferred = list(filter(lambda x: x.attribute != attribute, self._inferred_intent))\n            self._inferred_intent = new_inferred\n            self._intent = new_inferred\n        elif remove_first:\n            new_inferred = []\n            skip_check = False\n            for i in range(0, len(self._inferred_intent)):\n                if self._inferred_intent[i].value == \"\":  # clause is type attribute\n                    column_spec = []\n                    column_names = self._inferred_intent[i].attribute\n                    # if only one variable in a column, columnName results in a string and not a list so\n                    # you need to differentiate the cases\n                    if isinstance(column_names, list):\n                        for column in column_names:\n                            if (column != attribute) or skip_check:\n                                column_spec.append(column)\n                            elif remove_first:\n                                remove_first = True\n                        new_inferred.append(Clause(column_spec))\n                    else:\n                        if column_names != attribute or skip_check:\n                            new_inferred.append(Clause(attribute=column_names))\n                        elif remove_first:\n                            skip_check = True\n                else:\n                    new_inferred.append(self._inferred_intent[i])\n            self._intent = new_inferred\n            self._inferred_intent = new_inferred\n\n    def to_altair(self, standalone=False) -> str:\n        \"\"\"\n        Generate minimal Altair code to visualize the Vis\n\n        Parameters\n        ----------\n        standalone : bool, optional\n                Flag to determine if outputted code uses user-defined variable names or can be run independently, by default False\n\n        Returns\n        -------\n        str\n                String version of the Altair code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"Altair\")\n        self._code = renderer.create_vis(self, standalone)\n\n        if lux.config.executor.name == \"PandasExecutor\":\n            function_code = \"def plot_data(source_df, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(source_df, vis)\\n\"\n        else:\n            function_code = \"def plot_data(tbl, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(tbl, vis)\\n\"\n\n        vis_code_lines = self._code.split(\"\\n\")\n        for i in range(2, len(vis_code_lines) - 1):\n            function_code += \"\\t\" + vis_code_lines[i] + \"\\n\"\n        function_code += \"\\treturn chart\\n#plot_data(your_df, vis) this creates an Altair plot using your source data and vis specification\"\n        function_code = function_code.replace(\"alt.Chart(tbl)\", \"alt.Chart(visData)\")\n\n        if \"mark_circle\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_scatterplot\")\n        elif \"mark_bar\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_barchart\")\n        elif \"mark_line\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_linechart\")\n        elif \"mark_rect\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_heatmap\")\n        return function_code\n\n    def to_matplotlib(self) -> str:\n        \"\"\"\n        Generate minimal Matplotlib code to visualize the Vis\n\n        Returns\n        -------\n        str\n                String version of the Matplotlib code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def _to_matplotlib_svg(self) -> str:\n        \"\"\"\n        Private method to render Vis as SVG with Matplotlib\n\n        Returns\n        -------\n        str\n                String version of the SVG.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib_svg\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def to_vegalite(self, prettyOutput=True) -> Union[dict, str]:\n        \"\"\"\n        Generate minimal Vega-Lite code to visualize the Vis\n\n        Returns\n        -------\n        Union[dict,str]\n                String or Dictionary of the VegaLite JSON specification\n        \"\"\"\n        import json\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"VegaLite\")\n        self._code = renderer.create_vis(self)\n        if prettyOutput:\n            return (\n                \"** Remove this comment -- Copy Text Below to Vega Editor(vega.github.io/editor) to visualize and edit **\\n\"\n                + json.dumps(self._code, indent=2)\n            )\n        else:\n            return self._code\n\n    def to_code(self, language=\"vegalite\", **kwargs):\n        \"\"\"\n        Export Vis object to code specification\n\n        Parameters\n        ----------\n        language : str, optional\n            choice of target language to produce the visualization code in, by default \"vegalite\"\n\n        Returns\n        -------\n        spec:\n            visualization specification corresponding to the Vis object\n        \"\"\"\n        if language == \"vegalite\":\n            return self.to_vegalite(**kwargs)\n        elif language == \"altair\":\n            return self.to_altair(**kwargs)\n        elif language == \"matplotlib\":\n            return self.to_matplotlib()\n        elif language == \"matplotlib_svg\":\n            return self._to_matplotlib_svg()\n        elif language == \"python\":\n            lux.config.tracer.start_tracing()\n            lux.config.executor.execute(lux.vis.VisList.VisList(input_lst=[self]), self._source)\n            lux.config.tracer.stop_tracing()\n            self._trace_code = lux.config.tracer.process_executor_code(lux.config.tracer_relevant_lines)\n            lux.config.tracer_relevant_lines = []\n            return self._trace_code\n        elif language == \"SQL\":\n            if self._query:\n                return self._query\n            else:\n                warnings.warn(\n                    \"The data for this Vis was not collected via a SQL database. Use the 'python' parameter to view the code used to generate the data.\",\n                    stacklevel=2,\n                )\n        else:\n            warnings.warn(\n                \"Unsupported plotting backend. Lux currently only support 'altair', 'vegalite', or 'matplotlib'\",\n                stacklevel=2,\n            )\n\n    def refresh_source(self, ldf):  # -> Vis:\n        \"\"\"\n        Loading the source data into the Vis by instantiating the specification and\n        populating the Vis based on the source data, effectively \"materializing\" the Vis.\n\n        Parameters\n        ----------\n        ldf : LuxDataframe\n                Input Dataframe to be attached to the Vis\n\n        Returns\n        -------\n        Vis\n                Complete Vis with fully-specified fields\n\n        See Also\n        --------\n        lux.Vis.VisList.refresh_source\n\n        Note\n        ----\n        Function derives a new _inferred_intent by instantiating the intent specification on the new data\n        \"\"\"\n        if ldf is not None:\n            from lux.processor.Parser import Parser\n            from lux.processor.Validator import Validator\n            from lux.processor.Compiler import Compiler\n\n            self.check_not_vislist_intent()\n\n            ldf.maintain_metadata()\n            self._source = ldf\n            self._inferred_intent = Parser.parse(self._intent)\n            Validator.validate_intent(self._inferred_intent, ldf)\n\n            Compiler.compile_vis(ldf, self)\n            lux.config.executor.execute([self], ldf)\n\n    def check_not_vislist_intent(self):\n\n        syntaxMsg = (\n            \"The intent that you specified corresponds to more than one visualization. \"\n            \"Please replace the Vis constructor with VisList to generate a list of visualizations. \"\n            \"For more information, see: https://lux-api.readthedocs.io/en/latest/source/guide/vis.html#working-with-collections-of-visualization-with-vislist\"\n        )\n\n        for i in range(len(self._intent)):\n            clause = self._intent[i]\n            if isinstance(clause, str):\n                if \"|\" in clause or \"?\" in clause:\n                    raise TypeError(syntaxMsg)\n            if isinstance(clause, list):\n                raise TypeError(syntaxMsg)\n", "prompt": "Please write a python function called 'get_attr_by_channel' base the context. This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.:param self: Vis. An instance of the Vis class.\n:param channel: The channel to filter the inferred intent list.\n:return: List. The filtered list of objects from the inferred intent list..\n        The context you need to refer to is as follows: class Vis:\n    \"\"\"\n    Vis Object represents a collection of fully fleshed out specifications required for data fetching and visualization.\n    \"\"\"\n\n    def __init__(self, intent, source=None, title=\"\", score=0.0):\n        self._intent = intent  # user's original intent to Vis\n        self._inferred_intent = intent  # re-written, expanded version of user's original intent\n        self._source = source  # original data attached to the Vis\n        self._vis_data = None  # processed data for Vis (e.g., selected, aggregated, binned)\n        self._code = None\n        self._mark = \"\"\n        self._min_max = {}\n        self._postbin = None\n        self.title = title\n        self.score = score\n        self._all_column = False\n        self.approx = False\n        self.refresh_source(self._source)\n\n    def __repr__(self):\n        all_clause = all([isinstance(unit, lux.Clause) for unit in self._inferred_intent])\n        if all_clause:\n            filter_intents = None\n            channels, additional_channels = [], []\n            for clause in self._inferred_intent:\n\n                if hasattr(clause, \"value\"):\n                    if clause.value != \"\":\n                        filter_intents = clause\n                if hasattr(clause, \"attribute\"):\n                    if clause.attribute != \"\":\n                        if clause.aggregation != \"\" and clause.aggregation is not None:\n                            attribute = f\"{clause._aggregation_name.upper()}({clause.attribute})\"\n                        elif clause.bin_size > 0:\n                            attribute = f\"BIN({clause.attribute})\"\n                        else:\n                            attribute = clause.attribute\n                        if clause.channel == \"x\":\n                            channels.insert(0, [clause.channel, attribute])\n                        elif clause.channel == \"y\":\n                            channels.insert(1, [clause.channel, attribute])\n                        elif clause.channel != \"\":\n                            additional_channels.append([clause.channel, attribute])\n\n            channels.extend(additional_channels)\n            str_channels = \"\"\n            for channel in channels:\n                str_channels += f\"{channel[0]}: {channel[1]}, \"\n\n            if filter_intents:\n                return f\"<Vis  ({str_channels[:-2]} -- [{filter_intents.attribute}{filter_intents.filter_op}{filter_intents.value}]) mark: {self._mark}, score: {self.score} >\"\n            else:\n                return f\"<Vis  ({str_channels[:-2]}) mark: {self._mark}, score: {self.score} >\"\n        else:\n            # When Vis not compiled (e.g., when self._source not populated), print original intent\n            return f\"<Vis  ({str(self._intent)}) mark: {self._mark}, score: {self.score} >\"\n\n    @property\n    def data(self):\n        return self._vis_data\n\n    @property\n    def code(self):\n        return self._code\n\n    @property\n    def mark(self):\n        return self._mark\n\n    @property\n    def min_max(self):\n        return self._min_max\n\n    @property\n    def intent(self):\n        return self._intent\n\n    @intent.setter\n    def intent(self, intent: List[Clause]) -> None:\n        self.set_intent(intent)\n\n    def set_intent(self, intent: List[Clause]) -> None:\n        \"\"\"\n        Sets the intent of the Vis and refresh the source based on the new intent\n\n        Parameters\n        ----------\n        intent : List[Clause]\n                Query specifying the desired VisList\n        \"\"\"\n        self._intent = intent\n        self.refresh_source(self._source)\n\n    def _ipython_display_(self):\n        from lux.utils.utils import check_import_lux_widget\n        from IPython.display import display\n\n        check_import_lux_widget()\n        import luxwidget\n\n        if self.data is None:\n            raise Exception(\n                \"No data is populated in Vis. In order to generate data required for the vis, use the 'refresh_source' function to populate the Vis with a data source (e.g., vis.refresh_source(df)).\"\n            )\n        else:\n            from lux.core.frame import LuxDataFrame\n\n            widget = luxwidget.LuxWidget(\n                currentVis=LuxDataFrame.current_vis_to_JSON([self]),\n                recommendations=[],\n                intent=\"\",\n                message=\"\",\n                config={\"plottingScale\": lux.config.plotting_scale},\n            )\n            display(widget)\n\n    def get_attr_by_attr_name(self, attr_name):\n        return list(filter(lambda x: x.attribute == attr_name, self._inferred_intent))\n\n###The function: get_attr_by_channel###\n    def get_attr_by_data_model(self, dmodel, exclude_record=False):\n        if exclude_record:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if x.attribute != \"Record\" and hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n        else:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n\n    def get_attr_by_data_type(self, dtype):\n        return list(\n            filter(\n                lambda x: x.data_type == dtype and x.value == \"\" if hasattr(x, \"data_type\") else False,\n                self._inferred_intent,\n            )\n        )\n\n    def remove_filter_from_spec(self, value):\n        new_intent = list(filter(lambda x: x.value != value, self._inferred_intent))\n        self.set_intent(new_intent)\n\n    def remove_column_from_spec(self, attribute, remove_first: bool = False):\n        \"\"\"\n        Removes an attribute from the Vis's clause\n\n        Parameters\n        ----------\n        attribute : str\n                attribute to be removed\n        remove_first : bool, optional\n                Boolean flag to determine whether to remove all instances of the attribute or only one (first) instance, by default False\n        \"\"\"\n        if not remove_first:\n            new_inferred = list(filter(lambda x: x.attribute != attribute, self._inferred_intent))\n            self._inferred_intent = new_inferred\n            self._intent = new_inferred\n        elif remove_first:\n            new_inferred = []\n            skip_check = False\n            for i in range(0, len(self._inferred_intent)):\n                if self._inferred_intent[i].value == \"\":  # clause is type attribute\n                    column_spec = []\n                    column_names = self._inferred_intent[i].attribute\n                    # if only one variable in a column, columnName results in a string and not a list so\n                    # you need to differentiate the cases\n                    if isinstance(column_names, list):\n                        for column in column_names:\n                            if (column != attribute) or skip_check:\n                                column_spec.append(column)\n                            elif remove_first:\n                                remove_first = True\n                        new_inferred.append(Clause(column_spec))\n                    else:\n                        if column_names != attribute or skip_check:\n                            new_inferred.append(Clause(attribute=column_names))\n                        elif remove_first:\n                            skip_check = True\n                else:\n                    new_inferred.append(self._inferred_intent[i])\n            self._intent = new_inferred\n            self._inferred_intent = new_inferred\n\n    def to_altair(self, standalone=False) -> str:\n        \"\"\"\n        Generate minimal Altair code to visualize the Vis\n\n        Parameters\n        ----------\n        standalone : bool, optional\n                Flag to determine if outputted code uses user-defined variable names or can be run independently, by default False\n\n        Returns\n        -------\n        str\n                String version of the Altair code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"Altair\")\n        self._code = renderer.create_vis(self, standalone)\n\n        if lux.config.executor.name == \"PandasExecutor\":\n            function_code = \"def plot_data(source_df, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(source_df, vis)\\n\"\n        else:\n            function_code = \"def plot_data(tbl, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(tbl, vis)\\n\"\n\n        vis_code_lines = self._code.split(\"\\n\")\n        for i in range(2, len(vis_code_lines) - 1):\n            function_code += \"\\t\" + vis_code_lines[i] + \"\\n\"\n        function_code += \"\\treturn chart\\n#plot_data(your_df, vis) this creates an Altair plot using your source data and vis specification\"\n        function_code = function_code.replace(\"alt.Chart(tbl)\", \"alt.Chart(visData)\")\n\n        if \"mark_circle\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_scatterplot\")\n        elif \"mark_bar\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_barchart\")\n        elif \"mark_line\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_linechart\")\n        elif \"mark_rect\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_heatmap\")\n        return function_code\n\n    def to_matplotlib(self) -> str:\n        \"\"\"\n        Generate minimal Matplotlib code to visualize the Vis\n\n        Returns\n        -------\n        str\n                String version of the Matplotlib code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def _to_matplotlib_svg(self) -> str:\n        \"\"\"\n        Private method to render Vis as SVG with Matplotlib\n\n        Returns\n        -------\n        str\n                String version of the SVG.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib_svg\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def to_vegalite(self, prettyOutput=True) -> Union[dict, str]:\n        \"\"\"\n        Generate minimal Vega-Lite code to visualize the Vis\n\n        Returns\n        -------\n        Union[dict,str]\n                String or Dictionary of the VegaLite JSON specification\n        \"\"\"\n        import json\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"VegaLite\")\n        self._code = renderer.create_vis(self)\n        if prettyOutput:\n            return (\n                \"** Remove this comment -- Copy Text Below to Vega Editor(vega.github.io/editor) to visualize and edit **\\n\"\n                + json.dumps(self._code, indent=2)\n            )\n        else:\n            return self._code\n\n    def to_code(self, language=\"vegalite\", **kwargs):\n        \"\"\"\n        Export Vis object to code specification\n\n        Parameters\n        ----------\n        language : str, optional\n            choice of target language to produce the visualization code in, by default \"vegalite\"\n\n        Returns\n        -------\n        spec:\n            visualization specification corresponding to the Vis object\n        \"\"\"\n        if language == \"vegalite\":\n            return self.to_vegalite(**kwargs)\n        elif language == \"altair\":\n            return self.to_altair(**kwargs)\n        elif language == \"matplotlib\":\n            return self.to_matplotlib()\n        elif language == \"matplotlib_svg\":\n            return self._to_matplotlib_svg()\n        elif language == \"python\":\n            lux.config.tracer.start_tracing()\n            lux.config.executor.execute(lux.vis.VisList.VisList(input_lst=[self]), self._source)\n            lux.config.tracer.stop_tracing()\n            self._trace_code = lux.config.tracer.process_executor_code(lux.config.tracer_relevant_lines)\n            lux.config.tracer_relevant_lines = []\n            return self._trace_code\n        elif language == \"SQL\":\n            if self._query:\n                return self._query\n            else:\n                warnings.warn(\n                    \"The data for this Vis was not collected via a SQL database. Use the 'python' parameter to view the code used to generate the data.\",\n                    stacklevel=2,\n                )\n        else:\n            warnings.warn(\n                \"Unsupported plotting backend. Lux currently only support 'altair', 'vegalite', or 'matplotlib'\",\n                stacklevel=2,\n            )\n\n    def refresh_source(self, ldf):  # -> Vis:\n        \"\"\"\n        Loading the source data into the Vis by instantiating the specification and\n        populating the Vis based on the source data, effectively \"materializing\" the Vis.\n\n        Parameters\n        ----------\n        ldf : LuxDataframe\n                Input Dataframe to be attached to the Vis\n\n        Returns\n        -------\n        Vis\n                Complete Vis with fully-specified fields\n\n        See Also\n        --------\n        lux.Vis.VisList.refresh_source\n\n        Note\n        ----\n        Function derives a new _inferred_intent by instantiating the intent specification on the new data\n        \"\"\"\n        if ldf is not None:\n            from lux.processor.Parser import Parser\n            from lux.processor.Validator import Validator\n            from lux.processor.Compiler import Compiler\n\n            self.check_not_vislist_intent()\n\n            ldf.maintain_metadata()\n            self._source = ldf\n            self._inferred_intent = Parser.parse(self._intent)\n            Validator.validate_intent(self._inferred_intent, ldf)\n\n            Compiler.compile_vis(ldf, self)\n            lux.config.executor.execute([self], ldf)\n\n    def check_not_vislist_intent(self):\n\n        syntaxMsg = (\n            \"The intent that you specified corresponds to more than one visualization. \"\n            \"Please replace the Vis constructor with VisList to generate a list of visualizations. \"\n            \"For more information, see: https://lux-api.readthedocs.io/en/latest/source/guide/vis.html#working-with-collections-of-visualization-with-vislist\"\n        )\n\n        for i in range(len(self._intent)):\n            clause = self._intent[i]\n            if isinstance(clause, str):\n                if \"|\" in clause or \"?\" in clause:\n                    raise TypeError(syntaxMsg)\n            if isinstance(clause, list):\n                raise TypeError(syntaxMsg)\n", "test_list": ["def test_refresh_inplace():\n    df = pd.DataFrame({'date': ['2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01'], 'value': [10.5, 15.2, 20.3, 25.2]})\n    with pytest.warns(UserWarning, match=\"Lux detects that the attribute 'date' may be temporal.\"):\n        df._ipython_display_()\n    assert df.data_type['date'] == 'temporal'\n    from lux.vis.Vis import Vis\n    vis = Vis(['date', 'value'], df)\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n    df.maintain_metadata()\n    inverted_data_type = lux.config.executor.invert_data_type(df.data_type)\n    assert inverted_data_type['temporal'][0] == 'date'\n    vis.refresh_source(df)\n    assert vis.mark == 'line'\n    assert vis.get_attr_by_channel('x')[0].attribute == 'date'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'value'", "def test_special_char():\n    dataset = [{'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 5}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 3}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 6}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 7}, {'special.char': 1, 'normal': 2}, {'special.char': 3, 'normal': 10}, {'special.char': 1, 'normal': 1}, {'special.char': 5, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}]\n    test = pd.DataFrame(dataset)\n    from lux.vis.Vis import Vis\n    vis = Vis(['special.char'], test)\n    assert vis.mark == 'bar'\n    assert vis.intent == ['special.char']\n    assert vis.get_attr_by_channel('x')[0].attribute == 'Record'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'special.char'\n    vis = vis.to_altair()\n    assert \"alt.Y('specialchar', type= 'nominal', axis=alt.Axis(labelOverlap=True, title='special.char'))\" in vis\n    assert \"alt.X('Record', type= 'quantitative', title='Number of Records', axis=alt.Axis(title='Number of Records')\" in vis\n    test = test.rename(columns={'special.char': 'special..char..'})\n    vis = Vis(['special..char..'], test)\n    assert vis.mark == 'bar'\n    assert vis.intent == ['special..char..']\n    assert vis.get_attr_by_channel('x')[0].attribute == 'Record'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'special..char..'\n    vis = vis.to_altair()\n    assert \"alt.Y('specialchar', type= 'nominal', axis=alt.Axis(labelOverlap=True, title='special..char..')\" in vis\n    assert \"alt.X('Record', type= 'quantitative', title='Number of Records', axis=alt.Axis(title='Number of Records')\" in vis"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'get_attr_by_channel' should accept a string as the 'channel' parameter and return a list of objects from '_inferred_intent' that have a matching 'channel' attribute.", "unit_test": "def test_get_attr_by_channel_input_output():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='y', attribute='value'),\n        lux.Clause(channel='color', attribute='category')\n    ]\n    result = vis.get_attr_by_channel('x')\n    assert len(result) == 1\n    assert result[0].attribute == 'date'", "test": "tests/test_dates.py::test_get_attr_by_channel_input_output"}, "Exception Handling": {"requirement": "The function 'get_attr_by_channel' should raise a TypeError if the 'channel' parameter is not a string.", "unit_test": "def test_get_attr_by_channel_exception_handling():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    try:\n        vis.get_attr_by_channel(123)\n    except TypeError as e:\n        assert str(e) == \"Channel must be a string\"", "test": "tests/test_dates.py::test_get_attr_by_channel_exception_handling"}, "Edge Case Handling": {"requirement": "The function 'get_attr_by_channel' should return an empty list if no objects in '_inferred_intent' match the given 'channel'.", "unit_test": "def test_get_attr_by_channel_edge_case():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    result = vis.get_attr_by_channel('z')\n    assert result == []", "test": "tests/test_dates.py::test_get_attr_by_channel_edge_case"}, "Functionality Extension": {"requirement": "Extend 'get_attr_by_channel' to optionally filter by a secondary 'attribute' parameter, returning only objects that match both 'channel' and 'attribute'.", "unit_test": "def test_get_attr_by_channel_functionality_extension():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='x', attribute='value')\n    ]\n    result = vis.get_attr_by_channel('x', 'date')\n    assert len(result) == 1\n    assert result[0].attribute == 'date'", "test": "tests/test_dates.py::test_get_attr_by_channel_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that 'get_attr_by_channel' has complete parameter and return type annotations.", "unit_test": "def test_get_attr_by_channel_annotation_coverage():\n    import inspect\n    sig = inspect.signature(Vis.get_attr_by_channel)\n    assert sig.parameters['channel'].annotation == str\n    assert sig.return_annotation == list", "test": "tests/test_dates.py::test_get_attr_by_channel_annotation_coverage"}, "Code Complexity": {"requirement": "The function 'get_attr_by_channel' should have a cyclomatic complexity of 3, indicating a simple function with no branching.", "unit_test": "def test_get_attr_by_channel_code_complexity():\n    from radon.complexity import cc_visit\n    code = inspect.getsource(Vis.get_attr_by_channel)\n    complexity = cc_visit(code)\n    assert complexity[0].complexity == 1", "test": "tests/test_dates.py::test_get_attr_by_channel_code_complexity"}, "Code Standard": {"requirement": "Ensure 'get_attr_by_channel' adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_check_code_style():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['vis.py'])\n    assert result.total_errors == 0", "test": "tests/test_dates.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that 'get_attr_by_channel' utilizes the '_inferred_intent' attribute from the Vis class context.", "unit_test": "def test_get_attr_by_channel_context_usage():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    result = vis.get_attr_by_channel('x')\n    assert result == vis._inferred_intent", "test": "tests/test_dates.py::test_get_attr_by_channel_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure 'get_attr_by_channel' correctly filters '_inferred_intent' based on the 'channel' attribute of each object.", "unit_test": "def test_get_attr_by_channel_context_usage_correctness():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='y', attribute='value')\n    ]\n    result = vis.get_attr_by_channel('x')\n    assert len(result) == 1\n    assert result[0].channel == 'x'", "test": "tests/test_dates.py::test_get_attr_by_channel_context_usage_correctness"}}}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "type": "method", "project_path": "Scientific-Engineering/folium", "completion_path": "Scientific-Engineering/folium/folium/features.py", "signature_position": [323, 323], "body_position": [324, 329], "dependency": {"intra_class": ["folium.features.VegaLite.data"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns the major version number of the Vega-Lite schema used in the VegaLite instance. It extracts the major version number from the \"$schema\" attribute in the instance's data.", "Arguments": ":param self: VegaLite. An instance of the VegaLite class.\n:return: int. The major version number of the Vega-Lite schema used in the instance."}, "tests": ["tests/test_features.py::test_vegalite_major_version"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def vegalite_major_version(self) -> int:\n        if \"$schema\" not in self.data:\n            return None\n\n        schema = self.data[\"$schema\"]\n\n        return int(schema.split(\"/\")[-1].split(\".\")[0].lstrip(\"v\"))\n", "context": "class VegaLite(Element):\n    \"\"\"\n    Creates a Vega-Lite chart element.\n\n    Parameters\n    ----------\n    data: JSON-like str or object\n        The Vega-Lite description of the chart.\n        It can also be any object that has a method `to_json`,\n        so that you can (for instance) provide an `Altair` chart.\n    width: int or str, default None\n        The width of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    height: int or str, default None\n        The height of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    left: int or str, default '0%'\n        The horizontal distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    top: int or str, default '0%'\n        The vertical distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    position: str, default 'relative'\n        The `position` argument that the CSS shall contain.\n        Ex: 'relative', 'absolute'\n\n    \"\"\"\n\n    _template = Template(\"\")\n\n    def __init__(\n        self, data, width=None, height=None, left=\"0%\", top=\"0%\", position=\"relative\"\n    ):\n        super(self.__class__, self).__init__()\n        self._name = \"VegaLite\"\n        self.data = data.to_json() if hasattr(data, \"to_json\") else data\n        if isinstance(self.data, str):\n            self.data = json.loads(self.data)\n\n        self.json = json.dumps(self.data)\n\n        # Size Parameters.\n        self.width = _parse_size(\n            self.data.get(\"width\", \"100%\") if width is None else width\n        )\n        self.height = _parse_size(\n            self.data.get(\"height\", \"100%\") if height is None else height\n        )\n        self.left = _parse_size(left)\n        self.top = _parse_size(top)\n        self.position = position\n\n    def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self._parent.html.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\"\n                ).render(this=self, kwargs=kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        figure = self.get_root()\n        assert isinstance(\n            figure, Figure\n        ), \"You cannot render this Element if it is not in a Figure.\"\n\n        figure.header.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\"\n                ).render(this=self, **kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        embed_mapping = {\n            1: self._embed_vegalite_v1,\n            2: self._embed_vegalite_v2,\n            3: self._embed_vegalite_v3,\n            4: self._embed_vegalite_v4,\n            5: self._embed_vegalite_v5,\n        }\n\n        # Version 2 is assumed as the default, if no version is given in the schema.\n        embed_vegalite = embed_mapping.get(\n            self.vegalite_major_version, self._embed_vegalite_v2\n        )\n        embed_vegalite(figure)\n\n    @property\n###The function: vegalite_major_version###\n    def _embed_vegalite_v5(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@5\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v4(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@4\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v3(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@4\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@3\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v2(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@3\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@2\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _vega_embed(self):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    vegaEmbed({{this.get_name()}}, {{this.json}})\n                        .then(function(result) {})\n                        .catch(console.error);\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n    def _embed_vegalite_v1(self, figure):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    var embedSpec = {\n                        mode: \"vega-lite\",\n                        spec: {{this.json}}\n                    };\n                    vg.embed(\n                        {{this.get_name()}}, embedSpec, function(error, result) {}\n                    );\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n        figure.header.add_child(\n            JavascriptLink(\"https://d3js.org/d3.v3.min.js\"), name=\"d3\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdnjs.cloudflare.com/ajax/libs/vega/2.6.5/vega.js\"),\n            name=\"vega\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-lite/1.3.1/vega-lite.js\"\n            ),\n            name=\"vega-lite\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-embed/2.2.0/vega-embed.js\"\n            ),\n            name=\"vega-embed\",\n        )  # noqa\n", "prompt": "Please write a python function called 'vegalite_major_version' base the context. This function returns the major version number of the Vega-Lite schema used in the VegaLite instance. It extracts the major version number from the \"$schema\" attribute in the instance's data.:param self: VegaLite. An instance of the VegaLite class.\n:return: int. The major version number of the Vega-Lite schema used in the instance..\n        The context you need to refer to is as follows: class VegaLite(Element):\n    \"\"\"\n    Creates a Vega-Lite chart element.\n\n    Parameters\n    ----------\n    data: JSON-like str or object\n        The Vega-Lite description of the chart.\n        It can also be any object that has a method `to_json`,\n        so that you can (for instance) provide an `Altair` chart.\n    width: int or str, default None\n        The width of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    height: int or str, default None\n        The height of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    left: int or str, default '0%'\n        The horizontal distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    top: int or str, default '0%'\n        The vertical distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    position: str, default 'relative'\n        The `position` argument that the CSS shall contain.\n        Ex: 'relative', 'absolute'\n\n    \"\"\"\n\n    _template = Template(\"\")\n\n    def __init__(\n        self, data, width=None, height=None, left=\"0%\", top=\"0%\", position=\"relative\"\n    ):\n        super(self.__class__, self).__init__()\n        self._name = \"VegaLite\"\n        self.data = data.to_json() if hasattr(data, \"to_json\") else data\n        if isinstance(self.data, str):\n            self.data = json.loads(self.data)\n\n        self.json = json.dumps(self.data)\n\n        # Size Parameters.\n        self.width = _parse_size(\n            self.data.get(\"width\", \"100%\") if width is None else width\n        )\n        self.height = _parse_size(\n            self.data.get(\"height\", \"100%\") if height is None else height\n        )\n        self.left = _parse_size(left)\n        self.top = _parse_size(top)\n        self.position = position\n\n    def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self._parent.html.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\"\n                ).render(this=self, kwargs=kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        figure = self.get_root()\n        assert isinstance(\n            figure, Figure\n        ), \"You cannot render this Element if it is not in a Figure.\"\n\n        figure.header.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\"\n                ).render(this=self, **kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        embed_mapping = {\n            1: self._embed_vegalite_v1,\n            2: self._embed_vegalite_v2,\n            3: self._embed_vegalite_v3,\n            4: self._embed_vegalite_v4,\n            5: self._embed_vegalite_v5,\n        }\n\n        # Version 2 is assumed as the default, if no version is given in the schema.\n        embed_vegalite = embed_mapping.get(\n            self.vegalite_major_version, self._embed_vegalite_v2\n        )\n        embed_vegalite(figure)\n\n    @property\n###The function: vegalite_major_version###\n    def _embed_vegalite_v5(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@5\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v4(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@4\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v3(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@4\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@3\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v2(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@3\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@2\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _vega_embed(self):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    vegaEmbed({{this.get_name()}}, {{this.json}})\n                        .then(function(result) {})\n                        .catch(console.error);\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n    def _embed_vegalite_v1(self, figure):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    var embedSpec = {\n                        mode: \"vega-lite\",\n                        spec: {{this.json}}\n                    };\n                    vg.embed(\n                        {{this.get_name()}}, embedSpec, function(error, result) {}\n                    );\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n        figure.header.add_child(\n            JavascriptLink(\"https://d3js.org/d3.v3.min.js\"), name=\"d3\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdnjs.cloudflare.com/ajax/libs/vega/2.6.5/vega.js\"),\n            name=\"vega\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-lite/1.3.1/vega-lite.js\"\n            ),\n            name=\"vega-lite\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-embed/2.2.0/vega-embed.js\"\n            ),\n            name=\"vega-embed\",\n        )  # noqa\n", "test_list": ["@pytest.mark.parametrize('version', [1, 2, 3, 4, 5, None])\ndef test_vegalite_major_version(vegalite_spec, version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    if version is None:\n        assert vegalite.vegalite_major_version is None\n    else:\n        assert vegalite.vegalite_major_version == version"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'vegalite_major_version' should return an integer representing the major version number extracted from the '$schema' attribute of the Vega-Lite instance's data. If the '$schema' attribute is missing or not a valid URL, the function should return None.", "unit_test": "@pytest.mark.parametrize('vegalite_spec, expected_version', [({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'}, 4), ({'$schema': 'https://vega.github.io/schema/vega-lite/v3.json'}, 3), ({'$schema': 'https://vega.github.io/schema/vega-lite/v2.json'}, 2), ({'$schema': 'https://vega.github.io/schema/vega-lite/v1.json'}, 1), ({'$schema': 'https://vega.github.io/schema/vega-lite/v5.json'}, 5), ({'$schema': 'invalid_url'}, None), ({}, None)])\ndef test_vegalite_major_version_output(vegalite_spec, expected_version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version == expected_version", "test": "tests/test_features.py::test_vegalite_major_version_output"}, "Exception Handling": {"requirement": "The function 'vegalite_major_version' should handle exceptions gracefully and return None if the '$schema' attribute is not a string or if any error occurs during the extraction of the major version number.", "unit_test": "@pytest.mark.parametrize('vegalite_spec', [{'data': {'$schema': 123}}, {'data': {'$schema': None}}, {'data': {'$schema': {}}}, {'data': {'$schema': []}}])\ndef test_vegalite_major_version_exception_handling(vegalite_spec):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version is None", "test": "tests/test_features.py::test_vegalite_major_version_exception_handling"}, "Edge Case Handling": {"requirement": "The function 'vegalite_major_version' should correctly handle edge cases where the '$schema' attribute is present but does not conform to the expected URL format, returning None in such cases.", "unit_test": "@pytest.mark.parametrize('vegalite_spec', [{'data': {'$schema': 'https://vega.github.io/schema/vega-lite/v.json'}}, {'data': {'$schema': 'https://vega.github.io/schema/vega-lite/vx.json'}}, {'data': {'$schema': 'https://vega.github.io/schema/vega-lite/5.json'}}])\ndef test_vegalite_major_version_edge_cases(vegalite_spec):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version is None", "test": "tests/test_features.py::test_vegalite_major_version_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'vegalite_major_version' function to also return the minor version number as a tuple (major, minor) if the minor version is present in the '$schema' URL.", "unit_test": "@pytest.mark.parametrize('vegalite_spec, expected_version', [({'$schema': 'https://vega.github.io/schema/vega-lite/v4.1.json'}, (4, 1)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v3.2.json'}, (3, 2)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v2.0.json'}, (2, 0)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v1.5.json'}, (1, 5)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v5.json'}, (5, None))])\ndef test_vegalite_major_minor_version(vegalite_spec, expected_version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version == expected_version", "test": "tests/test_features.py::test_vegalite_major_minor_version"}, "Annotation Coverage": {"requirement": "Ensure that the 'vegalite_major_version' function is fully documented with appropriate docstrings, including parameter types and return types.", "unit_test": "def test_vegalite_major_version_annotations():\n    assert 'vegalite_major_version' in folium.features.VegaLite.__dict__\n    assert folium.features.VegaLite.vegalite_major_version.__doc__ is not None\n    assert ':param self: VegaLite' in folium.features.VegaLite.vegalite_major_version.__doc__\n    assert ':return: int' in folium.features.VegaLite.vegalite_major_version.__doc__", "test": "tests/test_features.py::test_vegalite_major_version_annotations"}, "Context Usage Verification": {"requirement": "Verify that the 'vegalite_major_version' function utilizes the 'data' attribute from the 'VegaLite' class context to extract the '$schema' attribute.", "unit_test": "def test_vegalite_major_version_context_usage():\n    vegalite = folium.features.VegaLite({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'})\n    assert vegalite.data['$schema'] == 'https://vega.github.io/schema/vega-lite/v4.json'\n    assert vegalite.vegalite_major_version == 4", "test": "tests/test_features.py::test_vegalite_major_version_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'vegalite_major_version' function correctly uses the 'data' attribute from the 'VegaLite' class to access and parse the '$schema' attribute.", "unit_test": "def test_vegalite_major_version_context_correctness():\n    vegalite = folium.features.VegaLite({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'})\n    assert vegalite.data['$schema'] == 'https://vega.github.io/schema/vega-lite/v4.json'\n    assert isinstance(vegalite.vegalite_major_version, int)\n    assert vegalite.vegalite_major_version == 4", "test": "tests/test_features.py::test_vegalite_major_version_context_correctness"}}}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "type": "method", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/en_spell.py", "signature_position": [90, 90], "body_position": [96, 97], "dependency": {"intra_class": ["pycorrector.en_spell.EnSpell.check_init", "pycorrector.en_spell.EnSpell.edits1", "pycorrector.en_spell.EnSpell.edits2", "pycorrector.en_spell.EnSpell.known"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function generates possible spelling corrections for a given word. It checks whether zero, one, or two edits are needed to correct the word. If zero edit is needed, it returns the set of the given words. If one edit is needed, it returns the set of known words by applying one edit. If two edits are needed, it returns the set of known words by applying two edits. If no corrections are found, it returns the original word. It checks if the EnSpell instance has been initialized before performing the operation.", "Arguments": ":param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word for which spelling corrections need to be generated.\n:return: Set of strings. The set of possible spelling corrections for the word."}, "tests": ["tests/en_spell_bug_fix_test.py::EnBugTestCase::test_en_bug_correct2", "tests/en_spell_dict_test.py::TestEnSpell::test_candidates"], "indent": 4, "domain": "Text-Processing", "code": "    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n", "context": "class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n###The function: candidates###\n    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "prompt": "Please write a python function called 'candidates' base the context. This function generates possible spelling corrections for a given word. It checks whether zero, one, or two edits are needed to correct the word. If zero edit is needed, it returns the set of the given words. If one edit is needed, it returns the set of known words by applying one edit. If two edits are needed, it returns the set of known words by applying two edits. If no corrections are found, it returns the original word. It checks if the EnSpell instance has been initialized before performing the operation.:param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word for which spelling corrections need to be generated.\n:return: Set of strings. The set of possible spelling corrections for the word..\n        The context you need to refer to is as follows: class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n###The function: candidates###\n    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "test_list": ["def test_en_bug_correct2(self):\n    \"\"\"\u6d4b\u8bd5\u82f1\u6587\u7ea0\u9519bug\"\"\"\n    print(spell.word_freq_dict.get('whould'))\n    print(spell.candidates('whould'))\n    a = spell.correct_word('whould')\n    print(a)\n    r = spell.correct('contend proble poety adress whould niether  quaties')\n    print(r)\n    assert spell.correct('whould')[0] == 'would'", "def test_candidates(self):\n    \"\"\" test spell checker candidates \"\"\"\n    spell = EnSpell()\n    spell.check_init()\n    print(spell.word_freq_dict.get('ths'), spell.candidates('ths'))\n    self.assertEqual(len(spell.candidates('ths')) > 0, True)\n    self.assertEqual(spell.candidates('the'), {'the'})\n    self.assertEqual(spell.candidates('hi'), {'hi'})\n    self.assertEqual(''.join(spell.candidates('manasaeds')), 'manasaeds')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'candidates' function should accept a string input and return a set of strings as output. It should handle both valid and invalid word inputs gracefully.", "unit_test": "def test_candidates_input_output(self):\n    spell = EnSpell()\n    spell.check_init()\n    result = spell.candidates('example')\n    self.assertIsInstance(result, set)\n    self.assertTrue(all(isinstance(word, str) for word in result))\n    result_invalid = spell.candidates('')\n    self.assertEqual(result_invalid, set())", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_input_output"}, "Exception Handling": {"requirement": "The 'candidates' function should raise a ValueError if the input word is not a string.", "unit_test": "def test_candidates_exception_handling(self):\n    spell = EnSpell()\n    spell.check_init()\n    with self.assertRaises(ValueError):\n        spell.candidates(123)", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_input_output"}, "Edge Case Handling": {"requirement": "The 'candidates' function should correctly handle edge cases such as empty strings and single-character words.", "unit_test": "def test_candidates_edge_cases(self):\n    spell = EnSpell()\n    spell.check_init()\n    self.assertEqual(spell.candidates(''), set())\n    self.assertEqual(spell.candidates('a'), {'a'})", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'candidates' function to include a parameter that limits the number of suggestions returned.", "unit_test": "def test_candidates_functionality_extension(self):\n    spell = EnSpell()\n    spell.check_init()\n    result = spell.candidates('example', limit=5)\n    self.assertLessEqual(len(result), 5)", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that all functions, including 'candidates', have complete and accurate docstrings describing parameters, return types, and functionality.", "unit_test": "def test_annotation_coverage(self):\n    import inspect\n    spell = EnSpell()\n    docstring = inspect.getdoc(spell.candidates)\n    self.assertIsNotNone(docstring)\n    self.assertIn(':param', docstring)\n    self.assertIn(':return:', docstring)", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'candidates' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": "def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    with open('path_to_file_containing_candidates_function.py', 'r') as file:\n        code = file.read()\n    complexity = cc_visit(code)\n    candidates_complexity = next((c for c in complexity if c.name == 'candidates'), None)\n    self.assertIsNotNone(candidates_complexity)\n    self.assertLessEqual(candidates_complexity.complexity, 10)", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_code_complexity"}, "Code Standard": {"requirement": "The 'candidates' function should adhere to PEP 8 standards, including proper naming conventions and spacing.", "unit_test": "def test_code_standard(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_candidates_function.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'candidates' function should utilize the 'edits1', 'edits2', and 'known' methods from the EnSpell class.", "unit_test": "def test_context_usage_verification(self):\n    spell = EnSpell()\n    spell.check_init()\n    with unittest.mock.patch.object(spell, 'edits1', wraps=spell.edits1) as mock_edits1,\n         unittest.mock.patch.object(spell, 'edits2', wraps=spell.edits2) as mock_edits2,\n         unittest.mock.patch.object(spell, 'known', wraps=spell.known) as mock_known:\n        spell.candidates('example')\n        mock_edits1.assert_called()\n        mock_edits2.assert_called()\n        mock_known.assert_called()", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'candidates' function should correctly apply the 'edits1', 'edits2', and 'known' methods to generate spelling corrections.", "unit_test": "def test_context_usage_correctness(self):\n    spell = EnSpell()\n    spell.check_init()\n    known_words = spell.known(spell.edits1('example'))\n    self.assertEqual(spell.candidates('example'), known_words)\n    known_words_two_edits = spell.known(spell.edits2('example'))\n    self.assertEqual(spell.candidates('example'), known_words.union(known_words_two_edits))", "test": "tests/en_spell_dict_test.py::TestEnSpell::test_context_usage_correctness"}}}
