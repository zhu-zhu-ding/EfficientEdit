{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "type": "method", "project_path": "System/prometheus-client", "completion_path": "System/prometheus-client/prometheus_client/mmap_dict.py", "signature_position": [121, 121], "body_position": [122, 125], "dependency": {"intra_class": ["prometheus_client.mmap_dict.MmapedDict._init_value", "prometheus_client.mmap_dict.MmapedDict._m", "prometheus_client.mmap_dict.MmapedDict._positions"], "intra_file": ["prometheus_client.mmap_dict._unpack_two_doubles"], "cross_file": []}, "requirement": {"Functionality": "Read the value corresponding to the given key from the MmapedDict instance. If the key is not found in the instance, it initializes the value and then returns it.", "Arguments": ":param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to read the value from the instance.\n:return: The value corresponding to the key."}, "tests": ["tests/test_multiprocess.py::TestMmapedDict::test_process_restart"], "indent": 4, "domain": "System", "code": "    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n", "context": "import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n###The function: read_value###\n    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "prompt": "Please write a python function called 'read_value' base the context. Read the value corresponding to the given key from the MmapedDict instance. If the key is not found in the instance, it initializes the value and then returns it.:param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to read the value from the instance.\n:return: The value corresponding to the key..\n        The context you need to refer to is as follows: import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n###The function: read_value###\n    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "test_list": ["def test_process_restart(self):\n    self.d.write_value('abc', 123.0, 987.0)\n    self.d.close()\n    self.d = mmap_dict.MmapedDict(self.tempfile)\n    self.assertEqual((123, 987.0), self.d.read_value('abc'))\n    self.assertEqual([('abc', 123.0, 987.0)], list(self.d.read_all_values()))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'read_value' function should return a tuple of (value, timestamp) when a valid key is provided. If the key does not exist, it should initialize the value to 0.0 and timestamp to 0.0, then return this tuple.", "unit_test": "def test_read_value_initialization(self):\n    result = self.d.read_value('nonexistent_key')\n    self.assertEqual((0.0, 0.0), result)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_initialization"}, "Exception Handling": {"requirement": "The 'read_value' function should raise a KeyError with a descriptive message if the key is malformed or cannot be decoded.", "unit_test": "def test_read_value_malformed_key(self):\n    with self.assertRaises(KeyError) as context:\n        self.d.read_value(b'\\x80\\x81\\x82')\n    self.assertIn('Malformed key', str(context.exception))", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_malformed_key"}, "Edge Case Handling": {"requirement": "The 'read_value' function should handle the case where the mmap file is empty and return (0.0, 0.0) for any key.", "unit_test": "def test_read_value_empty_mmap(self):\n    empty_dict = mmap_dict.MmapedDict(self.tempfile, read_mode=True)\n    result = empty_dict.read_value('any_key')\n    self.assertEqual((0.0, 0.0), result)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_empty_mmap"}, "Functionality Extension": {"requirement": "Extend 'read_value' to accept an optional default value and timestamp, which are returned if the key is not found.", "unit_test": "def test_read_value_with_default(self):\n    result = self.d.read_value('nonexistent_key', default_value=1.0, default_timestamp=100.0)\n    self.assertEqual((1.0, 100.0), result)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_with_default"}, "Annotation Coverage": {"requirement": "Ensure that the 'read_value' function has complete type annotations for all parameters and return types.", "unit_test": "def test_read_value_annotations(self):\n    annotations = self.d.read_value.__annotations__\n    self.assertEqual(annotations['key'], str)\n    self.assertEqual(annotations['return'], tuple)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_annotations"}, "Code Complexity": {"requirement": "The 'read_value' function should have a cyclomatic complexity of 5 or less.", "unit_test": "def test_read_value_complexity(self):\n    complexity = calculate_cyclomatic_complexity(self.d.read_value)\n    self.assertLessEqual(complexity, 5)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_code_complexity"}, "Code Standard": {"requirement": "The 'read_value' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_read_value_pep8(self):\n    pep8_checker = pep8.Checker(self.d.read_value)\n    result = pep8_checker.check_all()\n    self.assertEqual(result, 0)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'read_value' function should utilize the '_init_value' method from the MmapedDict class when initializing a new key.", "unit_test": "def test_read_value_uses_init_value(self):\n    with mock.patch.object(self.d, '_init_value', wraps=self.d._init_value) as mock_init:\n        self.d.read_value('new_key')\n        mock_init.assert_called_once_with('new_key')", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_uses_init_value"}, "Context Usage Correctness Verification": {"requirement": "The 'read_value' function should correctly update the '_positions' dictionary when a new key is initialized.", "unit_test": "def test_read_value_updates_positions(self):\n    self.d.read_value('new_key')\n    self.assertIn('new_key', self.d._positions)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_updates_positions"}}}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "type": "method", "project_path": "Internet/boto", "completion_path": "Internet/boto/boto/ec2/securitygroup.py", "signature_position": [97, 99], "body_position": [105, 116], "dependency": {"intra_class": ["boto.ec2.securitygroup.SecurityGroup.rules"], "intra_file": ["boto.ec2.securitygroup.IPPermissions", "boto.ec2.securitygroup.IPPermissions.__init__", "boto.ec2.securitygroup.IPPermissions.add_grant", "boto.ec2.securitygroup.IPPermissions.from_port", "boto.ec2.securitygroup.IPPermissions.ip_protocol", "boto.ec2.securitygroup.IPPermissions.to_port"], "cross_file": []}, "requirement": {"Functionality": "Add a rule to a SecurityGroup instance. Note that this method only changes the local version of the instance. No information is sent to EC2.", "Arguments": ":param self: SecurityGroup. An instance of the SecurityGroup class.\n:param ip_protocol: String. The IP protocol for the rule.\n:param from_port: Integer. The starting port range for the rule.\n:param to_port: Integer. The ending port range for the rule.\n:param src_group_name: String. The name of the source security group.\n:param src_group_owner_id: String. The ID of the owner of the source security group.\n:param cidr_ip: String. The CIDR IP range for the rule.\n:param src_group_group_id: String. The ID of the source security group.\n:param dry_run: Bool. Whether to perform a dry run. Defaults to False.\n:return: No return values."}, "tests": ["tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule"], "indent": 4, "domain": "Internet", "code": "    def add_rule(self, ip_protocol, from_port, to_port,\n                 src_group_name, src_group_owner_id, cidr_ip,\n                 src_group_group_id, dry_run=False):\n        \"\"\"\n        Add a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        rule = IPPermissions(self)\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        self.rules.append(rule)\n        rule.add_grant(\n            src_group_name,\n            src_group_owner_id,\n            cidr_ip,\n            src_group_group_id,\n            dry_run=dry_run\n        )\n", "context": "# Copyright (c) 2006-2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Security Group\n\"\"\"\nfrom boto.ec2.ec2object import TaggedEC2Object\nfrom boto.exception import BotoClientError\n\n\nclass SecurityGroup(TaggedEC2Object):\n\n    def __init__(self, connection=None, owner_id=None,\n                 name=None, description=None, id=None):\n        super(SecurityGroup, self).__init__(connection)\n        self.id = id\n        self.owner_id = owner_id\n        self.name = name\n        self.description = description\n        self.vpc_id = None\n        self.rules = IPPermissionsList()\n        self.rules_egress = IPPermissionsList()\n\n    def __repr__(self):\n        return 'SecurityGroup:%s' % self.name\n\n    def startElement(self, name, attrs, connection):\n        retval = super(SecurityGroup, self).startElement(name, attrs, connection)\n        if retval is not None:\n            return retval\n        if name == 'ipPermissions':\n            return self.rules\n        elif name == 'ipPermissionsEgress':\n            return self.rules_egress\n        else:\n            return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ownerId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.id = value\n        elif name == 'groupName':\n            self.name = value\n        elif name == 'vpcId':\n            self.vpc_id = value\n        elif name == 'groupDescription':\n            self.description = value\n        elif name == 'ipRanges':\n            pass\n        elif name == 'return':\n            if value == 'false':\n                self.status = False\n            elif value == 'true':\n                self.status = True\n            else:\n                raise Exception(\n                    'Unexpected value of status %s for group %s' % (\n                        value,\n                        self.name\n                    )\n                )\n        else:\n            setattr(self, name, value)\n\n    def delete(self, dry_run=False):\n        if self.vpc_id:\n            return self.connection.delete_security_group(\n                group_id=self.id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.delete_security_group(\n                self.name,\n                dry_run=dry_run\n            )\n\n###The function: add_rule###\n    def remove_rule(self, ip_protocol, from_port, to_port,\n                    src_group_name, src_group_owner_id, cidr_ip,\n                    src_group_group_id, dry_run=False):\n        \"\"\"\n        Remove a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        if not self.rules:\n            raise ValueError(\"The security group has no rules\")\n\n        target_rule = None\n        for rule in self.rules:\n            if rule.ip_protocol == ip_protocol:\n                if rule.from_port == from_port:\n                    if rule.to_port == to_port:\n                        target_rule = rule\n                        target_grant = None\n                        for grant in rule.grants:\n                            if grant.name == src_group_name or grant.group_id == src_group_group_id:\n                                if grant.owner_id == src_group_owner_id:\n                                    if grant.cidr_ip == cidr_ip:\n                                        target_grant = grant\n                        if target_grant:\n                            rule.grants.remove(target_grant)\n            if len(rule.grants) == 0:\n                self.rules.remove(target_rule)\n\n    def authorize(self, ip_protocol=None, from_port=None, to_port=None,\n                  cidr_ip=None, src_group=None, dry_run=False):\n        \"\"\"\n        Add a new rule to this security group.\n        You need to pass in either src_group_name\n        OR ip_protocol, from_port, to_port,\n        and cidr_ip.  In other words, either you are authorizing another\n        group or you are authorizing some ip-based rule.\n\n        :type ip_protocol: string\n        :param ip_protocol: Either tcp | udp | icmp\n\n        :type from_port: int\n        :param from_port: The beginning port number you are enabling\n\n        :type to_port: int\n        :param to_port: The ending port number you are enabling\n\n        :type cidr_ip: string or list of strings\n        :param cidr_ip: The CIDR block you are providing access to.\n                        See http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing\n\n        :type src_group: :class:`boto.ec2.securitygroup.SecurityGroup` or\n                         :class:`boto.ec2.securitygroup.GroupOrCIDR`\n        :param src_group: The Security Group you are granting access to.\n\n        :rtype: bool\n        :return: True if successful.\n        \"\"\"\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.authorize_security_group(group_name,\n                                                          src_group_name,\n                                                          src_group_owner_id,\n                                                          ip_protocol,\n                                                          from_port,\n                                                          to_port,\n                                                          cidr_ip,\n                                                          group_id,\n                                                          src_group_group_id,\n                                                          dry_run=dry_run)\n        if status:\n            if not isinstance(cidr_ip, list):\n                cidr_ip = [cidr_ip]\n            for single_cidr_ip in cidr_ip:\n                self.add_rule(ip_protocol, from_port, to_port, src_group_name,\n                              src_group_owner_id, single_cidr_ip,\n                              src_group_group_id, dry_run=dry_run)\n        return status\n\n    def revoke(self, ip_protocol=None, from_port=None, to_port=None,\n               cidr_ip=None, src_group=None, dry_run=False):\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.revoke_security_group(group_name,\n                                                       src_group_name,\n                                                       src_group_owner_id,\n                                                       ip_protocol,\n                                                       from_port,\n                                                       to_port,\n                                                       cidr_ip,\n                                                       group_id,\n                                                       src_group_group_id,\n                                                       dry_run=dry_run)\n        if status:\n            self.remove_rule(ip_protocol, from_port, to_port, src_group_name,\n                             src_group_owner_id, cidr_ip, src_group_group_id,\n                             dry_run=dry_run)\n        return status\n\n    def copy_to_region(self, region, name=None, dry_run=False):\n        \"\"\"\n        Create a copy of this security group in another region.\n        Note that the new security group will be a separate entity\n        and will not stay in sync automatically after the copy\n        operation.\n\n        :type region: :class:`boto.ec2.regioninfo.RegionInfo`\n        :param region: The region to which this security group will be copied.\n\n        :type name: string\n        :param name: The name of the copy.  If not supplied, the copy\n                     will have the same name as this security group.\n\n        :rtype: :class:`boto.ec2.securitygroup.SecurityGroup`\n        :return: The new security group.\n        \"\"\"\n        if region.name == self.region:\n            raise BotoClientError('Unable to copy to the same Region')\n        conn_params = self.connection.get_params()\n        rconn = region.connect(**conn_params)\n        sg = rconn.create_security_group(\n            name or self.name,\n            self.description,\n            dry_run=dry_run\n        )\n        source_groups = []\n        for rule in self.rules:\n            for grant in rule.grants:\n                grant_nom = grant.name or grant.group_id\n                if grant_nom:\n                    if grant_nom not in source_groups:\n                        source_groups.append(grant_nom)\n                        sg.authorize(None, None, None, None, grant,\n                                     dry_run=dry_run)\n                else:\n                    sg.authorize(rule.ip_protocol, rule.from_port, rule.to_port,\n                                 grant.cidr_ip, dry_run=dry_run)\n        return sg\n\n    def instances(self, dry_run=False):\n        \"\"\"\n        Find all of the current instances that are running within this\n        security group.\n\n        :rtype: list of :class:`boto.ec2.instance.Instance`\n        :return: A list of Instance objects\n        \"\"\"\n        rs = []\n        if self.vpc_id:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'instance.group-id': self.id},\n                dry_run=dry_run\n            ))\n        else:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'group-id': self.id},\n                dry_run=dry_run\n            ))\n        instances = [i for r in rs for i in r.instances]\n        return instances\n\n\nclass IPPermissionsList(list):\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.append(IPPermissions(self))\n            return self[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        pass\n\n\nclass IPPermissions(object):\n\n    def __init__(self, parent=None):\n        self.parent = parent\n        self.ip_protocol = None\n        self.from_port = None\n        self.to_port = None\n        self.grants = []\n\n    def __repr__(self):\n        return 'IPPermissions:%s(%s-%s)' % (self.ip_protocol,\n                                            self.from_port, self.to_port)\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.grants.append(GroupOrCIDR(self))\n            return self.grants[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ipProtocol':\n            self.ip_protocol = value\n        elif name == 'fromPort':\n            self.from_port = value\n        elif name == 'toPort':\n            self.to_port = value\n        else:\n            setattr(self, name, value)\n\n    def add_grant(self, name=None, owner_id=None, cidr_ip=None, group_id=None,\n                  dry_run=False):\n        grant = GroupOrCIDR(self)\n        grant.owner_id = owner_id\n        grant.group_id = group_id\n        grant.name = name\n        grant.cidr_ip = cidr_ip\n        self.grants.append(grant)\n        return grant\n\n\nclass GroupOrCIDR(object):\n\n    def __init__(self, parent=None):\n        self.owner_id = None\n        self.group_id = None\n        self.name = None\n        self.cidr_ip = None\n\n    def __repr__(self):\n        if self.cidr_ip:\n            return '%s' % self.cidr_ip\n        else:\n            return '%s-%s' % (self.name or self.group_id, self.owner_id)\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'userId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.group_id = value\n        elif name == 'groupName':\n            self.name = value\n        if name == 'cidrIp':\n            self.cidr_ip = value\n        else:\n            setattr(self, name, value)\n", "prompt": "Please write a python function called 'add_rule' base the context. Add a rule to a SecurityGroup instance. Note that this method only changes the local version of the instance. No information is sent to EC2.:param self: SecurityGroup. An instance of the SecurityGroup class.\n:param ip_protocol: String. The IP protocol for the rule.\n:param from_port: Integer. The starting port range for the rule.\n:param to_port: Integer. The ending port range for the rule.\n:param src_group_name: String. The name of the source security group.\n:param src_group_owner_id: String. The ID of the owner of the source security group.\n:param cidr_ip: String. The CIDR IP range for the rule.\n:param src_group_group_id: String. The ID of the source security group.\n:param dry_run: Bool. Whether to perform a dry run. Defaults to False.\n:return: No return values..\n        The context you need to refer to is as follows: # Copyright (c) 2006-2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Security Group\n\"\"\"\nfrom boto.ec2.ec2object import TaggedEC2Object\nfrom boto.exception import BotoClientError\n\n\nclass SecurityGroup(TaggedEC2Object):\n\n    def __init__(self, connection=None, owner_id=None,\n                 name=None, description=None, id=None):\n        super(SecurityGroup, self).__init__(connection)\n        self.id = id\n        self.owner_id = owner_id\n        self.name = name\n        self.description = description\n        self.vpc_id = None\n        self.rules = IPPermissionsList()\n        self.rules_egress = IPPermissionsList()\n\n    def __repr__(self):\n        return 'SecurityGroup:%s' % self.name\n\n    def startElement(self, name, attrs, connection):\n        retval = super(SecurityGroup, self).startElement(name, attrs, connection)\n        if retval is not None:\n            return retval\n        if name == 'ipPermissions':\n            return self.rules\n        elif name == 'ipPermissionsEgress':\n            return self.rules_egress\n        else:\n            return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ownerId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.id = value\n        elif name == 'groupName':\n            self.name = value\n        elif name == 'vpcId':\n            self.vpc_id = value\n        elif name == 'groupDescription':\n            self.description = value\n        elif name == 'ipRanges':\n            pass\n        elif name == 'return':\n            if value == 'false':\n                self.status = False\n            elif value == 'true':\n                self.status = True\n            else:\n                raise Exception(\n                    'Unexpected value of status %s for group %s' % (\n                        value,\n                        self.name\n                    )\n                )\n        else:\n            setattr(self, name, value)\n\n    def delete(self, dry_run=False):\n        if self.vpc_id:\n            return self.connection.delete_security_group(\n                group_id=self.id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.delete_security_group(\n                self.name,\n                dry_run=dry_run\n            )\n\n###The function: add_rule###\n    def remove_rule(self, ip_protocol, from_port, to_port,\n                    src_group_name, src_group_owner_id, cidr_ip,\n                    src_group_group_id, dry_run=False):\n        \"\"\"\n        Remove a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        if not self.rules:\n            raise ValueError(\"The security group has no rules\")\n\n        target_rule = None\n        for rule in self.rules:\n            if rule.ip_protocol == ip_protocol:\n                if rule.from_port == from_port:\n                    if rule.to_port == to_port:\n                        target_rule = rule\n                        target_grant = None\n                        for grant in rule.grants:\n                            if grant.name == src_group_name or grant.group_id == src_group_group_id:\n                                if grant.owner_id == src_group_owner_id:\n                                    if grant.cidr_ip == cidr_ip:\n                                        target_grant = grant\n                        if target_grant:\n                            rule.grants.remove(target_grant)\n            if len(rule.grants) == 0:\n                self.rules.remove(target_rule)\n\n    def authorize(self, ip_protocol=None, from_port=None, to_port=None,\n                  cidr_ip=None, src_group=None, dry_run=False):\n        \"\"\"\n        Add a new rule to this security group.\n        You need to pass in either src_group_name\n        OR ip_protocol, from_port, to_port,\n        and cidr_ip.  In other words, either you are authorizing another\n        group or you are authorizing some ip-based rule.\n\n        :type ip_protocol: string\n        :param ip_protocol: Either tcp | udp | icmp\n\n        :type from_port: int\n        :param from_port: The beginning port number you are enabling\n\n        :type to_port: int\n        :param to_port: The ending port number you are enabling\n\n        :type cidr_ip: string or list of strings\n        :param cidr_ip: The CIDR block you are providing access to.\n                        See http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing\n\n        :type src_group: :class:`boto.ec2.securitygroup.SecurityGroup` or\n                         :class:`boto.ec2.securitygroup.GroupOrCIDR`\n        :param src_group: The Security Group you are granting access to.\n\n        :rtype: bool\n        :return: True if successful.\n        \"\"\"\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.authorize_security_group(group_name,\n                                                          src_group_name,\n                                                          src_group_owner_id,\n                                                          ip_protocol,\n                                                          from_port,\n                                                          to_port,\n                                                          cidr_ip,\n                                                          group_id,\n                                                          src_group_group_id,\n                                                          dry_run=dry_run)\n        if status:\n            if not isinstance(cidr_ip, list):\n                cidr_ip = [cidr_ip]\n            for single_cidr_ip in cidr_ip:\n                self.add_rule(ip_protocol, from_port, to_port, src_group_name,\n                              src_group_owner_id, single_cidr_ip,\n                              src_group_group_id, dry_run=dry_run)\n        return status\n\n    def revoke(self, ip_protocol=None, from_port=None, to_port=None,\n               cidr_ip=None, src_group=None, dry_run=False):\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.revoke_security_group(group_name,\n                                                       src_group_name,\n                                                       src_group_owner_id,\n                                                       ip_protocol,\n                                                       from_port,\n                                                       to_port,\n                                                       cidr_ip,\n                                                       group_id,\n                                                       src_group_group_id,\n                                                       dry_run=dry_run)\n        if status:\n            self.remove_rule(ip_protocol, from_port, to_port, src_group_name,\n                             src_group_owner_id, cidr_ip, src_group_group_id,\n                             dry_run=dry_run)\n        return status\n\n    def copy_to_region(self, region, name=None, dry_run=False):\n        \"\"\"\n        Create a copy of this security group in another region.\n        Note that the new security group will be a separate entity\n        and will not stay in sync automatically after the copy\n        operation.\n\n        :type region: :class:`boto.ec2.regioninfo.RegionInfo`\n        :param region: The region to which this security group will be copied.\n\n        :type name: string\n        :param name: The name of the copy.  If not supplied, the copy\n                     will have the same name as this security group.\n\n        :rtype: :class:`boto.ec2.securitygroup.SecurityGroup`\n        :return: The new security group.\n        \"\"\"\n        if region.name == self.region:\n            raise BotoClientError('Unable to copy to the same Region')\n        conn_params = self.connection.get_params()\n        rconn = region.connect(**conn_params)\n        sg = rconn.create_security_group(\n            name or self.name,\n            self.description,\n            dry_run=dry_run\n        )\n        source_groups = []\n        for rule in self.rules:\n            for grant in rule.grants:\n                grant_nom = grant.name or grant.group_id\n                if grant_nom:\n                    if grant_nom not in source_groups:\n                        source_groups.append(grant_nom)\n                        sg.authorize(None, None, None, None, grant,\n                                     dry_run=dry_run)\n                else:\n                    sg.authorize(rule.ip_protocol, rule.from_port, rule.to_port,\n                                 grant.cidr_ip, dry_run=dry_run)\n        return sg\n\n    def instances(self, dry_run=False):\n        \"\"\"\n        Find all of the current instances that are running within this\n        security group.\n\n        :rtype: list of :class:`boto.ec2.instance.Instance`\n        :return: A list of Instance objects\n        \"\"\"\n        rs = []\n        if self.vpc_id:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'instance.group-id': self.id},\n                dry_run=dry_run\n            ))\n        else:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'group-id': self.id},\n                dry_run=dry_run\n            ))\n        instances = [i for r in rs for i in r.instances]\n        return instances\n\n\nclass IPPermissionsList(list):\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.append(IPPermissions(self))\n            return self[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        pass\n\n\nclass IPPermissions(object):\n\n    def __init__(self, parent=None):\n        self.parent = parent\n        self.ip_protocol = None\n        self.from_port = None\n        self.to_port = None\n        self.grants = []\n\n    def __repr__(self):\n        return 'IPPermissions:%s(%s-%s)' % (self.ip_protocol,\n                                            self.from_port, self.to_port)\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.grants.append(GroupOrCIDR(self))\n            return self.grants[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ipProtocol':\n            self.ip_protocol = value\n        elif name == 'fromPort':\n            self.from_port = value\n        elif name == 'toPort':\n            self.to_port = value\n        else:\n            setattr(self, name, value)\n\n    def add_grant(self, name=None, owner_id=None, cidr_ip=None, group_id=None,\n                  dry_run=False):\n        grant = GroupOrCIDR(self)\n        grant.owner_id = owner_id\n        grant.group_id = group_id\n        grant.name = name\n        grant.cidr_ip = cidr_ip\n        self.grants.append(grant)\n        return grant\n\n\nclass GroupOrCIDR(object):\n\n    def __init__(self, parent=None):\n        self.owner_id = None\n        self.group_id = None\n        self.name = None\n        self.cidr_ip = None\n\n    def __repr__(self):\n        if self.cidr_ip:\n            return '%s' % self.cidr_ip\n        else:\n            return '%s-%s' % (self.name or self.group_id, self.owner_id)\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'userId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.group_id = value\n        elif name == 'groupName':\n            self.name = value\n        if name == 'cidrIp':\n            self.cidr_ip = value\n        else:\n            setattr(self, name, value)\n", "test_list": ["def test_add_rule(self):\n    sg = SecurityGroup()\n    self.assertEqual(len(sg.rules), 0)\n    sg.add_rule(ip_protocol='http', from_port='80', to_port='8080', src_group_name='groupy', src_group_owner_id='12345', cidr_ip='10.0.0.1', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add_rule' function should accept valid input types for each parameter and ensure that the rule is correctly added to the 'rules' list of the SecurityGroup instance.", "unit_test": "def test_add_rule_input_output_conditions(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)\n    rule = sg.rules[0]\n    self.assertEqual(rule.ip_protocol, 'tcp')\n    self.assertEqual(rule.from_port, 22)\n    self.assertEqual(rule.to_port, 22)\n    self.assertEqual(rule.grants[0].cidr_ip, '192.168.1.0/24')", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_input_output_conditions"}, "Exception Handling": {"requirement": "The 'add_rule' function should raise a ValueError if any of the required parameters are missing or invalid.", "unit_test": "def test_add_rule_exception_handling(self):\n    sg = SecurityGroup()\n    with self.assertRaises(ValueError):\n        sg.add_rule(ip_protocol=None, from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_exception_handling"}, "Edge Case Handling": {"requirement": "The 'add_rule' function should handle edge cases such as adding a rule with the same parameters multiple times without duplication.", "unit_test": "def test_add_rule_edge_case_handling(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'add_rule' function to allow adding multiple CIDR IP ranges in a single call.", "unit_test": "def test_add_rule_functionality_extension(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip=['192.168.1.0/24', '10.0.0.0/24'], src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)\n    self.assertEqual(len(sg.rules[0].grants), 2)", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'add_rule' function has complete parameter type annotations and a detailed docstring explaining its behavior.", "unit_test": "def test_add_rule_annotation_coverage(self):\n    sg = SecurityGroup()\n    self.assertTrue(hasattr(sg.add_rule, '__annotations__'))\n    self.assertIn('ip_protocol', sg.add_rule.__annotations__)\n    self.assertIn('from_port', sg.add_rule.__annotations__)\n    self.assertIn('to_port', sg.add_rule.__annotations__)\n    self.assertIn('src_group_name', sg.add_rule.__annotations__)\n    self.assertIn('src_group_owner_id', sg.add_rule.__annotations__)\n    self.assertIn('cidr_ip', sg.add_rule.__annotations__)\n    self.assertIn('src_group_group_id', sg.add_rule.__annotations__)\n    self.assertIn('dry_run', sg.add_rule.__annotations__)", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_annotation_coverage"}, "Code Complexity": {"requirement": "The 'add_rule' function should maintain a cyclomatic complexity of no more than 5 to ensure readability and maintainability.", "unit_test": "def test_add_rule_code_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(SecurityGroup.add_rule)\n    complexity = cc_visit(source_code)\n    self.assertTrue(all(c.complexity <= 5 for c in complexity))", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'add_rule' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_add_rule_code_standard(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_your_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'add_rule' function correctly utilizes the 'rules' attribute of the SecurityGroup class.", "unit_test": "def test_add_rule_context_usage_verification(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertIsInstance(sg.rules, IPPermissionsList)\n    self.assertEqual(len(sg.rules), 1)", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'add_rule' function correctly adds an IPPermissions object to the 'rules' list with the specified parameters.", "unit_test": "def test_add_rule_context_usage_correctness_verification(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    rule = sg.rules[0]\n    self.assertEqual(rule.ip_protocol, 'tcp')\n    self.assertEqual(rule.from_port, 22)\n    self.assertEqual(rule.to_port, 22)\n    self.assertEqual(rule.grants[0].cidr_ip, '192.168.1.0/24')", "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_context_usage_correctness_verification"}}}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "type": "method", "project_path": "Scientific-Engineering/bentoml", "completion_path": "Scientific-Engineering/bentoml/src/bentoml/_internal/runner/container.py", "signature_position": [550, 554], "body_position": [555, 556], "dependency": {"intra_class": ["bentoml._internal.runner.container.DefaultContainer.batches_to_batch"], "intra_file": ["bentoml._internal.runner.container.Payload"], "cross_file": []}, "requirement": {"Functionality": "This function takes a sequence of payloads and converts them into batches. It creates a list of batches on each payload in the sequence. Then, it combines the batches into a single batch along the specified batch dimension.", "Arguments": ":param cls: DefaultContainer. The class itself.\n:param payloads: Sequence of Payload. The payloads to be converted into batches.\n:param batch_dim: int. The dimension along which the batches will be combined. Defaults to 0.\n:return: tuple[list[Any], list[int]]. A tuple containing the list of batches and a list of integers representing the batch sizes."}, "tests": ["tests/unit/_internal/runner/test_container.py::test_default_container"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[list[t.Any], list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n", "context": "from __future__ import annotations\n\nimport abc\nimport base64\nimport io\nimport itertools\nimport pickle\nimport typing as t\n\nfrom ..types import LazyType\nfrom ..utils import LazyLoader\nfrom ..utils.pickle import fixed_torch_loads\nfrom ..utils.pickle import pep574_dumps\nfrom ..utils.pickle import pep574_loads\n\nSingleType = t.TypeVar(\"SingleType\")\nBatchType = t.TypeVar(\"BatchType\")\n\nTRITON_EXC_MSG = \"tritonclient is required to use triton with BentoML. Install with 'pip install \\\"tritonclient[all]>=2.29.0\\\"'.\"\n\nif t.TYPE_CHECKING:\n    import tritonclient.grpc.aio as tritongrpcclient\n    import tritonclient.http.aio as tritonhttpclient\n\n    from .. import external_typing as ext\nelse:\n    np = LazyLoader(\"np\", globals(), \"numpy\")\n    tritongrpcclient = LazyLoader(\n        \"tritongrpcclient\", globals(), \"tritonclient.grpc.aio\", exc_msg=TRITON_EXC_MSG\n    )\n    tritonhttpclient = LazyLoader(\n        \"tritonhttpclient\", globals(), \"tritonclient.http.aio\", exc_msg=TRITON_EXC_MSG\n    )\n\n\nclass Payload(t.NamedTuple):\n    data: bytes\n    meta: dict[str, bool | int | float | str | list[int]]\n    container: str\n    batch_size: int = -1\n\n\nclass DataContainer(t.Generic[SingleType, BatchType]):\n    @classmethod\n    def create_payload(\n        cls,\n        data: bytes,\n        batch_size: int,\n        meta: dict[str, bool | int | float | str | list[int]] | None = None,\n    ) -> Payload:\n        return Payload(data, meta or {}, container=cls.__name__, batch_size=batch_size)\n\n    @classmethod\n    @abc.abstractmethod\n    def to_payload(cls, batch: BatchType, batch_dim: int) -> Payload:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_payload(cls, payload: Payload) -> BatchType:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: SingleType, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return cls.to_triton_http_payload(inp, meta)\n        else:\n            return cls.to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: SingleType,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via gRPC client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: SingleType,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via HTTP client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[BatchType]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n\nclass TritonInferInputDataContainer(\n    DataContainer[\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n    ]\n):\n    @classmethod\n    def to_payload(cls, batch: tritongrpcclient.InferInput, batch_dim: int) -> Payload:\n        raise NotImplementedError\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> tritongrpcclient.InferInput:\n        raise NotImplementedError\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        return t.cast(\"tritongrpcclient.InferInput\", inp)\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        return t.cast(\"tritonhttpclient.InferInput\", inp)\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[tritongrpcclient.InferInput | tritonhttpclient.InferInput],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[tritongrpcclient.InferInput | tritonhttpclient.InferInput]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[Payload]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n\nclass NdarrayContainer(DataContainer[\"ext.NpNDArray\", \"ext.NpNDArray\"]):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.NpNDArray]:\n        return np.split(batch, indices[1:-1], axis=batch_dim)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        InferInput = tritongrpcclient.InferInput(\n            meta.name, inp.shape, tritongrpcclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        InferInput = tritonhttpclient.InferInput(\n            meta[\"name\"], inp.shape, tritonhttpclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array\n        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PandasDataFrameContainer(\n    DataContainer[t.Union[\"ext.PdDataFrame\", \"ext.PdSeries\"], \"ext.PdDataFrame\"]\n):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:\n        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PILImageContainer(DataContainer[\"ext.PILImage\", \"ext.PILImage\"]):\n    _error = (\n        \"PIL.Image doesn't support batch inference.\"\n        \"You can convert it to numpy.ndarray before passing to the runner.\"\n    )\n\n    @classmethod\n    def to_payload(cls, batch: ext.PILImage, batch_dim: int) -> Payload:\n        buffer = io.BytesIO()\n        batch.save(buffer, format=batch.format)\n        return cls.create_payload(buffer.getvalue(), batch_size=1)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> ext.PILImage:\n        from ..io_descriptors.image import PIL\n\n        return PIL.Image.open(io.BytesIO(payload.data))\n\n    @classmethod\n    def batch_to_payloads(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[ext.PILImage], batch_dim: int\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[ext.PILImage]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n\nclass DefaultContainer(DataContainer[t.Any, t.List[t.Any]]):\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        return fixed_torch_loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n###The function: from_batch_payloads###\n\nclass DataContainerRegistry:\n    CONTAINER_SINGLE_TYPE_MAP: t.Dict[\n        LazyType[t.Any], t.Type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n    CONTAINER_BATCH_TYPE_MAP: t.Dict[\n        LazyType[t.Any], type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n\n    @classmethod\n    def register_container(\n        cls,\n        single_type: LazyType[t.Any] | type,\n        batch_type: LazyType[t.Any] | type,\n        container_cls: t.Type[DataContainer[t.Any, t.Any]],\n    ):\n        single_type = LazyType.from_type(single_type)\n        batch_type = LazyType.from_type(batch_type)\n\n        cls.CONTAINER_BATCH_TYPE_MAP[batch_type] = container_cls\n        cls.CONTAINER_SINGLE_TYPE_MAP[single_type] = container_cls\n\n    @classmethod\n    def find_by_single_type(\n        cls, type_: t.Type[SingleType] | LazyType[SingleType]\n    ) -> t.Type[DataContainer[SingleType, t.Any]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_SINGLE_TYPE_MAP:\n            return cls.CONTAINER_SINGLE_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_SINGLE_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_SINGLE_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_batch_type(\n        cls, type_: t.Type[BatchType] | LazyType[BatchType]\n    ) -> t.Type[DataContainer[t.Any, BatchType]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_BATCH_TYPE_MAP:\n            return cls.CONTAINER_BATCH_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_BATCH_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_BATCH_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_name(cls, name: str) -> t.Type[DataContainer[t.Any, t.Any]]:\n        for container_cls in cls.CONTAINER_BATCH_TYPE_MAP.values():\n            if container_cls.__name__ == name:\n                return container_cls\n        if name == DefaultContainer.__name__:\n            return DefaultContainer\n        raise ValueError(f\"can not find specified container class by name {name}\")\n\n\ndef register_builtin_containers():\n    DataContainerRegistry.register_container(\n        LazyType(\"numpy\", \"ndarray\"), LazyType(\"numpy\", \"ndarray\"), NdarrayContainer\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.series\", \"Series\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"PIL.Image\", \"Image\"),\n        LazyType(\"PIL.Image\", \"Image\"),\n        PILImageContainer,\n    )\n\n\nregister_builtin_containers()\n\n\nclass AutoContainer(DataContainer[t.Any, t.Any]):\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.to_payload(batch, batch_dim)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        container_cls = DataContainerRegistry.find_by_name(payload.container)\n        return container_cls.from_payload(payload)\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: t.Any, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_http_payload(inp, meta)\n        else:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int = 0\n    ) -> tuple[BatchType, list[int]]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batches[0]))\n        return container_cls.batches_to_batch(batches, batch_dim)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[BatchType]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_batches(batch, indices, batch_dim)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: t.Any,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_payloads(batch, indices, batch_dim)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[t.Any, list[int]]:\n        container_cls = DataContainerRegistry.find_by_name(payloads[0].container)\n        return container_cls.from_batch_payloads(payloads, batch_dim)\n", "prompt": "Please write a python function called 'from_batch_payloads' base the context. This function takes a sequence of payloads and converts them into batches. It creates a list of batches on each payload in the sequence. Then, it combines the batches into a single batch along the specified batch dimension.:param cls: DefaultContainer. The class itself.\n:param payloads: Sequence of Payload. The payloads to be converted into batches.\n:param batch_dim: int. The dimension along which the batches will be combined. Defaults to 0.\n:return: tuple[list[Any], list[int]]. A tuple containing the list of batches and a list of integers representing the batch sizes..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nimport abc\nimport base64\nimport io\nimport itertools\nimport pickle\nimport typing as t\n\nfrom ..types import LazyType\nfrom ..utils import LazyLoader\nfrom ..utils.pickle import fixed_torch_loads\nfrom ..utils.pickle import pep574_dumps\nfrom ..utils.pickle import pep574_loads\n\nSingleType = t.TypeVar(\"SingleType\")\nBatchType = t.TypeVar(\"BatchType\")\n\nTRITON_EXC_MSG = \"tritonclient is required to use triton with BentoML. Install with 'pip install \\\"tritonclient[all]>=2.29.0\\\"'.\"\n\nif t.TYPE_CHECKING:\n    import tritonclient.grpc.aio as tritongrpcclient\n    import tritonclient.http.aio as tritonhttpclient\n\n    from .. import external_typing as ext\nelse:\n    np = LazyLoader(\"np\", globals(), \"numpy\")\n    tritongrpcclient = LazyLoader(\n        \"tritongrpcclient\", globals(), \"tritonclient.grpc.aio\", exc_msg=TRITON_EXC_MSG\n    )\n    tritonhttpclient = LazyLoader(\n        \"tritonhttpclient\", globals(), \"tritonclient.http.aio\", exc_msg=TRITON_EXC_MSG\n    )\n\n\nclass Payload(t.NamedTuple):\n    data: bytes\n    meta: dict[str, bool | int | float | str | list[int]]\n    container: str\n    batch_size: int = -1\n\n\nclass DataContainer(t.Generic[SingleType, BatchType]):\n    @classmethod\n    def create_payload(\n        cls,\n        data: bytes,\n        batch_size: int,\n        meta: dict[str, bool | int | float | str | list[int]] | None = None,\n    ) -> Payload:\n        return Payload(data, meta or {}, container=cls.__name__, batch_size=batch_size)\n\n    @classmethod\n    @abc.abstractmethod\n    def to_payload(cls, batch: BatchType, batch_dim: int) -> Payload:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_payload(cls, payload: Payload) -> BatchType:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: SingleType, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return cls.to_triton_http_payload(inp, meta)\n        else:\n            return cls.to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: SingleType,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via gRPC client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: SingleType,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via HTTP client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[BatchType]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n\nclass TritonInferInputDataContainer(\n    DataContainer[\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n    ]\n):\n    @classmethod\n    def to_payload(cls, batch: tritongrpcclient.InferInput, batch_dim: int) -> Payload:\n        raise NotImplementedError\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> tritongrpcclient.InferInput:\n        raise NotImplementedError\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        return t.cast(\"tritongrpcclient.InferInput\", inp)\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        return t.cast(\"tritonhttpclient.InferInput\", inp)\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[tritongrpcclient.InferInput | tritonhttpclient.InferInput],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[tritongrpcclient.InferInput | tritonhttpclient.InferInput]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[Payload]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n\nclass NdarrayContainer(DataContainer[\"ext.NpNDArray\", \"ext.NpNDArray\"]):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.NpNDArray]:\n        return np.split(batch, indices[1:-1], axis=batch_dim)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        InferInput = tritongrpcclient.InferInput(\n            meta.name, inp.shape, tritongrpcclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        InferInput = tritonhttpclient.InferInput(\n            meta[\"name\"], inp.shape, tritonhttpclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array\n        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PandasDataFrameContainer(\n    DataContainer[t.Union[\"ext.PdDataFrame\", \"ext.PdSeries\"], \"ext.PdDataFrame\"]\n):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:\n        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PILImageContainer(DataContainer[\"ext.PILImage\", \"ext.PILImage\"]):\n    _error = (\n        \"PIL.Image doesn't support batch inference.\"\n        \"You can convert it to numpy.ndarray before passing to the runner.\"\n    )\n\n    @classmethod\n    def to_payload(cls, batch: ext.PILImage, batch_dim: int) -> Payload:\n        buffer = io.BytesIO()\n        batch.save(buffer, format=batch.format)\n        return cls.create_payload(buffer.getvalue(), batch_size=1)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> ext.PILImage:\n        from ..io_descriptors.image import PIL\n\n        return PIL.Image.open(io.BytesIO(payload.data))\n\n    @classmethod\n    def batch_to_payloads(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[ext.PILImage], batch_dim: int\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[ext.PILImage]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n\nclass DefaultContainer(DataContainer[t.Any, t.List[t.Any]]):\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        return fixed_torch_loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n###The function: from_batch_payloads###\n\nclass DataContainerRegistry:\n    CONTAINER_SINGLE_TYPE_MAP: t.Dict[\n        LazyType[t.Any], t.Type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n    CONTAINER_BATCH_TYPE_MAP: t.Dict[\n        LazyType[t.Any], type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n\n    @classmethod\n    def register_container(\n        cls,\n        single_type: LazyType[t.Any] | type,\n        batch_type: LazyType[t.Any] | type,\n        container_cls: t.Type[DataContainer[t.Any, t.Any]],\n    ):\n        single_type = LazyType.from_type(single_type)\n        batch_type = LazyType.from_type(batch_type)\n\n        cls.CONTAINER_BATCH_TYPE_MAP[batch_type] = container_cls\n        cls.CONTAINER_SINGLE_TYPE_MAP[single_type] = container_cls\n\n    @classmethod\n    def find_by_single_type(\n        cls, type_: t.Type[SingleType] | LazyType[SingleType]\n    ) -> t.Type[DataContainer[SingleType, t.Any]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_SINGLE_TYPE_MAP:\n            return cls.CONTAINER_SINGLE_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_SINGLE_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_SINGLE_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_batch_type(\n        cls, type_: t.Type[BatchType] | LazyType[BatchType]\n    ) -> t.Type[DataContainer[t.Any, BatchType]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_BATCH_TYPE_MAP:\n            return cls.CONTAINER_BATCH_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_BATCH_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_BATCH_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_name(cls, name: str) -> t.Type[DataContainer[t.Any, t.Any]]:\n        for container_cls in cls.CONTAINER_BATCH_TYPE_MAP.values():\n            if container_cls.__name__ == name:\n                return container_cls\n        if name == DefaultContainer.__name__:\n            return DefaultContainer\n        raise ValueError(f\"can not find specified container class by name {name}\")\n\n\ndef register_builtin_containers():\n    DataContainerRegistry.register_container(\n        LazyType(\"numpy\", \"ndarray\"), LazyType(\"numpy\", \"ndarray\"), NdarrayContainer\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.series\", \"Series\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"PIL.Image\", \"Image\"),\n        LazyType(\"PIL.Image\", \"Image\"),\n        PILImageContainer,\n    )\n\n\nregister_builtin_containers()\n\n\nclass AutoContainer(DataContainer[t.Any, t.Any]):\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.to_payload(batch, batch_dim)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        container_cls = DataContainerRegistry.find_by_name(payload.container)\n        return container_cls.from_payload(payload)\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: t.Any, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_http_payload(inp, meta)\n        else:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int = 0\n    ) -> tuple[BatchType, list[int]]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batches[0]))\n        return container_cls.batches_to_batch(batches, batch_dim)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[BatchType]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_batches(batch, indices, batch_dim)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: t.Any,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_payloads(batch, indices, batch_dim)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[t.Any, list[int]]:\n        container_cls = DataContainerRegistry.find_by_name(payloads[0].container)\n        return container_cls.from_batch_payloads(payloads, batch_dim)\n", "test_list": ["@pytest.mark.parametrize('batch_dim_exc', [AssertionError])\n@pytest.mark.parametrize('wrong_batch_dim', [1, 19])\ndef test_default_container(batch_dim_exc: t.Type[Exception], wrong_batch_dim: int):\n    l1 = [1, 2, 3]\n    l2 = [3, 4, 5, 6]\n    batch, indices = c.DefaultContainer.batches_to_batch([l1, l2])\n    assert batch == l1 + l2\n    assert indices == [0, 3, 7]\n    restored_l1, restored_l2 = c.DefaultContainer.batch_to_batches(batch, indices)\n    assert restored_l1 == l1\n    assert restored_l2 == l2\n    with pytest.raises(batch_dim_exc):\n        c.DefaultContainer.batches_to_batch([l1, l2], batch_dim=wrong_batch_dim)\n    with pytest.raises(batch_dim_exc):\n        c.DefaultContainer.batch_to_batches(batch, indices, batch_dim=wrong_batch_dim)\n\n    def _generator():\n        yield 'apple'\n        yield 'banana'\n        yield 'cherry'\n    assert c.DefaultContainer.from_payload(c.DefaultContainer.to_payload(_generator(), batch_dim=0)) == list(_generator())\n    assert c.DefaultContainer.from_batch_payloads(c.DefaultContainer.batch_to_payloads(batch, indices)) == (batch, indices)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'from_batch_payloads' should correctly convert a sequence of Payload objects into a single batch and a list of batch sizes, ensuring the output types match the expected return types.", "unit_test": "def test_from_batch_payloads_output_type():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=4)]\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n    assert isinstance(batch, list)\n    assert isinstance(batch_sizes, list)\n    assert all(isinstance(size, int) for size in batch_sizes)", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_output_type"}, "Exception Handling": {"requirement": "The function 'from_batch_payloads' should raise a ValueError if the payloads have inconsistent container types.", "unit_test": "def test_from_batch_payloads_inconsistent_container():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='AnotherContainer', batch_size=4)]\n    with pytest.raises(ValueError):\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=0)", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_inconsistent_container"}, "Edge Case Handling": {"requirement": "The function 'from_batch_payloads' should handle an empty sequence of payloads gracefully, returning an empty batch and an empty list of batch sizes.", "unit_test": "def test_from_batch_payloads_empty_sequence():\n    payloads = []\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n    assert batch == []\n    assert batch_sizes == []", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_empty_sequence"}, "Functionality Extension": {"requirement": "Extend the 'from_batch_payloads' function to support an optional parameter 'validate' that, when set to True, checks if all payloads have the same batch size and raises an AssertionError if not.", "unit_test": "def test_from_batch_payloads_validate_batch_size():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=3)]\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0, validate=True)\n    assert batch_sizes == [3, 3]\n\n    payloads_mismatch = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                         Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=4)]\n    with pytest.raises(AssertionError):\n        DefaultContainer.from_batch_payloads(payloads_mismatch, batch_dim=0, validate=True)", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_validate_batch_size"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'from_batch_payloads' function are annotated with type hints.", "unit_test": "def test_from_batch_payloads_annotations():\n    annotations = DefaultContainer.from_batch_payloads.__annotations__\n    assert 'payloads' in annotations\n    assert annotations['payloads'] == t.Sequence[Payload]\n    assert 'batch_dim' in annotations\n    assert annotations['batch_dim'] == int\n    assert 'return' in annotations\n    assert annotations['return'] == t.Tuple[t.Any, list[int]]", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'from_batch_payloads' function should not exceed 5.", "unit_test": "def test_from_batch_payloads_complexity():\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(DefaultContainer.from_batch_payloads)\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 10", "test": "tests/unit/_internal/runner/test_container.py::test_code_complexity"}, "Code Standard": {"requirement": "The 'from_batch_payloads' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_from_batch_payloads_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/module.py'])\n    assert result.total_errors == 0", "test": "tests/unit/_internal/runner/test_container.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'from_batch_payloads' function should utilize the 'batches_to_batch' method from the 'DefaultContainer' class to combine batches.", "unit_test": "def test_from_batch_payloads_uses_batches_to_batch():\n    with mock.patch('bentoml._internal.runner.container.DefaultContainer.batches_to_batch') as mock_batches_to_batch:\n        payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3)]\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n        mock_batches_to_batch.assert_called_once()", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_uses_batches_to_batch"}, "Context Usage Correctness Verification": {"requirement": "The 'from_batch_payloads' function should correctly pass the batch dimension parameter to the 'batches_to_batch' method.", "unit_test": "def test_from_batch_payloads_correct_batch_dim_usage():\n    with mock.patch('bentoml._internal.runner.container.DefaultContainer.batches_to_batch') as mock_batches_to_batch:\n        payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3)]\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=2)\n        mock_batches_to_batch.assert_called_with(mock.ANY, 2)", "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_correct_batch_dim_usage"}}}
{"namespace": "sqlitedict.SqliteDict.commit", "type": "method", "project_path": "Database/sqlitedict", "completion_path": "Database/sqlitedict/sqlitedict.py", "signature_position": [370, 370], "body_position": [377, 378], "dependency": {"intra_class": ["sqlitedict.SqliteDict.conn"], "intra_file": ["sqlitedict.SqliteMultithread.commit"], "cross_file": []}, "requirement": {"Functionality": "This function is used to persist all data in the SqliteDict instance to disk. It commits the changes made to the database. If `blocking` is set to False, the commit command is queued but the data is not guaranteed to be persisted immediately.", "Arguments": ":param self: SqliteDict. An instance of the SqliteDict class.\n:param blocking: Bool. Whether to block until the commit is complete. Defaults to True.\n:return: No return values."}, "tests": ["tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_overwrite_using_flag_w", "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_readonly", "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_default_reuse_existing_flag_c"], "indent": 4, "domain": "Database", "code": "    def commit(self, blocking=True):\n        \"\"\"\n        Persist all data to disk.\n\n        When `blocking` is False, the commit command is queued, but the data is\n        not guaranteed persisted (default implication when autocommit=True).\n        \"\"\"\n        if self.conn is not None:\n            self.conn.commit(blocking)\n", "context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# This code is distributed under the terms and conditions\n# from the Apache License, Version 2.0\n#\n# http://opensource.org/licenses/apache2.0.php\n#\n# This code was inspired by:\n#  * http://code.activestate.com/recipes/576638-draft-for-an-sqlite3-based-dbm/\n#  * http://code.activestate.com/recipes/526618/\n\n\"\"\"\nA lightweight wrapper around Python's sqlite3 database, with a dict-like interface\nand multi-thread access support::\n\n>>> mydict = SqliteDict('some.db', autocommit=True) # the mapping will be persisted to file `some.db`\n>>> mydict['some_key'] = any_picklable_object\n>>> print mydict['some_key']\n>>> print len(mydict) # etc... all dict functions work\n\nPickle is used internally to serialize the values. Keys are strings.\n\nIf you don't use autocommit (default is no autocommit for performance), then\ndon't forget to call `mydict.commit()` when done with a transaction.\n\n\"\"\"\n\nimport sqlite3\nimport os\nimport sys\nimport tempfile\nimport threading\nimport logging\nimport traceback\nfrom base64 import b64decode, b64encode\nimport weakref\n\n__version__ = '2.1.0'\n\n\ndef reraise(tp, value, tb=None):\n    if value is None:\n        value = tp()\n    if value.__traceback__ is not tb:\n        raise value.with_traceback(tb)\n    raise value\n\n\ntry:\n    from cPickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\nexcept ImportError:\n    from pickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\n\n# some Python 3 vs 2 imports\ntry:\n    from collections import UserDict as DictClass\nexcept ImportError:\n    from UserDict import DictMixin as DictClass\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\n\nlogger = logging.getLogger(__name__)\n\n#\n# There's a thread that holds the actual SQL connection (SqliteMultithread).\n# We communicate with this thread via queues (request and responses).\n# The requests can either be SQL commands or one of the \"special\" commands\n# below:\n#\n# _REQUEST_CLOSE: request that the SQL connection be closed\n# _REQUEST_COMMIT: request that any changes be committed to the DB\n#\n# Responses are either SQL records (e.g. results of a SELECT) or the magic\n# _RESPONSE_NO_MORE command, which indicates nothing else will ever be written\n# to the response queue.\n#\n_REQUEST_CLOSE = '--close--'\n_REQUEST_COMMIT = '--commit--'\n_RESPONSE_NO_MORE = '--no more--'\n\n#\n# We work with weak references for better memory efficiency.\n# Dereferencing, checking the referent queue still exists, and putting to it\n# is boring and repetitive, so we have a _put function to handle it for us.\n#\n_PUT_OK, _PUT_REFERENT_DESTROYED, _PUT_NOOP = 0, 1, 2\n\n\ndef _put(queue_reference, item):\n    if queue_reference is not None:\n        queue = queue_reference()\n        if queue is None:\n            #\n            # We got a reference to a queue, but that queue no longer exists\n            #\n            retval = _PUT_REFERENT_DESTROYED\n        else:\n            queue.put(item)\n            retval = _PUT_OK\n\n        del queue\n        return retval\n\n    #\n    # We didn't get a reference to a queue, so do nothing (no-op).\n    #\n    return _PUT_NOOP\n\n\ndef open(*args, **kwargs):\n    \"\"\"See documentation of the SqliteDict class.\"\"\"\n    return SqliteDict(*args, **kwargs)\n\n\ndef encode(obj):\n    \"\"\"Serialize an object using pickle to a binary format accepted by SQLite.\"\"\"\n    return sqlite3.Binary(dumps(obj, protocol=PICKLE_PROTOCOL))\n\n\ndef decode(obj):\n    \"\"\"Deserialize objects retrieved from SQLite.\"\"\"\n    return loads(bytes(obj))\n\n\ndef encode_key(key):\n    \"\"\"Serialize a key using pickle + base64 encoding to text accepted by SQLite.\"\"\"\n    return b64encode(dumps(key, protocol=PICKLE_PROTOCOL)).decode(\"ascii\")\n\n\ndef decode_key(key):\n    \"\"\"Deserialize a key retrieved from SQLite.\"\"\"\n    return loads(b64decode(key.encode(\"ascii\")))\n\n\ndef identity(obj):\n    \"\"\"Identity f(x) = x function for encoding/decoding.\"\"\"\n    return obj\n\n\nclass SqliteDict(DictClass):\n    VALID_FLAGS = ['c', 'r', 'w', 'n']\n\n    def __init__(self, filename=None, tablename='unnamed', flag='c',\n                 autocommit=False, journal_mode=\"DELETE\", encode=encode,\n                 decode=decode, encode_key=identity, decode_key=identity,\n                 timeout=5, outer_stack=True):\n        \"\"\"\n        Initialize a thread-safe sqlite-backed dictionary. The dictionary will\n        be a table `tablename` in database file `filename`. A single file (=database)\n        may contain multiple tables.\n\n        If no `filename` is given, a random file in temp will be used (and deleted\n        from temp once the dict is closed/deleted).\n\n        If you enable `autocommit`, changes will be committed after each operation\n        (more inefficient but safer). Otherwise, changes are committed on `self.commit()`,\n        `self.clear()` and `self.close()`.\n\n        Set `journal_mode` to 'OFF' if you're experiencing sqlite I/O problems\n        or if you need performance and don't care about crash-consistency.\n\n        Set `outer_stack` to False to disable the output of the outer exception\n        to the error logs.  This may improve the efficiency of sqlitedict\n        operation at the expense of a detailed exception trace.\n\n        The `flag` parameter. Exactly one of:\n          'c': default mode, open for read/write, creating the db/table if necessary.\n          'w': open for r/w, but drop `tablename` contents first (start with empty table)\n          'r': open as read-only\n          'n': create a new database (erasing any existing tables, not just `tablename`!).\n\n        The `encode` and `decode` parameters are used to customize how the values\n        are serialized and deserialized.\n        The `encode` parameter must be a function that takes a single Python\n        object and returns a serialized representation.\n        The `decode` function must be a function that takes the serialized\n        representation produced by `encode` and returns a deserialized Python\n        object.\n        The default is to use pickle.\n\n        The `timeout` defines the maximum time (in seconds) to wait for initial Thread startup.\n\n        \"\"\"\n        self.in_temp = filename is None\n        if self.in_temp:\n            fd, filename = tempfile.mkstemp(prefix='sqldict')\n            os.close(fd)\n\n        if flag not in SqliteDict.VALID_FLAGS:\n            raise RuntimeError(\"Unrecognized flag: %s\" % flag)\n        self.flag = flag\n\n        if flag == 'n':\n            if os.path.exists(filename):\n                os.remove(filename)\n\n        dirname = os.path.dirname(filename)\n        if dirname:\n            if not os.path.exists(dirname):\n                raise RuntimeError('Error! The directory does not exist, %s' % dirname)\n\n        self.filename = filename\n\n        # Use standard SQL escaping of double quote characters in identifiers, by doubling them.\n        # See https://github.com/RaRe-Technologies/sqlitedict/pull/113\n        self.tablename = tablename.replace('\"', '\"\"')\n\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        self.encode = encode\n        self.decode = decode\n        self.encode_key = encode_key\n        self.decode_key = decode_key\n        self._outer_stack = outer_stack\n\n        logger.debug(\"opening Sqlite table %r in %r\" % (tablename, filename))\n        self.conn = self._new_conn()\n        if self.flag == 'r':\n            if self.tablename not in SqliteDict.get_tablenames(self.filename):\n                msg = 'Refusing to create a new table \"%s\" in read-only DB mode' % tablename\n                raise RuntimeError(msg)\n        else:\n            MAKE_TABLE = 'CREATE TABLE IF NOT EXISTS \"%s\" (key TEXT PRIMARY KEY, value BLOB)' % self.tablename\n            self.conn.execute(MAKE_TABLE)\n            self.conn.commit()\n        if flag == 'w':\n            self.clear()\n\n    def _new_conn(self):\n        return SqliteMultithread(\n            self.filename,\n            autocommit=self.autocommit,\n            journal_mode=self.journal_mode,\n            outer_stack=self._outer_stack,\n        )\n\n    def __enter__(self):\n        if not hasattr(self, 'conn') or self.conn is None:\n            self.conn = self._new_conn()\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n\n    def __str__(self):\n        return \"SqliteDict(%s)\" % (self.filename)\n\n    def __repr__(self):\n        return str(self)  # no need of something complex\n\n    def __len__(self):\n        # `select count (*)` is super slow in sqlite (does a linear scan!!)\n        # As a result, len() is very slow too once the table size grows beyond trivial.\n        # We could keep the total count of rows ourselves, by means of triggers,\n        # but that seems too complicated and would slow down normal operation\n        # (insert/delete etc).\n        GET_LEN = 'SELECT COUNT(*) FROM \"%s\"' % self.tablename\n        rows = self.conn.select_one(GET_LEN)[0]\n        return rows if rows is not None else 0\n\n    def __bool__(self):\n        # No elements is False, otherwise True\n        GET_MAX = 'SELECT MAX(ROWID) FROM \"%s\"' % self.tablename\n        m = self.conn.select_one(GET_MAX)[0]\n        # Explicit better than implicit and bla bla\n        return True if m is not None else False\n\n    def iterkeys(self):\n        GET_KEYS = 'SELECT key FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key in self.conn.select(GET_KEYS):\n            yield self.decode_key(key[0])\n\n    def itervalues(self):\n        GET_VALUES = 'SELECT value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for value in self.conn.select(GET_VALUES):\n            yield self.decode(value[0])\n\n    def iteritems(self):\n        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n\n    def keys(self):\n        return self.iterkeys()\n\n    def values(self):\n        return self.itervalues()\n\n    def items(self):\n        return self.iteritems()\n\n    def __contains__(self, key):\n        HAS_ITEM = 'SELECT 1 FROM \"%s\" WHERE key = ?' % self.tablename\n        return self.conn.select_one(HAS_ITEM, (self.encode_key(key),)) is not None\n\n    def __getitem__(self, key):\n        GET_ITEM = 'SELECT value FROM \"%s\" WHERE key = ?' % self.tablename\n        item = self.conn.select_one(GET_ITEM, (self.encode_key(key),))\n        if item is None:\n            raise KeyError(key)\n        return self.decode(item[0])\n\n    def __setitem__(self, key, value):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        ADD_ITEM = 'REPLACE INTO \"%s\" (key, value) VALUES (?,?)' % self.tablename\n        self.conn.execute(ADD_ITEM, (self.encode_key(key), self.encode(value)))\n        if self.autocommit:\n            self.commit()\n\n    def __delitem__(self, key):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete from read-only SqliteDict')\n\n        if key not in self:\n            raise KeyError(key)\n        DEL_ITEM = 'DELETE FROM \"%s\" WHERE key = ?' % self.tablename\n        self.conn.execute(DEL_ITEM, (self.encode_key(key),))\n        if self.autocommit:\n            self.commit()\n\n    def update(self, items=(), **kwds):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n\n    def __iter__(self):\n        return self.iterkeys()\n\n    def clear(self):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n\n    @staticmethod\n    def get_tablenames(filename):\n        \"\"\"get the names of the tables in an sqlite db as a list\"\"\"\n        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]\n\n###The function: commit###    sync = commit\n\n    def close(self, do_log=True, force=False):\n        if do_log:\n            logger.debug(\"closing %s\" % self)\n        if hasattr(self, 'conn') and self.conn is not None:\n            if self.conn.autocommit and not force:\n                # typically calls to commit are non-blocking when autocommit is\n                # used.  However, we need to block on close() to ensure any\n                # awaiting exceptions are handled and that all data is\n                # persisted to disk before returning.\n                self.conn.commit(blocking=True)\n            self.conn.close(force=force)\n            self.conn = None\n        if self.in_temp:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n    def terminate(self):\n        \"\"\"Delete the underlying database file. Use with care.\"\"\"\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))\n\n    def __del__(self):\n        # like close(), but assume globals are gone by now (do not log!)\n        try:\n            self.close(do_log=False, force=True)\n        except Exception:\n            # prevent error log flood in case of multiple SqliteDicts\n            # closed after connection lost (exceptions are always ignored\n            # in __del__ method.\n            pass\n\n\nclass SqliteMultithread(threading.Thread):\n    \"\"\"\n    Wrap sqlite connection in a way that allows concurrent requests from multiple threads.\n\n    This is done by internally queueing the requests and processing them sequentially\n    in a separate thread (in the same order they arrived).\n\n    \"\"\"\n    def __init__(self, filename, autocommit, journal_mode, outer_stack=True):\n        super(SqliteMultithread, self).__init__()\n        self.filename = filename\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        # use request queue of unlimited size\n        self.reqs = Queue()\n        self.daemon = True\n        self._outer_stack = outer_stack\n        self.log = logging.getLogger('sqlitedict.SqliteMultithread')\n\n        #\n        # Parts of this object's state get accessed from different threads, so\n        # we use synchronization to avoid race conditions.  For example,\n        # .exception gets set inside the new daemon thread that we spawned, but\n        # gets read from the main thread.  This is particularly important\n        # during initialization: the Thread needs some time to actually start\n        # working, and until this happens, any calls to e.g.\n        # check_raise_error() will prematurely return None, meaning all is\n        # well.  If the that connection happens to fail, we'll never know about\n        # it, and instead wait for a result that never arrives (effectively,\n        # deadlocking).  Locking solves this problem by eliminating the race\n        # condition.\n        #\n        self._lock = threading.Lock()\n        self._lock.acquire()\n        self.exception = None\n\n        self.start()\n\n    def _connect(self):\n        \"\"\"Connect to the underlying database.\n\n        Raises an exception on failure.  Returns the connection and cursor on success.\n        \"\"\"\n        try:\n            if self.autocommit:\n                conn = sqlite3.connect(self.filename, isolation_level=None, check_same_thread=False)\n            else:\n                conn = sqlite3.connect(self.filename, check_same_thread=False)\n        except Exception:\n            self.log.exception(\"Failed to initialize connection for filename: %s\" % self.filename)\n            self.exception = sys.exc_info()\n            raise\n\n        try:\n            conn.execute('PRAGMA journal_mode = %s' % self.journal_mode)\n            conn.text_factory = str\n            cursor = conn.cursor()\n            conn.commit()\n            cursor.execute('PRAGMA synchronous=OFF')\n        except Exception:\n            self.log.exception(\"Failed to execute PRAGMA statements.\")\n            self.exception = sys.exc_info()\n            raise\n\n        return conn, cursor\n\n    def run(self):\n        #\n        # Nb. this is what actually runs inside the new daemon thread.\n        # self._lock is locked at this stage - see the initializer function.\n        #\n        try:\n            conn, cursor = self._connect()\n        finally:\n            self._lock.release()\n\n        res_ref = None\n        while True:\n            #\n            # req: an SQL command or one of the --magic-- commands we use internally\n            # arg: arguments for the command\n            # res_ref: a weak reference to the queue into which responses must be placed\n            # outer_stack: the outer stack, for producing more informative traces in case of error\n            #\n            req, arg, res_ref, outer_stack = self.reqs.get()\n\n            if req == _REQUEST_CLOSE:\n                assert res_ref, ('--close-- without return queue', res_ref)\n                break\n            elif req == _REQUEST_COMMIT:\n                conn.commit()\n                _put(res_ref, _RESPONSE_NO_MORE)\n            else:\n                try:\n                    cursor.execute(req, arg)\n                except Exception:\n                    with self._lock:\n                        self.exception = (e_type, e_value, e_tb) = sys.exc_info()\n\n                    inner_stack = traceback.extract_stack()\n\n                    # An exception occurred in our thread, but we may not\n                    # immediately able to throw it in our calling thread, if it has\n                    # no return `res` queue: log as level ERROR both the inner and\n                    # outer exception immediately.\n                    #\n                    # Any iteration of res.get() or any next call will detect the\n                    # inner exception and re-raise it in the calling Thread; though\n                    # it may be confusing to see an exception for an unrelated\n                    # statement, an ERROR log statement from the 'sqlitedict.*'\n                    # namespace contains the original outer stack location.\n                    self.log.error('Inner exception:')\n                    for item in traceback.format_list(inner_stack):\n                        self.log.error(item)\n                    self.log.error('')  # deliniate traceback & exception w/blank line\n                    for item in traceback.format_exception_only(e_type, e_value):\n                        self.log.error(item)\n\n                    self.log.error('')  # exception & outer stack w/blank line\n\n                    if self._outer_stack:\n                        self.log.error('Outer stack:')\n                        for item in traceback.format_list(outer_stack):\n                            self.log.error(item)\n                        self.log.error('Exception will be re-raised at next call.')\n                    else:\n                        self.log.error(\n                            'Unable to show the outer stack. Pass '\n                            'outer_stack=True when initializing the '\n                            'SqliteDict instance to show the outer stack.'\n                        )\n\n                if res_ref:\n                    for rec in cursor:\n                        if _put(res_ref, rec) == _PUT_REFERENT_DESTROYED:\n                            #\n                            # The queue we are sending responses to got garbage\n                            # collected.  Nobody is listening anymore, so we\n                            # stop sending responses.\n                            #\n                            break\n\n                    _put(res_ref, _RESPONSE_NO_MORE)\n\n                if self.autocommit:\n                    conn.commit()\n\n        self.log.debug('received: %s, send: --no more--', req)\n        conn.close()\n\n        _put(res_ref, _RESPONSE_NO_MORE)\n\n    def check_raise_error(self):\n        \"\"\"\n        Check for and raise exception for any previous sqlite query.\n\n        For the `execute*` family of method calls, such calls are non-blocking and any\n        exception raised in the thread cannot be handled by the calling Thread (usually\n        MainThread).  This method is called on `close`, and prior to any subsequent\n        calls to the `execute*` methods to check for and raise an exception in a\n        previous call to the MainThread.\n        \"\"\"\n        with self._lock:\n            if self.exception:\n                e_type, e_value, e_tb = self.exception\n\n                # clear self.exception, if the caller decides to handle such\n                # exception, we should not repeatedly re-raise it.\n                self.exception = None\n\n                self.log.error('An exception occurred from a previous statement, view '\n                               'the logging namespace \"sqlitedict\" for outer stack.')\n\n                # The third argument to raise is the traceback object, and it is\n                # substituted instead of the current location as the place where\n                # the exception occurred, this is so that when using debuggers such\n                # as `pdb', or simply evaluating the naturally raised traceback, we\n                # retain the original (inner) location of where the exception\n                # occurred.\n                reraise(e_type, e_value, e_tb)\n\n    def execute(self, req, arg=None, res=None):\n        \"\"\"\n        `execute` calls are non-blocking: just queue up the request and return immediately.\n\n        :param req: The request (an SQL command)\n        :param arg: Arguments to the SQL command\n        :param res: A queue in which to place responses as they become available\n        \"\"\"\n        self.check_raise_error()\n        stack = None\n\n        if self._outer_stack:\n            # NOTE: This might be a lot of information to pump into an input\n            # queue, affecting performance.  I've also seen earlier versions of\n            # jython take a severe performance impact for throwing exceptions\n            # so often.\n            stack = traceback.extract_stack()[:-1]\n\n        #\n        # We pass a weak reference to the response queue instead of a regular\n        # reference, because we want the queues to be garbage-collected\n        # more aggressively.\n        #\n        res_ref = None\n        if res:\n            res_ref = weakref.ref(res)\n\n        self.reqs.put((req, arg or tuple(), res_ref, stack))\n\n    def executemany(self, req, items):\n        for item in items:\n            self.execute(req, item)\n        self.check_raise_error()\n\n    def select(self, req, arg=None):\n        \"\"\"\n        Unlike sqlite's native select, this select doesn't handle iteration efficiently.\n\n        The result of `select` starts filling up with values as soon as the\n        request is dequeued, and although you can iterate over the result normally\n        (`for res in self.select(): ...`), the entire result will be in memory.\n        \"\"\"\n        res = Queue()  # results of the select will appear as items in this queue\n        self.execute(req, arg, res)\n        while True:\n            rec = res.get()\n            self.check_raise_error()\n            if rec == _RESPONSE_NO_MORE:\n                break\n            yield rec\n\n    def select_one(self, req, arg=None):\n        \"\"\"Return only the first row of the SELECT, or None if there are no matching rows.\"\"\"\n        try:\n            return next(iter(self.select(req, arg)))\n        except StopIteration:\n            return None\n\n    def commit(self, blocking=True):\n        if blocking:\n            # by default, we await completion of commit() unless\n            # blocking=False.  This ensures any available exceptions for any\n            # previous statement are thrown before returning, and that the\n            # data has actually persisted to disk!\n            self.select_one(_REQUEST_COMMIT)\n        else:\n            # otherwise, we fire and forget as usual.\n            self.execute(_REQUEST_COMMIT)\n\n    def close(self, force=False):\n        if force:\n            # If a SqliteDict is being killed or garbage-collected, then select_one()\n            # could hang forever because run() might already have exited and therefore\n            # can't process the request. Instead, push the close command to the requests\n            # queue directly. If run() is still alive, it will exit gracefully. If not,\n            # then there's nothing we can do anyway.\n            self.reqs.put((_REQUEST_CLOSE, None, weakref.ref(Queue()), None))\n        else:\n            # we abuse 'select' to \"iter\" over a \"--close--\" statement so that we\n            # can confirm the completion of close before joining the thread and\n            # returning (by semaphore '--no more--'\n            self.select_one(_REQUEST_CLOSE)\n            self.join()\n\n\n#\n# This is here for .github/workflows/release.yml\n#\nif __name__ == '__main__':\n    print(__version__)\n", "prompt": "Please write a python function called 'commit' base the context. This function is used to persist all data in the SqliteDict instance to disk. It commits the changes made to the database. If `blocking` is set to False, the commit command is queued but the data is not guaranteed to be persisted immediately.:param self: SqliteDict. An instance of the SqliteDict class.\n:param blocking: Bool. Whether to block until the commit is complete. Defaults to True.\n:return: No return values..\n        The context you need to refer to is as follows: #!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# This code is distributed under the terms and conditions\n# from the Apache License, Version 2.0\n#\n# http://opensource.org/licenses/apache2.0.php\n#\n# This code was inspired by:\n#  * http://code.activestate.com/recipes/576638-draft-for-an-sqlite3-based-dbm/\n#  * http://code.activestate.com/recipes/526618/\n\n\"\"\"\nA lightweight wrapper around Python's sqlite3 database, with a dict-like interface\nand multi-thread access support::\n\n>>> mydict = SqliteDict('some.db', autocommit=True) # the mapping will be persisted to file `some.db`\n>>> mydict['some_key'] = any_picklable_object\n>>> print mydict['some_key']\n>>> print len(mydict) # etc... all dict functions work\n\nPickle is used internally to serialize the values. Keys are strings.\n\nIf you don't use autocommit (default is no autocommit for performance), then\ndon't forget to call `mydict.commit()` when done with a transaction.\n\n\"\"\"\n\nimport sqlite3\nimport os\nimport sys\nimport tempfile\nimport threading\nimport logging\nimport traceback\nfrom base64 import b64decode, b64encode\nimport weakref\n\n__version__ = '2.1.0'\n\n\ndef reraise(tp, value, tb=None):\n    if value is None:\n        value = tp()\n    if value.__traceback__ is not tb:\n        raise value.with_traceback(tb)\n    raise value\n\n\ntry:\n    from cPickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\nexcept ImportError:\n    from pickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\n\n# some Python 3 vs 2 imports\ntry:\n    from collections import UserDict as DictClass\nexcept ImportError:\n    from UserDict import DictMixin as DictClass\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\n\nlogger = logging.getLogger(__name__)\n\n#\n# There's a thread that holds the actual SQL connection (SqliteMultithread).\n# We communicate with this thread via queues (request and responses).\n# The requests can either be SQL commands or one of the \"special\" commands\n# below:\n#\n# _REQUEST_CLOSE: request that the SQL connection be closed\n# _REQUEST_COMMIT: request that any changes be committed to the DB\n#\n# Responses are either SQL records (e.g. results of a SELECT) or the magic\n# _RESPONSE_NO_MORE command, which indicates nothing else will ever be written\n# to the response queue.\n#\n_REQUEST_CLOSE = '--close--'\n_REQUEST_COMMIT = '--commit--'\n_RESPONSE_NO_MORE = '--no more--'\n\n#\n# We work with weak references for better memory efficiency.\n# Dereferencing, checking the referent queue still exists, and putting to it\n# is boring and repetitive, so we have a _put function to handle it for us.\n#\n_PUT_OK, _PUT_REFERENT_DESTROYED, _PUT_NOOP = 0, 1, 2\n\n\ndef _put(queue_reference, item):\n    if queue_reference is not None:\n        queue = queue_reference()\n        if queue is None:\n            #\n            # We got a reference to a queue, but that queue no longer exists\n            #\n            retval = _PUT_REFERENT_DESTROYED\n        else:\n            queue.put(item)\n            retval = _PUT_OK\n\n        del queue\n        return retval\n\n    #\n    # We didn't get a reference to a queue, so do nothing (no-op).\n    #\n    return _PUT_NOOP\n\n\ndef open(*args, **kwargs):\n    \"\"\"See documentation of the SqliteDict class.\"\"\"\n    return SqliteDict(*args, **kwargs)\n\n\ndef encode(obj):\n    \"\"\"Serialize an object using pickle to a binary format accepted by SQLite.\"\"\"\n    return sqlite3.Binary(dumps(obj, protocol=PICKLE_PROTOCOL))\n\n\ndef decode(obj):\n    \"\"\"Deserialize objects retrieved from SQLite.\"\"\"\n    return loads(bytes(obj))\n\n\ndef encode_key(key):\n    \"\"\"Serialize a key using pickle + base64 encoding to text accepted by SQLite.\"\"\"\n    return b64encode(dumps(key, protocol=PICKLE_PROTOCOL)).decode(\"ascii\")\n\n\ndef decode_key(key):\n    \"\"\"Deserialize a key retrieved from SQLite.\"\"\"\n    return loads(b64decode(key.encode(\"ascii\")))\n\n\ndef identity(obj):\n    \"\"\"Identity f(x) = x function for encoding/decoding.\"\"\"\n    return obj\n\n\nclass SqliteDict(DictClass):\n    VALID_FLAGS = ['c', 'r', 'w', 'n']\n\n    def __init__(self, filename=None, tablename='unnamed', flag='c',\n                 autocommit=False, journal_mode=\"DELETE\", encode=encode,\n                 decode=decode, encode_key=identity, decode_key=identity,\n                 timeout=5, outer_stack=True):\n        \"\"\"\n        Initialize a thread-safe sqlite-backed dictionary. The dictionary will\n        be a table `tablename` in database file `filename`. A single file (=database)\n        may contain multiple tables.\n\n        If no `filename` is given, a random file in temp will be used (and deleted\n        from temp once the dict is closed/deleted).\n\n        If you enable `autocommit`, changes will be committed after each operation\n        (more inefficient but safer). Otherwise, changes are committed on `self.commit()`,\n        `self.clear()` and `self.close()`.\n\n        Set `journal_mode` to 'OFF' if you're experiencing sqlite I/O problems\n        or if you need performance and don't care about crash-consistency.\n\n        Set `outer_stack` to False to disable the output of the outer exception\n        to the error logs.  This may improve the efficiency of sqlitedict\n        operation at the expense of a detailed exception trace.\n\n        The `flag` parameter. Exactly one of:\n          'c': default mode, open for read/write, creating the db/table if necessary.\n          'w': open for r/w, but drop `tablename` contents first (start with empty table)\n          'r': open as read-only\n          'n': create a new database (erasing any existing tables, not just `tablename`!).\n\n        The `encode` and `decode` parameters are used to customize how the values\n        are serialized and deserialized.\n        The `encode` parameter must be a function that takes a single Python\n        object and returns a serialized representation.\n        The `decode` function must be a function that takes the serialized\n        representation produced by `encode` and returns a deserialized Python\n        object.\n        The default is to use pickle.\n\n        The `timeout` defines the maximum time (in seconds) to wait for initial Thread startup.\n\n        \"\"\"\n        self.in_temp = filename is None\n        if self.in_temp:\n            fd, filename = tempfile.mkstemp(prefix='sqldict')\n            os.close(fd)\n\n        if flag not in SqliteDict.VALID_FLAGS:\n            raise RuntimeError(\"Unrecognized flag: %s\" % flag)\n        self.flag = flag\n\n        if flag == 'n':\n            if os.path.exists(filename):\n                os.remove(filename)\n\n        dirname = os.path.dirname(filename)\n        if dirname:\n            if not os.path.exists(dirname):\n                raise RuntimeError('Error! The directory does not exist, %s' % dirname)\n\n        self.filename = filename\n\n        # Use standard SQL escaping of double quote characters in identifiers, by doubling them.\n        # See https://github.com/RaRe-Technologies/sqlitedict/pull/113\n        self.tablename = tablename.replace('\"', '\"\"')\n\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        self.encode = encode\n        self.decode = decode\n        self.encode_key = encode_key\n        self.decode_key = decode_key\n        self._outer_stack = outer_stack\n\n        logger.debug(\"opening Sqlite table %r in %r\" % (tablename, filename))\n        self.conn = self._new_conn()\n        if self.flag == 'r':\n            if self.tablename not in SqliteDict.get_tablenames(self.filename):\n                msg = 'Refusing to create a new table \"%s\" in read-only DB mode' % tablename\n                raise RuntimeError(msg)\n        else:\n            MAKE_TABLE = 'CREATE TABLE IF NOT EXISTS \"%s\" (key TEXT PRIMARY KEY, value BLOB)' % self.tablename\n            self.conn.execute(MAKE_TABLE)\n            self.conn.commit()\n        if flag == 'w':\n            self.clear()\n\n    def _new_conn(self):\n        return SqliteMultithread(\n            self.filename,\n            autocommit=self.autocommit,\n            journal_mode=self.journal_mode,\n            outer_stack=self._outer_stack,\n        )\n\n    def __enter__(self):\n        if not hasattr(self, 'conn') or self.conn is None:\n            self.conn = self._new_conn()\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n\n    def __str__(self):\n        return \"SqliteDict(%s)\" % (self.filename)\n\n    def __repr__(self):\n        return str(self)  # no need of something complex\n\n    def __len__(self):\n        # `select count (*)` is super slow in sqlite (does a linear scan!!)\n        # As a result, len() is very slow too once the table size grows beyond trivial.\n        # We could keep the total count of rows ourselves, by means of triggers,\n        # but that seems too complicated and would slow down normal operation\n        # (insert/delete etc).\n        GET_LEN = 'SELECT COUNT(*) FROM \"%s\"' % self.tablename\n        rows = self.conn.select_one(GET_LEN)[0]\n        return rows if rows is not None else 0\n\n    def __bool__(self):\n        # No elements is False, otherwise True\n        GET_MAX = 'SELECT MAX(ROWID) FROM \"%s\"' % self.tablename\n        m = self.conn.select_one(GET_MAX)[0]\n        # Explicit better than implicit and bla bla\n        return True if m is not None else False\n\n    def iterkeys(self):\n        GET_KEYS = 'SELECT key FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key in self.conn.select(GET_KEYS):\n            yield self.decode_key(key[0])\n\n    def itervalues(self):\n        GET_VALUES = 'SELECT value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for value in self.conn.select(GET_VALUES):\n            yield self.decode(value[0])\n\n    def iteritems(self):\n        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n\n    def keys(self):\n        return self.iterkeys()\n\n    def values(self):\n        return self.itervalues()\n\n    def items(self):\n        return self.iteritems()\n\n    def __contains__(self, key):\n        HAS_ITEM = 'SELECT 1 FROM \"%s\" WHERE key = ?' % self.tablename\n        return self.conn.select_one(HAS_ITEM, (self.encode_key(key),)) is not None\n\n    def __getitem__(self, key):\n        GET_ITEM = 'SELECT value FROM \"%s\" WHERE key = ?' % self.tablename\n        item = self.conn.select_one(GET_ITEM, (self.encode_key(key),))\n        if item is None:\n            raise KeyError(key)\n        return self.decode(item[0])\n\n    def __setitem__(self, key, value):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        ADD_ITEM = 'REPLACE INTO \"%s\" (key, value) VALUES (?,?)' % self.tablename\n        self.conn.execute(ADD_ITEM, (self.encode_key(key), self.encode(value)))\n        if self.autocommit:\n            self.commit()\n\n    def __delitem__(self, key):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete from read-only SqliteDict')\n\n        if key not in self:\n            raise KeyError(key)\n        DEL_ITEM = 'DELETE FROM \"%s\" WHERE key = ?' % self.tablename\n        self.conn.execute(DEL_ITEM, (self.encode_key(key),))\n        if self.autocommit:\n            self.commit()\n\n    def update(self, items=(), **kwds):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n\n    def __iter__(self):\n        return self.iterkeys()\n\n    def clear(self):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n\n    @staticmethod\n    def get_tablenames(filename):\n        \"\"\"get the names of the tables in an sqlite db as a list\"\"\"\n        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]\n\n###The function: commit###    sync = commit\n\n    def close(self, do_log=True, force=False):\n        if do_log:\n            logger.debug(\"closing %s\" % self)\n        if hasattr(self, 'conn') and self.conn is not None:\n            if self.conn.autocommit and not force:\n                # typically calls to commit are non-blocking when autocommit is\n                # used.  However, we need to block on close() to ensure any\n                # awaiting exceptions are handled and that all data is\n                # persisted to disk before returning.\n                self.conn.commit(blocking=True)\n            self.conn.close(force=force)\n            self.conn = None\n        if self.in_temp:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n    def terminate(self):\n        \"\"\"Delete the underlying database file. Use with care.\"\"\"\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))\n\n    def __del__(self):\n        # like close(), but assume globals are gone by now (do not log!)\n        try:\n            self.close(do_log=False, force=True)\n        except Exception:\n            # prevent error log flood in case of multiple SqliteDicts\n            # closed after connection lost (exceptions are always ignored\n            # in __del__ method.\n            pass\n\n\nclass SqliteMultithread(threading.Thread):\n    \"\"\"\n    Wrap sqlite connection in a way that allows concurrent requests from multiple threads.\n\n    This is done by internally queueing the requests and processing them sequentially\n    in a separate thread (in the same order they arrived).\n\n    \"\"\"\n    def __init__(self, filename, autocommit, journal_mode, outer_stack=True):\n        super(SqliteMultithread, self).__init__()\n        self.filename = filename\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        # use request queue of unlimited size\n        self.reqs = Queue()\n        self.daemon = True\n        self._outer_stack = outer_stack\n        self.log = logging.getLogger('sqlitedict.SqliteMultithread')\n\n        #\n        # Parts of this object's state get accessed from different threads, so\n        # we use synchronization to avoid race conditions.  For example,\n        # .exception gets set inside the new daemon thread that we spawned, but\n        # gets read from the main thread.  This is particularly important\n        # during initialization: the Thread needs some time to actually start\n        # working, and until this happens, any calls to e.g.\n        # check_raise_error() will prematurely return None, meaning all is\n        # well.  If the that connection happens to fail, we'll never know about\n        # it, and instead wait for a result that never arrives (effectively,\n        # deadlocking).  Locking solves this problem by eliminating the race\n        # condition.\n        #\n        self._lock = threading.Lock()\n        self._lock.acquire()\n        self.exception = None\n\n        self.start()\n\n    def _connect(self):\n        \"\"\"Connect to the underlying database.\n\n        Raises an exception on failure.  Returns the connection and cursor on success.\n        \"\"\"\n        try:\n            if self.autocommit:\n                conn = sqlite3.connect(self.filename, isolation_level=None, check_same_thread=False)\n            else:\n                conn = sqlite3.connect(self.filename, check_same_thread=False)\n        except Exception:\n            self.log.exception(\"Failed to initialize connection for filename: %s\" % self.filename)\n            self.exception = sys.exc_info()\n            raise\n\n        try:\n            conn.execute('PRAGMA journal_mode = %s' % self.journal_mode)\n            conn.text_factory = str\n            cursor = conn.cursor()\n            conn.commit()\n            cursor.execute('PRAGMA synchronous=OFF')\n        except Exception:\n            self.log.exception(\"Failed to execute PRAGMA statements.\")\n            self.exception = sys.exc_info()\n            raise\n\n        return conn, cursor\n\n    def run(self):\n        #\n        # Nb. this is what actually runs inside the new daemon thread.\n        # self._lock is locked at this stage - see the initializer function.\n        #\n        try:\n            conn, cursor = self._connect()\n        finally:\n            self._lock.release()\n\n        res_ref = None\n        while True:\n            #\n            # req: an SQL command or one of the --magic-- commands we use internally\n            # arg: arguments for the command\n            # res_ref: a weak reference to the queue into which responses must be placed\n            # outer_stack: the outer stack, for producing more informative traces in case of error\n            #\n            req, arg, res_ref, outer_stack = self.reqs.get()\n\n            if req == _REQUEST_CLOSE:\n                assert res_ref, ('--close-- without return queue', res_ref)\n                break\n            elif req == _REQUEST_COMMIT:\n                conn.commit()\n                _put(res_ref, _RESPONSE_NO_MORE)\n            else:\n                try:\n                    cursor.execute(req, arg)\n                except Exception:\n                    with self._lock:\n                        self.exception = (e_type, e_value, e_tb) = sys.exc_info()\n\n                    inner_stack = traceback.extract_stack()\n\n                    # An exception occurred in our thread, but we may not\n                    # immediately able to throw it in our calling thread, if it has\n                    # no return `res` queue: log as level ERROR both the inner and\n                    # outer exception immediately.\n                    #\n                    # Any iteration of res.get() or any next call will detect the\n                    # inner exception and re-raise it in the calling Thread; though\n                    # it may be confusing to see an exception for an unrelated\n                    # statement, an ERROR log statement from the 'sqlitedict.*'\n                    # namespace contains the original outer stack location.\n                    self.log.error('Inner exception:')\n                    for item in traceback.format_list(inner_stack):\n                        self.log.error(item)\n                    self.log.error('')  # deliniate traceback & exception w/blank line\n                    for item in traceback.format_exception_only(e_type, e_value):\n                        self.log.error(item)\n\n                    self.log.error('')  # exception & outer stack w/blank line\n\n                    if self._outer_stack:\n                        self.log.error('Outer stack:')\n                        for item in traceback.format_list(outer_stack):\n                            self.log.error(item)\n                        self.log.error('Exception will be re-raised at next call.')\n                    else:\n                        self.log.error(\n                            'Unable to show the outer stack. Pass '\n                            'outer_stack=True when initializing the '\n                            'SqliteDict instance to show the outer stack.'\n                        )\n\n                if res_ref:\n                    for rec in cursor:\n                        if _put(res_ref, rec) == _PUT_REFERENT_DESTROYED:\n                            #\n                            # The queue we are sending responses to got garbage\n                            # collected.  Nobody is listening anymore, so we\n                            # stop sending responses.\n                            #\n                            break\n\n                    _put(res_ref, _RESPONSE_NO_MORE)\n\n                if self.autocommit:\n                    conn.commit()\n\n        self.log.debug('received: %s, send: --no more--', req)\n        conn.close()\n\n        _put(res_ref, _RESPONSE_NO_MORE)\n\n    def check_raise_error(self):\n        \"\"\"\n        Check for and raise exception for any previous sqlite query.\n\n        For the `execute*` family of method calls, such calls are non-blocking and any\n        exception raised in the thread cannot be handled by the calling Thread (usually\n        MainThread).  This method is called on `close`, and prior to any subsequent\n        calls to the `execute*` methods to check for and raise an exception in a\n        previous call to the MainThread.\n        \"\"\"\n        with self._lock:\n            if self.exception:\n                e_type, e_value, e_tb = self.exception\n\n                # clear self.exception, if the caller decides to handle such\n                # exception, we should not repeatedly re-raise it.\n                self.exception = None\n\n                self.log.error('An exception occurred from a previous statement, view '\n                               'the logging namespace \"sqlitedict\" for outer stack.')\n\n                # The third argument to raise is the traceback object, and it is\n                # substituted instead of the current location as the place where\n                # the exception occurred, this is so that when using debuggers such\n                # as `pdb', or simply evaluating the naturally raised traceback, we\n                # retain the original (inner) location of where the exception\n                # occurred.\n                reraise(e_type, e_value, e_tb)\n\n    def execute(self, req, arg=None, res=None):\n        \"\"\"\n        `execute` calls are non-blocking: just queue up the request and return immediately.\n\n        :param req: The request (an SQL command)\n        :param arg: Arguments to the SQL command\n        :param res: A queue in which to place responses as they become available\n        \"\"\"\n        self.check_raise_error()\n        stack = None\n\n        if self._outer_stack:\n            # NOTE: This might be a lot of information to pump into an input\n            # queue, affecting performance.  I've also seen earlier versions of\n            # jython take a severe performance impact for throwing exceptions\n            # so often.\n            stack = traceback.extract_stack()[:-1]\n\n        #\n        # We pass a weak reference to the response queue instead of a regular\n        # reference, because we want the queues to be garbage-collected\n        # more aggressively.\n        #\n        res_ref = None\n        if res:\n            res_ref = weakref.ref(res)\n\n        self.reqs.put((req, arg or tuple(), res_ref, stack))\n\n    def executemany(self, req, items):\n        for item in items:\n            self.execute(req, item)\n        self.check_raise_error()\n\n    def select(self, req, arg=None):\n        \"\"\"\n        Unlike sqlite's native select, this select doesn't handle iteration efficiently.\n\n        The result of `select` starts filling up with values as soon as the\n        request is dequeued, and although you can iterate over the result normally\n        (`for res in self.select(): ...`), the entire result will be in memory.\n        \"\"\"\n        res = Queue()  # results of the select will appear as items in this queue\n        self.execute(req, arg, res)\n        while True:\n            rec = res.get()\n            self.check_raise_error()\n            if rec == _RESPONSE_NO_MORE:\n                break\n            yield rec\n\n    def select_one(self, req, arg=None):\n        \"\"\"Return only the first row of the SELECT, or None if there are no matching rows.\"\"\"\n        try:\n            return next(iter(self.select(req, arg)))\n        except StopIteration:\n            return None\n\n    def commit(self, blocking=True):\n        if blocking:\n            # by default, we await completion of commit() unless\n            # blocking=False.  This ensures any available exceptions for any\n            # previous statement are thrown before returning, and that the\n            # data has actually persisted to disk!\n            self.select_one(_REQUEST_COMMIT)\n        else:\n            # otherwise, we fire and forget as usual.\n            self.execute(_REQUEST_COMMIT)\n\n    def close(self, force=False):\n        if force:\n            # If a SqliteDict is being killed or garbage-collected, then select_one()\n            # could hang forever because run() might already have exited and therefore\n            # can't process the request. Instead, push the close command to the requests\n            # queue directly. If run() is still alive, it will exit gracefully. If not,\n            # then there's nothing we can do anyway.\n            self.reqs.put((_REQUEST_CLOSE, None, weakref.ref(Queue()), None))\n        else:\n            # we abuse 'select' to \"iter\" over a \"--close--\" statement so that we\n            # can confirm the completion of close before joining the thread and\n            # returning (by semaphore '--no more--'\n            self.select_one(_REQUEST_CLOSE)\n            self.join()\n\n\n#\n# This is here for .github/workflows/release.yml\n#\nif __name__ == '__main__':\n    print(__version__)\n", "test_list": ["def test_overwrite_using_flag_w(self):\n    \"\"\"Re-opening of a database with flag='w' destroys only the target table.\"\"\"\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db_1 = SqliteDict(filename=fname, tablename='one')\n    orig_db_1['key'] = 'value'\n    orig_db_1.commit()\n    orig_db_1.close()\n    orig_db_2 = SqliteDict(filename=fname, tablename='two')\n    orig_db_2['key'] = 'value'\n    orig_db_2.commit()\n    orig_db_2.close()\n    next_db_1 = SqliteDict(filename=fname, tablename='one', flag='w')\n    self.assertNotIn('key', next_db_1.keys())\n    next_db_2 = SqliteDict(filename=fname, tablename='two')\n    self.assertIn('key', next_db_2.keys())", "def test_readonly(self):\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db = SqliteDict(filename=fname)\n    orig_db['key'] = 'value'\n    orig_db['key_two'] = 2\n    orig_db.commit()\n    orig_db.close()\n    readonly_db = SqliteDict(filename=fname, flag='r')\n    self.assertTrue(readonly_db['key'] == 'value')\n    self.assertTrue(readonly_db['key_two'] == 2)\n\n    def attempt_write():\n        readonly_db['key'] = ['new_value']\n\n    def attempt_update():\n        readonly_db.update(key='value2', key_two=2.1)\n\n    def attempt_delete():\n        del readonly_db['key']\n\n    def attempt_clear():\n        readonly_db.clear()\n\n    def attempt_terminate():\n        readonly_db.terminate()\n    attempt_funcs = [attempt_write, attempt_update, attempt_delete, attempt_clear, attempt_terminate]\n    for func in attempt_funcs:\n        with self.assertRaises(RuntimeError):\n            func()", "def test_default_reuse_existing_flag_c(self):\n    \"\"\"Re-opening of a database does not destroy it.\"\"\"\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db = SqliteDict(filename=fname)\n    orig_db['key'] = 'value'\n    orig_db.commit()\n    orig_db.close()\n    next_db = SqliteDict(filename=fname)\n    self.assertIn('key', next_db.keys())\n    self.assertEqual(next_db['key'], 'value')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'commit' function should ensure that the 'blocking' parameter is a boolean and defaults to True if not provided.", "unit_test": "def test_commit_input_output_conditions():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit(blocking='not_a_boolean')\n    # Expecting a TypeError because 'blocking' should be a boolean\n    with self.assertRaises(TypeError):\n        db.commit(blocking='not_a_boolean')\n    db.close()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_input_output_conditions"}, "Exception Handling": {"requirement": "The 'commit' function should raise RuntimeError if the database connection is closed before committing.", "unit_test": "def test_commit_exception_handling():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.close()\n    with self.assertRaises(RuntimeError):\n        db.commit()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_exception_handling"}, "Edge Case Handling": {"requirement": "The 'commit' function should handle the case where no changes have been made since the last commit gracefully without errors.", "unit_test": "def test_commit_edge_case_handling():\n    db = SqliteDict(filename='test.db')\n    db.commit()  # No changes made\n    # Expecting no exception to be raised\n    try:\n        db.commit()\n    except Exception as e:\n        self.fail(f'Commit raised an exception unexpectedly: {e}')\n    db.close()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'commit' function to return a boolean indicating whether the commit was successful.", "unit_test": "def test_commit_functionality_extension():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    success = db.commit()\n    self.assertTrue(success)\n    db.close()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'commit' function has complete type annotations for all parameters and return types.", "unit_test": "def test_commit_annotation_coverage():\n    import inspect\n    signature = inspect.signature(SqliteDict.commit)\n    self.assertEqual(signature.parameters['self'].annotation, SqliteDict)\n    self.assertEqual(signature.parameters['blocking'].annotation, bool)\n    self.assertEqual(signature.return_annotation, None)", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_annotation_coverage"}, "Code Complexity": {"requirement": "The 'commit' function should maintain a cyclomatic complexity of 2 to ensure simplicity.", "unit_test": "def test_commit_code_complexity():\n    from radon.complexity import cc_visit\n    code = inspect.getsource(SqliteDict.commit)\n    complexity = cc_visit(code)\n    self.assertEqual(complexity[0].complexity, 2)", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'commit' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_commit_code_standard():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_sqlitedict.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'commit' function should utilize the 'conn' attribute of the SqliteDict class to perform the commit operation.", "unit_test": "def test_commit_context_usage_verification():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit()\n    # Verify that the commit operation uses the 'conn' attribute\n    self.assertIsNotNone(db.conn)\n    db.close()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'commit' function should correctly use the 'conn' attribute to ensure data is persisted to disk.", "unit_test": "def test_commit_context_usage_correctness_verification():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit()\n    db.close()\n    db_reopen = SqliteDict(filename='test.db')\n    self.assertEqual(db_reopen['key'], 'value')\n    db_reopen.close()", "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_context_usage_correctness_verification"}}}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "type": "method", "project_path": "Database/alembic", "completion_path": "Database/alembic/alembic/operations/ops.py", "signature_position": [2202, 2207], "body_position": [2208, 2213], "dependency": {"intra_class": ["alembic.operations.ops.DropColumnOp.__init__"], "intra_file": ["alembic.operations.ops.AddColumnOp", "alembic.operations.ops.AddColumnOp.from_column_and_tablename"], "cross_file": []}, "requirement": {"Functionality": "This function creates an instance of the class based on the given parameters.", "Arguments": ":param cls: A class.\n:param schema: Optional string. The schema of the table.\n:param tname: String. The name of the table.\n:param col: Column. The column to be dropped.\n:return: The created instance."}, "tests": ["tests/test_autogen_render.py::AutogenRenderTest::test_render_drop_column_w_schema", "tests/test_autogen_render.py::AutogenRenderTest::test_render_drop_column", "tests/test_autogen_diffs.py::OrigObjectTest::test_drop_column"], "indent": 4, "domain": "Database", "code": "    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:\n        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )\n", "context": "from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n    def to_constraint(self) -> Constraint:\n        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n###The function: from_column_and_tablename###\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n", "prompt": "Please write a python function called 'from_column_and_tablename' base the context. This function creates an instance of the class based on the given parameters.:param cls: A class.\n:param schema: Optional string. The schema of the table.\n:param tname: String. The name of the table.\n:param col: Column. The column to be dropped.\n:return: The created instance..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n    def to_constraint(self) -> Constraint:\n        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n###The function: from_column_and_tablename###\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n", "test_list": ["def test_render_drop_column_w_schema(self):\n    op_obj = ops.DropColumnOp.from_column_and_tablename('foo', 'bar', Column('x', Integer, server_default='5'))\n    eq_ignore_whitespace(autogenerate.render_op_text(self.autogen_context, op_obj), \"op.drop_column('bar', 'x', schema='foo')\")", "def test_render_drop_column(self):\n    op_obj = ops.DropColumnOp.from_column_and_tablename(None, 'foo', Column('x', Integer, server_default='5'))\n    eq_ignore_whitespace(autogenerate.render_op_text(self.autogen_context, op_obj), \"op.drop_column('foo', 'x')\")", "def test_drop_column(self):\n    t = self.table\n    op = ops.DropColumnOp.from_column_and_tablename(None, 't', t.c.x)\n    is_(op.to_column(), t.c.x)\n    is_(op.reverse().to_column(), t.c.x)\n    is_not_(None, op.to_column().table)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'from_column_and_tablename' should correctly instantiate a 'DropColumnOp' object with the given schema, table name, and column, ensuring that the column is of type 'Column'.", "unit_test": "def test_from_column_and_tablename_input_output():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert op_obj.schema == 'foo'\n    assert op_obj.table_name == 'bar'\n    assert isinstance(op_obj.to_column(), Column)", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_input_output"}, "Exception Handling": {"requirement": "The function 'from_column_and_tablename' should raise a TypeError if the 'col' parameter is not an instance of 'Column'.", "unit_test": "def test_from_column_and_tablename_type_error():\n    try:\n        DropColumnOp.from_column_and_tablename('foo', 'bar', 'not_a_column')\n    except TypeError as e:\n        assert str(e) == 'The col parameter must be an instance of Column.'", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_type_error"}, "Edge Case Handling": {"requirement": "The function 'from_column_and_tablename' should handle the case where the 'schema' parameter is None, defaulting to a schema-less operation.", "unit_test": "def test_from_column_and_tablename_no_schema():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename(None, 'bar', column)\n    assert op_obj.schema is None\n    assert op_obj.table_name == 'bar'", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_no_schema"}, "Functionality Extension": {"requirement": "Extend the function 'from_column_and_tablename' to accept an optional 'if_exists' parameter, which defaults to False, to conditionally drop the column only if it exists.", "unit_test": "def test_from_column_and_tablename_if_exists():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column, if_exists=True)\n    assert op_obj.kw.get('if_exists') is True", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_if_exists"}, "Annotation Coverage": {"requirement": "Ensure that the function 'from_column_and_tablename' includes type annotations for all parameters and the return type.", "unit_test": "def test_from_column_and_tablename_annotations():\n    from inspect import signature\n    sig = signature(DropColumnOp.from_column_and_tablename)\n    assert sig.parameters['schema'].annotation == Optional[str]\n    assert sig.parameters['tname'].annotation == str\n    assert sig.parameters['col'].annotation == Column\n    assert sig.return_annotation == DropColumnOp", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_annotations"}, "Code Complexity": {"requirement": "The function 'from_column_and_tablename' should maintain a cyclomatic complexity of 1, ensuring straightforward logic without branching.", "unit_test": "def test_from_column_and_tablename_complexity():\n    import radon.complexity as rc\n    code = '''\n    def from_column_and_tablename(cls, schema: Optional[str], tname: str, col: Column) -> DropColumnOp:\n        return cls(tname, col, schema=schema)\n    '''\n    complexity = rc.cc_visit(code)\n    assert complexity[0].complexity == 1", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_code_complexity"}, "Code Standard": {"requirement": "Ensure that the function 'from_column_and_tablename' adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_from_column_and_tablename_pep8():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_function.py'])\n    assert result.total_errors == 0", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the function 'from_column_and_tablename' uses the 'DropColumnOp' class from the 'alembic.operations.ops' module.", "unit_test": "def test_from_column_and_tablename_context_usage():\n    from alembic.operations.ops import DropColumnOp\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert isinstance(op_obj, DropColumnOp)", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the function 'from_column_and_tablename' correctly initializes a 'DropColumnOp' object using the '__init__' method of the 'DropColumnOp' class.", "unit_test": "def test_from_column_and_tablename_context_correctness():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert op_obj.schema == 'foo'\n    assert op_obj.table_name == 'bar'\n    assert op_obj.column_name == 'x'", "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_context_correctness"}}}
{"namespace": "pyramid.i18n.Translations.add", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/i18n.py", "signature_position": [277, 277], "body_position": [293, 307], "dependency": {"intra_class": ["pyramid.i18n.Translations.DEFAULT_DOMAIN", "pyramid.i18n.Translations._domains", "pyramid.i18n.Translations.domain", "pyramid.i18n.Translations.merge", "pyramid.i18n.Translations.plural"], "intra_file": ["pyramid.i18n.DEFAULT_PLURAL"], "cross_file": []}, "requirement": {"Functionality": "This function adds the given translations to the catalog. If the domain of the translations is different from the current catalog, they are added as a separate catalog. It also provides the option to merge translations for message domains that have already been added.", "Arguments": ":param self: Translations. An instance of the Translations class.\n:param translations: Translations. The Translations instance with the messages to add.\n:param merge: Bool. Whether translations for message domains that have already been added should be merged with the existing translations. Defaults to True.\n:return: Translations. The Translations instance (self) so that merge calls can be easily chained."}, "tests": ["tests/test_i18n.py::TestTranslations::test_add_default_domain_replaces_plural_first_time", "tests/test_i18n.py::TestTranslations::test_add_different_domain_merge_true_notexisting", "tests/test_i18n.py::TestTranslations::test_add_same_domain_merge_true", "tests/test_i18n.py::TestTranslations::test_add_different_domain_merge_true_existing"], "indent": 4, "domain": "Internet", "code": "    def add(self, translations, merge=True):\n        \"\"\"Add the given translations to the catalog.\n\n        If the domain of the translations is different than that of the\n        current catalog, they are added as a catalog that is only accessible\n        by the various ``d*gettext`` functions.\n\n        :param translations: the `Translations` instance with the messages to\n                             add\n        :param merge: whether translations for message domains that have\n                      already been added should be merged with the existing\n                      translations\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n        if domain == self.DEFAULT_DOMAIN and self.plural is DEFAULT_PLURAL:\n            self.plural = translations.plural\n\n        if merge and domain == self.domain:\n            return self.merge(translations)\n\n        existing = self._domains.get(domain)\n        if merge and existing is not None:\n            existing.merge(translations)\n        else:\n            translations.add_fallback(self)\n            self._domains[domain] = translations\n\n        return self\n", "context": "import gettext\nimport os\nfrom translationstring import Pluralizer, Translator\nfrom translationstring import TranslationString  # API\nfrom translationstring import TranslationStringFactory  # API\n\nfrom pyramid.decorator import reify\nfrom pyramid.interfaces import ILocaleNegotiator\nfrom pyramid.threadlocal import get_current_registry\n\nTranslationString = TranslationString  # PyFlakes\nTranslationStringFactory = TranslationStringFactory  # PyFlakes\n\nDEFAULT_PLURAL = lambda n: int(n != 1)\n\n\nclass Localizer:\n    \"\"\"\n    An object providing translation and pluralizations related to\n    the current request's locale name.  A\n    :class:`pyramid.i18n.Localizer` object is created using the\n    :func:`pyramid.i18n.get_localizer` function.\n    \"\"\"\n\n    def __init__(self, locale_name, translations):\n        self.locale_name = locale_name\n        self.translations = translations\n        self.pluralizer = None\n        self.translator = None\n\n    def translate(self, tstring, domain=None, mapping=None):\n        \"\"\"\n        Translate a :term:`translation string` to the current language\n        and interpolate any *replacement markers* in the result.  The\n        ``translate`` method accepts three arguments: ``tstring``\n        (required), ``domain`` (optional) and ``mapping`` (optional).\n        When called, it will translate the ``tstring`` translation\n        string using the current locale.  If the current locale could not be\n        determined, the result of interpolation of the default value is\n        returned.  The optional ``domain`` argument can be used to specify\n        or override the domain of the ``tstring`` (useful when ``tstring``\n        is a normal string rather than a translation string).  The optional\n        ``mapping`` argument can specify or override the ``tstring``\n        interpolation mapping, useful when the ``tstring`` argument is\n        a simple string instead of a translation string.\n\n        Example::\n\n           from pyramid.i18n import TranslationString\n           ts = TranslationString('Add ${item}', domain='mypackage',\n                                  mapping={'item':'Item'})\n           translated = localizer.translate(ts)\n\n        Example::\n\n           translated = localizer.translate('Add ${item}', domain='mypackage',\n                                            mapping={'item':'Item'})\n\n        \"\"\"\n        if self.translator is None:\n            self.translator = Translator(self.translations)\n        return self.translator(tstring, domain=domain, mapping=mapping)\n\n    def pluralize(self, singular, plural, n, domain=None, mapping=None):\n        \"\"\"\n        Return a string translation by using two\n        :term:`message identifier` objects as a singular/plural pair\n        and an ``n`` value representing the number that appears in the\n        message using gettext plural forms support.  The ``singular``\n        and ``plural`` objects should be strings. There is no\n        reason to use translation string objects as arguments as all\n        metadata is ignored.\n\n        ``n`` represents the number of elements. ``domain`` is the\n        translation domain to use to do the pluralization, and ``mapping``\n        is the interpolation mapping that should be used on the result. If\n        the ``domain`` is not supplied, a default domain is used (usually\n        ``messages``).\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('Add ${num} item',\n                                            'Add ${num} items',\n                                            num,\n                                            mapping={'num':num})\n\n        If using the gettext plural support, which is required for\n        languages that have pluralisation rules other than n != 1, the\n        ``singular`` argument must be the message_id defined in the\n        translation file. The plural argument is not used in this case.\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('item_plural',\n                                            '',\n                                            num,\n                                            mapping={'num':num})\n\n\n        \"\"\"\n        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )\n\n\ndef default_locale_negotiator(request):\n    \"\"\"The default :term:`locale negotiator`.  Returns a locale name\n    or ``None``.\n\n    - First, the negotiator looks for the ``_LOCALE_`` attribute of\n      the request object (possibly set by a view or a listener for an\n      :term:`event`). If the attribute exists and it is not ``None``,\n      its value will be used.\n\n    - Then it looks for the ``request.params['_LOCALE_']`` value.\n\n    - Then it looks for the ``request.cookies['_LOCALE_']`` value.\n\n    - Finally, the negotiator returns ``None`` if the locale could not\n      be determined via any of the previous checks (when a locale\n      negotiator returns ``None``, it signifies that the\n      :term:`default locale name` should be used.)\n    \"\"\"\n    name = '_LOCALE_'\n    locale_name = getattr(request, name, None)\n    if locale_name is None:\n        locale_name = request.params.get(name)\n        if locale_name is None:\n            locale_name = request.cookies.get(name)\n    return locale_name\n\n\ndef negotiate_locale_name(request):\n    \"\"\"Negotiate and return the :term:`locale name` associated with\n    the current request.\"\"\"\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = get_current_registry()\n    negotiator = registry.queryUtility(\n        ILocaleNegotiator, default=default_locale_negotiator\n    )\n    locale_name = negotiator(request)\n\n    if locale_name is None:\n        settings = registry.settings or {}\n        locale_name = settings.get('default_locale_name', 'en')\n\n    return locale_name\n\n\ndef get_locale_name(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use :attr:`pyramid.request.Request.locale_name` directly instead.\n        Return the :term:`locale name` associated with the current request.\n    \"\"\"\n    return request.locale_name\n\n\ndef make_localizer(current_locale_name, translation_directories):\n    \"\"\"Create a :class:`pyramid.i18n.Localizer` object\n    corresponding to the provided locale name from the\n    translations found in the list of translation directories.\"\"\"\n    translations = Translations()\n    translations._catalog = {}\n\n    locales_to_try = []\n    if '_' in current_locale_name:\n        locales_to_try = [current_locale_name.split('_')[0]]\n    locales_to_try.append(current_locale_name)\n\n    # intent: order locales left to right in least specific to most specific,\n    # e.g. ['de', 'de_DE'].  This services the intent of creating a\n    # translations object that returns a \"more specific\" translation for a\n    # region, but will fall back to a \"less specific\" translation for the\n    # locale if necessary.  Ordering from least specific to most specific\n    # allows us to call translations.add in the below loop to get this\n    # behavior.\n\n    for tdir in translation_directories:\n        locale_dirs = []\n        for lname in locales_to_try:\n            ldir = os.path.realpath(os.path.join(tdir, lname))\n            if os.path.isdir(ldir):\n                locale_dirs.append(ldir)\n\n        for locale_dir in locale_dirs:\n            messages_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n            if not os.path.isdir(os.path.realpath(messages_dir)):\n                continue\n            for mofile in os.listdir(messages_dir):\n                mopath = os.path.realpath(os.path.join(messages_dir, mofile))\n                if mofile.endswith('.mo') and os.path.isfile(mopath):\n                    with open(mopath, 'rb') as mofp:\n                        domain = mofile[:-3]\n                        dtrans = Translations(mofp, domain)\n                        translations.add(dtrans)\n\n    return Localizer(\n        locale_name=current_locale_name, translations=translations\n    )\n\n\ndef get_localizer(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use the :attr:`pyramid.request.Request.localizer` attribute directly\n        instead.  Retrieve a :class:`pyramid.i18n.Localizer` object\n        corresponding to the current request's locale name.\n    \"\"\"\n    return request.localizer\n\n\nclass Translations(gettext.GNUTranslations):\n    \"\"\"An extended translation catalog class (ripped off from Babel)\"\"\"\n\n    DEFAULT_DOMAIN = 'messages'\n\n    def __init__(self, fileobj=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Initialize the translations catalog.\n\n        :param fileobj: the file-like object the translation should be read\n                        from\n        \"\"\"\n        # germanic plural by default; self.plural will be overwritten by\n        # GNUTranslations._parse (called as a side effect if fileobj is\n        # passed to GNUTranslations.__init__) with a \"real\" self.plural for\n        # this domain; see https://github.com/Pylons/pyramid/issues/235\n        # It is only overridden the first time a new message file is found\n        # for a given domain, so all message files must have matching plural\n        # rules if they are in the same domain. We keep track of if we have\n        # overridden so we can special case the default domain, which is always\n        # instantiated before a message file is read.\n        # See also https://github.com/Pylons/pyramid/pull/2102\n        self.plural = DEFAULT_PLURAL\n        gettext.GNUTranslations.__init__(self, fp=fileobj)\n        self.files = list(filter(None, [getattr(fileobj, 'name', None)]))\n        self.domain = domain\n        self._domains = {}\n\n    @classmethod\n    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Load translations from the given directory.\n\n        :param dirname: the directory containing the ``MO`` files\n        :param locales: the list of locales in order of preference (items in\n                        this list can be either `Locale` objects or locale\n                        strings)\n        :param domain: the message domain\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\n                 matching translations were found\n        :rtype: `Translations`\n        \"\"\"\n        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)\n\n    def __repr__(self):\n        return '<%s: \"%s\">' % (\n            type(self).__name__,\n            self._info.get('project-id-version'),\n        )\n\n###The function: add###\n    def merge(self, translations):\n        \"\"\"Merge the given translations into the catalog.\n\n        Message translations in the specified catalog override any messages\n        with the same identifier in the existing catalog.\n\n        :param translations: the `Translations` instance with the messages to\n                             merge\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self\n\n    def dgettext(self, domain, message):\n        \"\"\"Like ``gettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dugettext(self, domain, message):\n        \"\"\"Like ``ugettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dngettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ngettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n    def dungettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ungettext()`` but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n\nclass LocalizerRequestMixin:\n    @reify\n    def localizer(self):\n        \"\"\"Convenience property to return a localizer\"\"\"\n        from pyramid.interfaces import ILocalizer\n        from pyramid.interfaces import ITranslationDirectories\n        registry = self.registry\n\n        current_locale_name = self.locale_name\n        localizer = registry.queryUtility(ILocalizer, name=current_locale_name)\n\n        if localizer is None:\n            # no localizer utility registered yet\n            tdirs = registry.queryUtility(ITranslationDirectories, default=[])\n            localizer = make_localizer(current_locale_name, tdirs)\n\n            registry.registerUtility(\n                localizer, ILocalizer, name=current_locale_name\n            )\n\n        return localizer\n\n    @reify\n    def locale_name(self):\n        locale_name = negotiate_locale_name(self)\n        return locale_name\n", "prompt": "Please write a python function called 'add' base the context. This function adds the given translations to the catalog. If the domain of the translations is different from the current catalog, they are added as a separate catalog. It also provides the option to merge translations for message domains that have already been added.:param self: Translations. An instance of the Translations class.\n:param translations: Translations. The Translations instance with the messages to add.\n:param merge: Bool. Whether translations for message domains that have already been added should be merged with the existing translations. Defaults to True.\n:return: Translations. The Translations instance (self) so that merge calls can be easily chained..\n        The context you need to refer to is as follows: import gettext\nimport os\nfrom translationstring import Pluralizer, Translator\nfrom translationstring import TranslationString  # API\nfrom translationstring import TranslationStringFactory  # API\n\nfrom pyramid.decorator import reify\nfrom pyramid.interfaces import ILocaleNegotiator\nfrom pyramid.threadlocal import get_current_registry\n\nTranslationString = TranslationString  # PyFlakes\nTranslationStringFactory = TranslationStringFactory  # PyFlakes\n\nDEFAULT_PLURAL = lambda n: int(n != 1)\n\n\nclass Localizer:\n    \"\"\"\n    An object providing translation and pluralizations related to\n    the current request's locale name.  A\n    :class:`pyramid.i18n.Localizer` object is created using the\n    :func:`pyramid.i18n.get_localizer` function.\n    \"\"\"\n\n    def __init__(self, locale_name, translations):\n        self.locale_name = locale_name\n        self.translations = translations\n        self.pluralizer = None\n        self.translator = None\n\n    def translate(self, tstring, domain=None, mapping=None):\n        \"\"\"\n        Translate a :term:`translation string` to the current language\n        and interpolate any *replacement markers* in the result.  The\n        ``translate`` method accepts three arguments: ``tstring``\n        (required), ``domain`` (optional) and ``mapping`` (optional).\n        When called, it will translate the ``tstring`` translation\n        string using the current locale.  If the current locale could not be\n        determined, the result of interpolation of the default value is\n        returned.  The optional ``domain`` argument can be used to specify\n        or override the domain of the ``tstring`` (useful when ``tstring``\n        is a normal string rather than a translation string).  The optional\n        ``mapping`` argument can specify or override the ``tstring``\n        interpolation mapping, useful when the ``tstring`` argument is\n        a simple string instead of a translation string.\n\n        Example::\n\n           from pyramid.i18n import TranslationString\n           ts = TranslationString('Add ${item}', domain='mypackage',\n                                  mapping={'item':'Item'})\n           translated = localizer.translate(ts)\n\n        Example::\n\n           translated = localizer.translate('Add ${item}', domain='mypackage',\n                                            mapping={'item':'Item'})\n\n        \"\"\"\n        if self.translator is None:\n            self.translator = Translator(self.translations)\n        return self.translator(tstring, domain=domain, mapping=mapping)\n\n    def pluralize(self, singular, plural, n, domain=None, mapping=None):\n        \"\"\"\n        Return a string translation by using two\n        :term:`message identifier` objects as a singular/plural pair\n        and an ``n`` value representing the number that appears in the\n        message using gettext plural forms support.  The ``singular``\n        and ``plural`` objects should be strings. There is no\n        reason to use translation string objects as arguments as all\n        metadata is ignored.\n\n        ``n`` represents the number of elements. ``domain`` is the\n        translation domain to use to do the pluralization, and ``mapping``\n        is the interpolation mapping that should be used on the result. If\n        the ``domain`` is not supplied, a default domain is used (usually\n        ``messages``).\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('Add ${num} item',\n                                            'Add ${num} items',\n                                            num,\n                                            mapping={'num':num})\n\n        If using the gettext plural support, which is required for\n        languages that have pluralisation rules other than n != 1, the\n        ``singular`` argument must be the message_id defined in the\n        translation file. The plural argument is not used in this case.\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('item_plural',\n                                            '',\n                                            num,\n                                            mapping={'num':num})\n\n\n        \"\"\"\n        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )\n\n\ndef default_locale_negotiator(request):\n    \"\"\"The default :term:`locale negotiator`.  Returns a locale name\n    or ``None``.\n\n    - First, the negotiator looks for the ``_LOCALE_`` attribute of\n      the request object (possibly set by a view or a listener for an\n      :term:`event`). If the attribute exists and it is not ``None``,\n      its value will be used.\n\n    - Then it looks for the ``request.params['_LOCALE_']`` value.\n\n    - Then it looks for the ``request.cookies['_LOCALE_']`` value.\n\n    - Finally, the negotiator returns ``None`` if the locale could not\n      be determined via any of the previous checks (when a locale\n      negotiator returns ``None``, it signifies that the\n      :term:`default locale name` should be used.)\n    \"\"\"\n    name = '_LOCALE_'\n    locale_name = getattr(request, name, None)\n    if locale_name is None:\n        locale_name = request.params.get(name)\n        if locale_name is None:\n            locale_name = request.cookies.get(name)\n    return locale_name\n\n\ndef negotiate_locale_name(request):\n    \"\"\"Negotiate and return the :term:`locale name` associated with\n    the current request.\"\"\"\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = get_current_registry()\n    negotiator = registry.queryUtility(\n        ILocaleNegotiator, default=default_locale_negotiator\n    )\n    locale_name = negotiator(request)\n\n    if locale_name is None:\n        settings = registry.settings or {}\n        locale_name = settings.get('default_locale_name', 'en')\n\n    return locale_name\n\n\ndef get_locale_name(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use :attr:`pyramid.request.Request.locale_name` directly instead.\n        Return the :term:`locale name` associated with the current request.\n    \"\"\"\n    return request.locale_name\n\n\ndef make_localizer(current_locale_name, translation_directories):\n    \"\"\"Create a :class:`pyramid.i18n.Localizer` object\n    corresponding to the provided locale name from the\n    translations found in the list of translation directories.\"\"\"\n    translations = Translations()\n    translations._catalog = {}\n\n    locales_to_try = []\n    if '_' in current_locale_name:\n        locales_to_try = [current_locale_name.split('_')[0]]\n    locales_to_try.append(current_locale_name)\n\n    # intent: order locales left to right in least specific to most specific,\n    # e.g. ['de', 'de_DE'].  This services the intent of creating a\n    # translations object that returns a \"more specific\" translation for a\n    # region, but will fall back to a \"less specific\" translation for the\n    # locale if necessary.  Ordering from least specific to most specific\n    # allows us to call translations.add in the below loop to get this\n    # behavior.\n\n    for tdir in translation_directories:\n        locale_dirs = []\n        for lname in locales_to_try:\n            ldir = os.path.realpath(os.path.join(tdir, lname))\n            if os.path.isdir(ldir):\n                locale_dirs.append(ldir)\n\n        for locale_dir in locale_dirs:\n            messages_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n            if not os.path.isdir(os.path.realpath(messages_dir)):\n                continue\n            for mofile in os.listdir(messages_dir):\n                mopath = os.path.realpath(os.path.join(messages_dir, mofile))\n                if mofile.endswith('.mo') and os.path.isfile(mopath):\n                    with open(mopath, 'rb') as mofp:\n                        domain = mofile[:-3]\n                        dtrans = Translations(mofp, domain)\n                        translations.add(dtrans)\n\n    return Localizer(\n        locale_name=current_locale_name, translations=translations\n    )\n\n\ndef get_localizer(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use the :attr:`pyramid.request.Request.localizer` attribute directly\n        instead.  Retrieve a :class:`pyramid.i18n.Localizer` object\n        corresponding to the current request's locale name.\n    \"\"\"\n    return request.localizer\n\n\nclass Translations(gettext.GNUTranslations):\n    \"\"\"An extended translation catalog class (ripped off from Babel)\"\"\"\n\n    DEFAULT_DOMAIN = 'messages'\n\n    def __init__(self, fileobj=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Initialize the translations catalog.\n\n        :param fileobj: the file-like object the translation should be read\n                        from\n        \"\"\"\n        # germanic plural by default; self.plural will be overwritten by\n        # GNUTranslations._parse (called as a side effect if fileobj is\n        # passed to GNUTranslations.__init__) with a \"real\" self.plural for\n        # this domain; see https://github.com/Pylons/pyramid/issues/235\n        # It is only overridden the first time a new message file is found\n        # for a given domain, so all message files must have matching plural\n        # rules if they are in the same domain. We keep track of if we have\n        # overridden so we can special case the default domain, which is always\n        # instantiated before a message file is read.\n        # See also https://github.com/Pylons/pyramid/pull/2102\n        self.plural = DEFAULT_PLURAL\n        gettext.GNUTranslations.__init__(self, fp=fileobj)\n        self.files = list(filter(None, [getattr(fileobj, 'name', None)]))\n        self.domain = domain\n        self._domains = {}\n\n    @classmethod\n    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Load translations from the given directory.\n\n        :param dirname: the directory containing the ``MO`` files\n        :param locales: the list of locales in order of preference (items in\n                        this list can be either `Locale` objects or locale\n                        strings)\n        :param domain: the message domain\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\n                 matching translations were found\n        :rtype: `Translations`\n        \"\"\"\n        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)\n\n    def __repr__(self):\n        return '<%s: \"%s\">' % (\n            type(self).__name__,\n            self._info.get('project-id-version'),\n        )\n\n###The function: add###\n    def merge(self, translations):\n        \"\"\"Merge the given translations into the catalog.\n\n        Message translations in the specified catalog override any messages\n        with the same identifier in the existing catalog.\n\n        :param translations: the `Translations` instance with the messages to\n                             merge\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self\n\n    def dgettext(self, domain, message):\n        \"\"\"Like ``gettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dugettext(self, domain, message):\n        \"\"\"Like ``ugettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dngettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ngettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n    def dungettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ungettext()`` but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n\nclass LocalizerRequestMixin:\n    @reify\n    def localizer(self):\n        \"\"\"Convenience property to return a localizer\"\"\"\n        from pyramid.interfaces import ILocalizer\n        from pyramid.interfaces import ITranslationDirectories\n        registry = self.registry\n\n        current_locale_name = self.locale_name\n        localizer = registry.queryUtility(ILocalizer, name=current_locale_name)\n\n        if localizer is None:\n            # no localizer utility registered yet\n            tdirs = registry.queryUtility(ITranslationDirectories, default=[])\n            localizer = make_localizer(current_locale_name, tdirs)\n\n            registry.registerUtility(\n                localizer, ILocalizer, name=current_locale_name\n            )\n\n        return localizer\n\n    @reify\n    def locale_name(self):\n        locale_name = negotiate_locale_name(self)\n        return locale_name\n", "test_list": ["def test_add_default_domain_replaces_plural_first_time(self):\n    inst = self._getTargetClass()(None, domain='messages')\n    inst2 = self._getTargetClass()(None, domain='messages')\n    inst3 = self._getTargetClass()(None, domain='messages')\n    inst._catalog = {}\n    inst2._catalog = {}\n    inst3._catalog = {}\n    self.assertEqual(inst.plural(0), 1)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)\n    inst2.plural = lambda n: n > 1\n    inst.add(inst2)\n    self.assertEqual(inst.plural(0), 0)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)\n    inst3.plural = lambda n: n > 0\n    inst.add(inst3)\n    self.assertEqual(inst.plural(0), 0)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)", "def test_add_different_domain_merge_true_notexisting(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2'], inst2)", "def test_add_same_domain_merge_true(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2._catalog['a'] = 'b'\n    inst.add(inst2)\n    self.assertEqual(inst._catalog['a'], 'b')", "def test_add_different_domain_merge_true_existing(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst3 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst2._catalog['a'] = 'b'\n    inst3.domain = 'domain2'\n    inst._domains['domain2'] = inst3\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2'], inst3)\n    self.assertEqual(inst3._catalog['a'], 'b')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add' function should ensure that the 'translations' parameter is an instance of the Translations class and that the 'merge' parameter is a boolean. If the input is not legal, please raise a TypeError.", "unit_test": "def test_add_input_output_conditions(self):\n    inst = self._makeOne()\n    with self.assertRaises(TypeError):\n        inst.add('not a Translations instance')\n    with self.assertRaises(TypeError):\n        inst.add(self._makeOne(), merge='not a boolean')", "test": "tests/test_i18n.py::TestTranslations::test_add_input_output_conditions"}, "Exception Handling": {"requirement": "The 'add' function should raise a ValueError if the 'translations' parameter is None.", "unit_test": "def test_add_exception_handling(self):\n    inst = self._makeOne()\n    with self.assertRaises(ValueError):\n        inst.add(None)", "test": "tests/test_i18n.py::TestTranslations::test_add_exception_handling"}, "Edge Case Handling": {"requirement": "The 'add' function should handle the case where the 'translations' parameter has an empty catalog gracefully.", "unit_test": "def test_add_edge_case_handling_empty_catalog(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2._catalog = {}\n    inst.add(inst2)\n    self.assertEqual(inst._catalog, {})", "test": "tests/test_i18n.py::TestTranslations::test_add_edge_case_handling_empty_catalog"}, "Functionality Extension": {"requirement": "Extend the 'add' function to allow adding multiple Translations instances at once by accepting a list of Translations objects.", "unit_test": "def test_add_functionality_extension_multiple_translations(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst3 = self._makeOne()\n    inst2._catalog['a'] = 'b'\n    inst3._catalog['c'] = 'd'\n    inst.add([inst2, inst3])\n    self.assertEqual(inst._catalog['a'], 'b')\n    self.assertEqual(inst._catalog['c'], 'd')", "test": "tests/test_i18n.py::TestTranslations::test_add_functionality_extension_multiple_translations"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'add' function are properly annotated with type hints.", "unit_test": "def test_add_annotation_coverage(self):\n    from typing import get_type_hints\n    hints = get_type_hints(self._makeOne().add)\n    self.assertEqual(hints['translations'], Translations)\n    self.assertEqual(hints['merge'], bool)\n    self.assertEqual(hints['return'], Translations)", "test": "tests/test_i18n.py::TestTranslations::test_add_annotation_coverage"}, "Code Complexity": {"requirement": "The 'add' function should maintain a cyclomatic complexity of 7 or less.", "unit_test": "def test_add_code_complexity(self):\n    from radon.complexity import cc_visit\n    code = inspect.getsource(self._makeOne().add)\n    complexity = cc_visit(code)\n    self.assertTrue(all(c.complexity <= 5 for c in complexity))", "test": "tests/test_i18n.py::TestTranslations::test_code_complexity"}, "Code Standard": {"requirement": "The 'add' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_add_code_standard(self):\n    import pep8\n    style = pep8.StyleGuide(quiet=True)\n    result = style.check_files([inspect.getfile(self._makeOne().add)])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/test_i18n.py::TestTranslations::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'add' function should utilize the '_domains' attributes from the Translations class.", "unit_test": "def test_add_context_usage_verification(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst.add(inst2)\n    self.assertIn('domain2', inst._domains)", "test": "tests/test_i18n.py::TestTranslations::test_add_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'add' function should correctly update the '_domains' dictionary when adding translations with a different domain.", "unit_test": "def test_add_context_usage_correctness_verification(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst2._catalog['a'] = 'b'\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2']._catalog['a'], 'b')", "test": "tests/test_i18n.py::TestTranslations::test_add_context_usage_correctness_verification"}}}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "type": "method", "project_path": "Utilities/python-for-android", "completion_path": "Utilities/python-for-android/pythonforandroid/prerequisites.py", "signature_position": [269, 269], "body_position": [270, 275], "dependency": {"intra_class": ["pythonforandroid.prerequisites.OpenSSLPrerequisite.homebrew_formula_name"], "intra_file": ["pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix"], "cross_file": []}, "requirement": {"Functionality": "Check if the OpenSSL prerequisite is met on a Darwin (MacOS) system. It checks if the Homebrew formula for OpenSSL is installed.", "Arguments": ":param self: OpenSSLPrerequisite. An instance of the OpenSSLPrerequisite class.\n:return: bool. True if the OpenSSL prerequisite is met, False otherwise."}, "tests": ["tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker"], "indent": 4, "domain": "Utilities", "code": "    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            is not None\n        )\n", "context": "#!/usr/bin/env python3\n\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport sys\n\nfrom pythonforandroid.logger import info, warning, error\nfrom pythonforandroid.util import ensure_dir\n\n\nclass Prerequisite(object):\n    name = \"Default\"\n    homebrew_formula_name = \"\"\n    mandatory = dict(linux=False, darwin=False)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def is_valid(self):\n        if self.checker():\n            info(f\"Prerequisite {self.name} is met\")\n            return (True, \"\")\n        elif not self.mandatory[sys.platform]:\n            warning(\n                f\"Prerequisite {self.name} is not met, but is marked as non-mandatory\"\n            )\n        else:\n            error(f\"Prerequisite {self.name} is not met\")\n\n    def checker(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_checker()\n        elif sys.platform == \"linux\":\n            return self.linux_checker()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def ask_to_install(self):\n        if (\n            os.environ.get(\"PYTHONFORANDROID_PREREQUISITES_INSTALL_INTERACTIVE\", \"1\")\n            == \"1\"\n        ):\n            res = input(\n                f\"Do you want automatically install prerequisite {self.name}? [y/N] \"\n            )\n            if res.lower() == \"y\":\n                return True\n            else:\n                return False\n        else:\n            info(\n                \"Session is not interactive (usually this happens during a CI run), so let's consider it as a YES\"\n            )\n            return True\n\n    def install(self):\n        info(f\"python-for-android can automatically install prerequisite: {self.name}\")\n        if self.ask_to_install():\n            if sys.platform == \"darwin\":\n                self.darwin_installer()\n            elif sys.platform == \"linux\":\n                self.linux_installer()\n            else:\n                raise Exception(\"Unsupported platform\")\n        else:\n            info(\n                f\"Skipping installation of prerequisite {self.name} as per user request\"\n            )\n\n    def show_helper(self):\n        if sys.platform == \"darwin\":\n            self.darwin_helper()\n        elif sys.platform == \"linux\":\n            self.linux_helper()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def install_is_supported(self):\n        return self.installer_is_supported[sys.platform]\n\n    def linux_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on linux for {self.name}\")\n\n    def darwin_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on macOS for {self.name}\")\n\n    def linux_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on linux for {self.name}\")\n\n    def darwin_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on macOS for {self.name}\")\n\n    def darwin_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on macOS\")\n\n    def linux_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on linux\")\n\n    def _darwin_get_brew_formula_location_prefix(self, formula, installed=False):\n        opts = [\"--installed\"] if installed else []\n        p = subprocess.Popen(\n            [\"brew\", \"--prefix\", formula, *opts],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(_stderr_res.decode(\"utf-8\").strip())\n            return None\n        else:\n            return _stdout_res.decode(\"utf-8\").strip()\n\n    def darwin_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on macOS for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    def linux_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on linux for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    @property\n    def pkg_config_location(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_pkg_config_location()\n        elif sys.platform == \"linux\":\n            return self.linux_pkg_config_location()\n\n\nclass HomebrewPrerequisite(Prerequisite):\n    name = \"homebrew\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def darwin_checker(self):\n        return shutil.which(\"brew\") is not None\n\n    def darwin_helper(self):\n        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )\n\n\nclass JDKPrerequisite(Prerequisite):\n    name = \"JDK\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n    min_supported_version = 11\n\n    def darwin_checker(self):\n        if \"JAVA_HOME\" in os.environ:\n            info(\"Found JAVA_HOME environment variable, using it\")\n            jdk_path = os.environ[\"JAVA_HOME\"]\n        else:\n            jdk_path = self._darwin_get_libexec_jdk_path(version=None)\n        return self._darwin_jdk_is_supported(jdk_path)\n\n    def _darwin_get_libexec_jdk_path(self, version=None):\n        version_args = []\n        if version is not None:\n            version_args = [\"-v\", version]\n        return (\n            subprocess.run(\n                [\"/usr/libexec/java_home\", *version_args],\n                stdout=subprocess.PIPE,\n            )\n            .stdout.strip()\n            .decode()\n        )\n\n    def _darwin_jdk_is_supported(self, jdk_path):\n        if not jdk_path:\n            return False\n\n        javac_bin = os.path.join(jdk_path, \"bin\", \"javac\")\n        if not os.path.exists(javac_bin):\n            return False\n\n        p = subprocess.Popen(\n            [javac_bin, \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(\"Failed to run javac to check JDK version\")\n            return False\n\n        if not _stdout_res:\n            _stdout_res = _stderr_res\n\n        res = _stdout_res.strip().decode()\n\n        major_version = int(res.split(\" \")[-1].split(\".\")[0])\n        if major_version >= self.min_supported_version:\n            info(f\"Found a valid JDK at {jdk_path}\")\n            return True\n        else:\n            error(f\"JDK {self.min_supported_version} or higher is required\")\n            return False\n\n    def darwin_helper(self):\n        info(\n            \"python-for-android requires a JDK 11 or higher to be installed on macOS,\"\n            \"but seems like you don't have one installed.\"\n        )\n        info(\n            \"If you think that a valid JDK is already installed, please verify that \"\n            \"you have a JDK 11 or higher installed and that `/usr/libexec/java_home` \"\n            \"shows the correct path.\"\n        )\n        info(\n            \"If you have multiple JDK installations, please make sure that you have \"\n            \"`JAVA_HOME` environment variable set to the correct JDK installation.\"\n        )\n\n    def darwin_installer(self):\n        info(\n            \"Looking for a JDK 11 or higher installation which is not the default one ...\"\n        )\n        jdk_path = self._darwin_get_libexec_jdk_path(version=\"11+\")\n\n        if not self._darwin_jdk_is_supported(jdk_path):\n            info(\"We're unlucky, there's no JDK 11 or higher installation available\")\n\n            base_url = \"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.2%2B8/\"\n            if platform.machine() == \"arm64\":\n                filename = \"OpenJDK17U-jdk_aarch64_mac_hotspot_17.0.2_8.tar.gz\"\n            else:\n                filename = \"OpenJDK17U-jdk_x64_mac_hotspot_17.0.2_8.tar.gz\"\n\n            info(f\"Downloading {filename} from {base_url}\")\n            subprocess.check_output(\n                [\n                    \"curl\",\n                    \"-L\",\n                    f\"{base_url}{filename}\",\n                    \"-o\",\n                    f\"/tmp/{filename}\",\n                ]\n            )\n\n            user_library_java_path = os.path.expanduser(\n                \"~/Library/Java/JavaVirtualMachines\"\n            )\n            info(f\"Extracting {filename} to {user_library_java_path}\")\n            ensure_dir(user_library_java_path)\n            subprocess.check_output(\n                [\"tar\", \"xzf\", f\"/tmp/{filename}\", \"-C\", user_library_java_path],\n            )\n\n            jdk_path = self._darwin_get_libexec_jdk_path(version=\"17.0.2+8\")\n\n        info(f\"Setting JAVA_HOME to {jdk_path}\")\n        os.environ[\"JAVA_HOME\"] = jdk_path\n\n\nclass OpenSSLPrerequisite(Prerequisite):\n    name = \"openssl\"\n    homebrew_formula_name = \"openssl@1.1\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n###The function: darwin_checker###\n    def darwin_pkg_config_location(self):\n        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n\n    def darwin_installer(self):\n        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n\nclass AutoconfPrerequisite(Prerequisite):\n    name = \"autoconf\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"autoconf\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n\nclass AutomakePrerequisite(Prerequisite):\n    name = \"automake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n\nclass LibtoolPrerequisite(Prerequisite):\n    name = \"libtool\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n\nclass PkgConfigPrerequisite(Prerequisite):\n    name = \"pkg-config\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n\nclass CmakePrerequisite(Prerequisite):\n    name = \"cmake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n\ndef get_required_prerequisites(platform=\"linux\"):\n    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]\n\n\ndef check_and_install_default_prerequisites():\n\n    prerequisites_not_met = []\n\n    warning(\n        \"prerequisites.py is experimental and does not support all prerequisites yet.\"\n    )\n    warning(\"Please report any issues to the python-for-android issue tracker.\")\n\n    # Phase 1: Check if all prerequisites are met and add the ones\n    # which are not to `prerequisites_not_met`\n    for prerequisite in get_required_prerequisites(sys.platform):\n        if not prerequisite.is_valid():\n            prerequisites_not_met.append(prerequisite)\n\n    # Phase 2: Setup/Install all prerequisites that are not met\n    # (where possible), otherwise show an helper.\n    for prerequisite in prerequisites_not_met:\n        prerequisite.show_helper()\n        if prerequisite.install_is_supported():\n            prerequisite.install()\n\n\nif __name__ == \"__main__\":\n    check_and_install_default_prerequisites()\n", "prompt": "Please write a python function called 'darwin_checker' base the context. Check if the OpenSSL prerequisite is met on a Darwin (MacOS) system. It checks if the Homebrew formula for OpenSSL is installed.:param self: OpenSSLPrerequisite. An instance of the OpenSSLPrerequisite class.\n:return: bool. True if the OpenSSL prerequisite is met, False otherwise..\n        The context you need to refer to is as follows: #!/usr/bin/env python3\n\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport sys\n\nfrom pythonforandroid.logger import info, warning, error\nfrom pythonforandroid.util import ensure_dir\n\n\nclass Prerequisite(object):\n    name = \"Default\"\n    homebrew_formula_name = \"\"\n    mandatory = dict(linux=False, darwin=False)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def is_valid(self):\n        if self.checker():\n            info(f\"Prerequisite {self.name} is met\")\n            return (True, \"\")\n        elif not self.mandatory[sys.platform]:\n            warning(\n                f\"Prerequisite {self.name} is not met, but is marked as non-mandatory\"\n            )\n        else:\n            error(f\"Prerequisite {self.name} is not met\")\n\n    def checker(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_checker()\n        elif sys.platform == \"linux\":\n            return self.linux_checker()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def ask_to_install(self):\n        if (\n            os.environ.get(\"PYTHONFORANDROID_PREREQUISITES_INSTALL_INTERACTIVE\", \"1\")\n            == \"1\"\n        ):\n            res = input(\n                f\"Do you want automatically install prerequisite {self.name}? [y/N] \"\n            )\n            if res.lower() == \"y\":\n                return True\n            else:\n                return False\n        else:\n            info(\n                \"Session is not interactive (usually this happens during a CI run), so let's consider it as a YES\"\n            )\n            return True\n\n    def install(self):\n        info(f\"python-for-android can automatically install prerequisite: {self.name}\")\n        if self.ask_to_install():\n            if sys.platform == \"darwin\":\n                self.darwin_installer()\n            elif sys.platform == \"linux\":\n                self.linux_installer()\n            else:\n                raise Exception(\"Unsupported platform\")\n        else:\n            info(\n                f\"Skipping installation of prerequisite {self.name} as per user request\"\n            )\n\n    def show_helper(self):\n        if sys.platform == \"darwin\":\n            self.darwin_helper()\n        elif sys.platform == \"linux\":\n            self.linux_helper()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def install_is_supported(self):\n        return self.installer_is_supported[sys.platform]\n\n    def linux_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on linux for {self.name}\")\n\n    def darwin_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on macOS for {self.name}\")\n\n    def linux_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on linux for {self.name}\")\n\n    def darwin_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on macOS for {self.name}\")\n\n    def darwin_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on macOS\")\n\n    def linux_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on linux\")\n\n    def _darwin_get_brew_formula_location_prefix(self, formula, installed=False):\n        opts = [\"--installed\"] if installed else []\n        p = subprocess.Popen(\n            [\"brew\", \"--prefix\", formula, *opts],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(_stderr_res.decode(\"utf-8\").strip())\n            return None\n        else:\n            return _stdout_res.decode(\"utf-8\").strip()\n\n    def darwin_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on macOS for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    def linux_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on linux for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    @property\n    def pkg_config_location(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_pkg_config_location()\n        elif sys.platform == \"linux\":\n            return self.linux_pkg_config_location()\n\n\nclass HomebrewPrerequisite(Prerequisite):\n    name = \"homebrew\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def darwin_checker(self):\n        return shutil.which(\"brew\") is not None\n\n    def darwin_helper(self):\n        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )\n\n\nclass JDKPrerequisite(Prerequisite):\n    name = \"JDK\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n    min_supported_version = 11\n\n    def darwin_checker(self):\n        if \"JAVA_HOME\" in os.environ:\n            info(\"Found JAVA_HOME environment variable, using it\")\n            jdk_path = os.environ[\"JAVA_HOME\"]\n        else:\n            jdk_path = self._darwin_get_libexec_jdk_path(version=None)\n        return self._darwin_jdk_is_supported(jdk_path)\n\n    def _darwin_get_libexec_jdk_path(self, version=None):\n        version_args = []\n        if version is not None:\n            version_args = [\"-v\", version]\n        return (\n            subprocess.run(\n                [\"/usr/libexec/java_home\", *version_args],\n                stdout=subprocess.PIPE,\n            )\n            .stdout.strip()\n            .decode()\n        )\n\n    def _darwin_jdk_is_supported(self, jdk_path):\n        if not jdk_path:\n            return False\n\n        javac_bin = os.path.join(jdk_path, \"bin\", \"javac\")\n        if not os.path.exists(javac_bin):\n            return False\n\n        p = subprocess.Popen(\n            [javac_bin, \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(\"Failed to run javac to check JDK version\")\n            return False\n\n        if not _stdout_res:\n            _stdout_res = _stderr_res\n\n        res = _stdout_res.strip().decode()\n\n        major_version = int(res.split(\" \")[-1].split(\".\")[0])\n        if major_version >= self.min_supported_version:\n            info(f\"Found a valid JDK at {jdk_path}\")\n            return True\n        else:\n            error(f\"JDK {self.min_supported_version} or higher is required\")\n            return False\n\n    def darwin_helper(self):\n        info(\n            \"python-for-android requires a JDK 11 or higher to be installed on macOS,\"\n            \"but seems like you don't have one installed.\"\n        )\n        info(\n            \"If you think that a valid JDK is already installed, please verify that \"\n            \"you have a JDK 11 or higher installed and that `/usr/libexec/java_home` \"\n            \"shows the correct path.\"\n        )\n        info(\n            \"If you have multiple JDK installations, please make sure that you have \"\n            \"`JAVA_HOME` environment variable set to the correct JDK installation.\"\n        )\n\n    def darwin_installer(self):\n        info(\n            \"Looking for a JDK 11 or higher installation which is not the default one ...\"\n        )\n        jdk_path = self._darwin_get_libexec_jdk_path(version=\"11+\")\n\n        if not self._darwin_jdk_is_supported(jdk_path):\n            info(\"We're unlucky, there's no JDK 11 or higher installation available\")\n\n            base_url = \"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.2%2B8/\"\n            if platform.machine() == \"arm64\":\n                filename = \"OpenJDK17U-jdk_aarch64_mac_hotspot_17.0.2_8.tar.gz\"\n            else:\n                filename = \"OpenJDK17U-jdk_x64_mac_hotspot_17.0.2_8.tar.gz\"\n\n            info(f\"Downloading {filename} from {base_url}\")\n            subprocess.check_output(\n                [\n                    \"curl\",\n                    \"-L\",\n                    f\"{base_url}{filename}\",\n                    \"-o\",\n                    f\"/tmp/{filename}\",\n                ]\n            )\n\n            user_library_java_path = os.path.expanduser(\n                \"~/Library/Java/JavaVirtualMachines\"\n            )\n            info(f\"Extracting {filename} to {user_library_java_path}\")\n            ensure_dir(user_library_java_path)\n            subprocess.check_output(\n                [\"tar\", \"xzf\", f\"/tmp/{filename}\", \"-C\", user_library_java_path],\n            )\n\n            jdk_path = self._darwin_get_libexec_jdk_path(version=\"17.0.2+8\")\n\n        info(f\"Setting JAVA_HOME to {jdk_path}\")\n        os.environ[\"JAVA_HOME\"] = jdk_path\n\n\nclass OpenSSLPrerequisite(Prerequisite):\n    name = \"openssl\"\n    homebrew_formula_name = \"openssl@1.1\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n###The function: darwin_checker###\n    def darwin_pkg_config_location(self):\n        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n\n    def darwin_installer(self):\n        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n\nclass AutoconfPrerequisite(Prerequisite):\n    name = \"autoconf\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"autoconf\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n\nclass AutomakePrerequisite(Prerequisite):\n    name = \"automake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n\nclass LibtoolPrerequisite(Prerequisite):\n    name = \"libtool\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n\nclass PkgConfigPrerequisite(Prerequisite):\n    name = \"pkg-config\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n\nclass CmakePrerequisite(Prerequisite):\n    name = \"cmake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n\ndef get_required_prerequisites(platform=\"linux\"):\n    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]\n\n\ndef check_and_install_default_prerequisites():\n\n    prerequisites_not_met = []\n\n    warning(\n        \"prerequisites.py is experimental and does not support all prerequisites yet.\"\n    )\n    warning(\"Please report any issues to the python-for-android issue tracker.\")\n\n    # Phase 1: Check if all prerequisites are met and add the ones\n    # which are not to `prerequisites_not_met`\n    for prerequisite in get_required_prerequisites(sys.platform):\n        if not prerequisite.is_valid():\n            prerequisites_not_met.append(prerequisite)\n\n    # Phase 2: Setup/Install all prerequisites that are not met\n    # (where possible), otherwise show an helper.\n    for prerequisite in prerequisites_not_met:\n        prerequisite.show_helper()\n        if prerequisite.install_is_supported():\n            prerequisite.install()\n\n\nif __name__ == \"__main__\":\n    check_and_install_default_prerequisites()\n", "test_list": ["@mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix')\ndef test_darwin_checker(self, _darwin_get_brew_formula_location_prefix):\n    _darwin_get_brew_formula_location_prefix.return_value = None\n    self.assertFalse(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.return_value = self.expected_homebrew_location_prefix\n    self.assertTrue(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.assert_called_with(self.expected_homebrew_formula_name, installed=True)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'darwin_checker' function should return a boolean value indicating whether the OpenSSL prerequisite is met on a Darwin system. It should return True if the Homebrew formula for OpenSSL is installed, and False otherwise.", "unit_test": "@mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix')\ndef test_darwin_checker_output(self, _darwin_get_brew_formula_location_prefix):\n    _darwin_get_brew_formula_location_prefix.return_value = None\n    self.assertFalse(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.return_value = '/usr/local/opt/openssl@1.1'\n    self.assertTrue(self.prerequisite.darwin_checker())", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_output"}, "Exception Handling": {"requirement": "The 'darwin_checker' function should handle exceptions gracefully, logging an error message if the subprocess call to check the Homebrew formula fails.", "unit_test": "@mock.patch('subprocess.Popen')\ndef test_darwin_checker_exception_handling(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'', b'Error'), 'returncode': 1}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    with self.assertLogs('pythonforandroid.logger', level='ERROR') as log:\n        self.assertFalse(self.prerequisite.darwin_checker())\n        self.assertIn('Error', log.output[0])", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_exception_handling"}, "Edge Case Handling": {"requirement": "The 'darwin_checker' function should handle edge cases such as an empty or malformed response from the subprocess call.", "unit_test": "@mock.patch('subprocess.Popen')\ndef test_darwin_checker_edge_cases(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    self.assertFalse(self.prerequisite.darwin_checker())", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'darwin_checker' function to also verify the version of OpenSSL installed, ensuring it meets a minimum version requirement.", "unit_test": "@mock.patch('subprocess.Popen')\ndef test_darwin_checker_version_check(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'OpenSSL 1.1.1', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    self.assertTrue(self.prerequisite.darwin_checker())\n    attrs = {'communicate.return_value': (b'OpenSSL 1.0.2', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    self.assertFalse(self.prerequisite.darwin_checker())", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_version_check"}, "Annotation Coverage": {"requirement": "Ensure that the 'darwin_checker' function is fully annotated with type hints for parameters and return types.", "unit_test": "def test_darwin_checker_annotations(self):\n    annotations = self.prerequisite.darwin_checker.__annotations__\n    self.assertEqual(annotations['return'], bool)", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_annotations"}, "Code Complexity": {"requirement": "The 'darwin_checker' function should maintain a cyclomatic complexity of 5 or lower to ensure readability and maintainability.", "unit_test": "def test_darwin_checker_complexity(self):\n    complexity = get_cyclomatic_complexity(self.prerequisite.darwin_checker)\n    self.assertLessEqual(complexity, 5)", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_code_complexity"}, "Code Standard": {"requirement": "The 'darwin_checker' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": "def test_darwin_checker_pep8(self):\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['path/to/your/module.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'darwin_checker' function should utilize the 'homebrew_formula_name' attribute from the OpenSSLPrerequisite class to determine the correct formula to check.", "unit_test": "def test_darwin_checker_context_usage(self):\n    self.assertEqual(self.prerequisite.homebrew_formula_name, 'openssl@1.1')\n    with mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix') as mocked_method:\n        mocked_method.return_value = '/usr/local/opt/openssl@1.1'\n        self.prerequisite.darwin_checker()\n        mocked_method.assert_called_with('openssl@1.1', installed=True)", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'darwin_checker' function correctly uses the 'homebrew_formula_name' attribute to check for the installation of the OpenSSL formula.", "unit_test": "def test_darwin_checker_correct_context_usage(self):\n    with mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix') as mocked_method:\n        mocked_method.return_value = '/usr/local/opt/openssl@1.1'\n        self.prerequisite.darwin_checker()\n        mocked_method.assert_called_with(self.prerequisite.homebrew_formula_name, installed=True)", "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_correct_context_usage"}}}
{"namespace": "boltons.cacheutils.LRI.pop", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/cacheutils.py", "signature_position": [268, 269], "body_position": [270, 279], "dependency": {"intra_class": ["boltons.cacheutils.LRI._lock", "boltons.cacheutils.LRI._remove_from_ll"], "intra_file": ["boltons.cacheutils._MISSING"], "cross_file": []}, "requirement": {"Functionality": "Pop the key in the LRI instance and return the corresponding value. If the key is not found and the default value is not passed, the exception is re-raised. This function bypasses the hit count and miss count.\n", "Arguments": ":param self: LRI, an instance of the LRI class.\n:param key: The key to remove in the instance.\n:param default: The value to return if the key is not found in the instance. Defaults to _UNSET.\n:return: The value corresponding to the key.\n"}, "tests": ["tests/test_cacheutils.py::test_lru_basic"], "indent": 4, "domain": "Utilities", "code": "    def pop(self, key, default=_MISSING):\n        # NB: hit/miss counts are bypassed for pop()\n        with self._lock:\n            try:\n                ret = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                ret = default\n            else:\n                self._remove_from_ll(key)\n            return ret\n", "context": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"``cacheutils`` contains consistent implementations of fundamental\ncache types. Currently there are two to choose from:\n\n  * :class:`LRI` - Least-recently inserted\n  * :class:`LRU` - Least-recently used\n\nBoth caches are :class:`dict` subtypes, designed to be as\ninterchangeable as possible, to facilitate experimentation. A key\npractice with performance enhancement with caching is ensuring that\nthe caching strategy is working. If the cache is constantly missing,\nit is just adding more overhead and code complexity. The standard\nstatistics are:\n\n  * ``hit_count`` - the number of times the queried key has been in\n    the cache\n  * ``miss_count`` - the number of times a key has been absent and/or\n    fetched by the cache\n  * ``soft_miss_count`` - the number of times a key has been absent,\n    but a default has been provided by the caller, as with\n    :meth:`dict.get` and :meth:`dict.setdefault`. Soft misses are a\n    subset of misses, so this number is always less than or equal to\n    ``miss_count``.\n\nAdditionally, ``cacheutils`` provides :class:`ThresholdCounter`, a\ncache-like bounded counter useful for online statistics collection.\n\nLearn more about `caching algorithms on Wikipedia\n<https://en.wikipedia.org/wiki/Cache_algorithms#Examples>`_.\n\n\"\"\"\n\n# TODO: TimedLRI\n# TODO: support 0 max_size?\n\n\nimport heapq\nimport weakref\nimport itertools\nfrom operator import attrgetter\n\ntry:\n    from threading import RLock\nexcept Exception:\n    class RLock(object):\n        'Dummy reentrant lock for builds without threads'\n        def __enter__(self):\n            pass\n\n        def __exit__(self, exctype, excinst, exctb):\n            pass\n\ntry:\n    from .typeutils import make_sentinel\n    _MISSING = make_sentinel(var_name='_MISSING')\n    _KWARG_MARK = make_sentinel(var_name='_KWARG_MARK')\nexcept ImportError:\n    _MISSING = object()\n    _KWARG_MARK = object()\n\ntry:\n    xrange\nexcept NameError:\n    # py3\n    xrange = range\n    unicode, str, bytes, basestring = str, bytes, bytes, (str, bytes)\n\nPREV, NEXT, KEY, VALUE = range(4)   # names for the link fields\nDEFAULT_MAX_SIZE = 128\n\n\nclass LRI(dict):\n    \"\"\"The ``LRI`` implements the basic *Least Recently Inserted* strategy to\n    caching. One could also think of this as a ``SizeLimitedDefaultDict``.\n\n    *on_miss* is a callable that accepts the missing key (as opposed\n    to :class:`collections.defaultdict`'s \"default_factory\", which\n    accepts no arguments.) Also note that, like the :class:`LRI`,\n    the ``LRI`` is instrumented with statistics tracking.\n\n    >>> cap_cache = LRI(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n    \"\"\"\n    def __init__(self, max_size=DEFAULT_MAX_SIZE, values=None,\n                 on_miss=None):\n        if max_size <= 0:\n            raise ValueError('expected max_size > 0, not %r' % max_size)\n        self.hit_count = self.miss_count = self.soft_miss_count = 0\n        self.max_size = max_size\n        self._lock = RLock()\n        self._init_ll()\n\n        if on_miss is not None and not callable(on_miss):\n            raise TypeError('expected on_miss to be a callable'\n                            ' (or None), not %r' % on_miss)\n        self.on_miss = on_miss\n\n        if values:\n            self.update(values)\n\n    # TODO: fromkeys()?\n\n    # linked list manipulation methods.\n    #\n    # invariants:\n    # 1) 'anchor' is the sentinel node in the doubly linked list.  there is\n    #    always only one, and its KEY and VALUE are both _MISSING.\n    # 2) the most recently accessed node comes immediately before 'anchor'.\n    # 3) the least recently accessed node comes immediately after 'anchor'.\n    def _init_ll(self):\n        anchor = []\n        anchor[:] = [anchor, anchor, _MISSING, _MISSING]\n        # a link lookup table for finding linked list links in O(1)\n        # time.\n        self._link_lookup = {}\n        self._anchor = anchor\n\n    def _print_ll(self):\n        print('***')\n        for (key, val) in self._get_flattened_ll():\n            print(key, val)\n        print('***')\n        return\n\n    def _get_flattened_ll(self):\n        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list\n\n    def _get_link_and_move_to_front_of_ll(self, key):\n        # find what will become the newest link. this may raise a\n        # KeyError, which is useful to __getitem__ and __setitem__\n        newest = self._link_lookup[key]\n\n        # splice out what will become the newest link.\n        newest[PREV][NEXT] = newest[NEXT]\n        newest[NEXT][PREV] = newest[PREV]\n\n        # move what will become the newest link immediately before\n        # anchor (invariant 2)\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        second_newest[NEXT] = anchor[PREV] = newest\n        newest[PREV] = second_newest\n        newest[NEXT] = anchor\n        return newest\n\n    def _set_key_and_add_to_front_of_ll(self, key, value):\n        # create a new link and place it immediately before anchor\n        # (invariant 2).\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        newest = [second_newest, anchor, key, value]\n        second_newest[NEXT] = anchor[PREV] = newest\n        self._link_lookup[key] = newest\n\n    def _set_key_and_evict_last_in_ll(self, key, value):\n        # the link after anchor is the oldest in the linked list\n        # (invariant 3).  the current anchor becomes a link that holds\n        # the newest key, and the oldest link becomes the new anchor\n        # (invariant 1).  now the newest link comes before anchor\n        # (invariant 2).  no links are moved; only their keys\n        # and values are changed.\n        oldanchor = self._anchor\n        oldanchor[KEY] = key\n        oldanchor[VALUE] = value\n\n        self._anchor = anchor = oldanchor[NEXT]\n        evicted = anchor[KEY]\n        anchor[KEY] = anchor[VALUE] = _MISSING\n        del self._link_lookup[evicted]\n        self._link_lookup[key] = oldanchor\n        return evicted\n\n    def _remove_from_ll(self, key):\n        # splice a link out of the list and drop it from our lookup\n        # table.\n        link = self._link_lookup.pop(key)\n        link[PREV][NEXT] = link[NEXT]\n        link[NEXT][PREV] = link[PREV]\n\n    def __setitem__(self, key, value):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if len(self) < self.max_size:\n                    self._set_key_and_add_to_front_of_ll(key, value)\n                else:\n                    evicted = self._set_key_and_evict_last_in_ll(key, value)\n                    super(LRI, self).__delitem__(evicted)\n                super(LRI, self).__setitem__(key, value)\n            else:\n                link[VALUE] = value\n\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            return default\n\n    def __delitem__(self, key):\n        with self._lock:\n            super(LRI, self).__delitem__(key)\n            self._remove_from_ll(key)\n\n###The function: pop###\n    def popitem(self):\n        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item\n\n    def clear(self):\n        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n\n    def copy(self):\n        return self.__class__(max_size=self.max_size, values=self)\n\n    def setdefault(self, key, default=None):\n        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n\n    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__\n        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return\n\n    def __eq__(self, other):\n        with self._lock:\n            if self is other:\n                return True\n            if len(other) != len(self):\n                return False\n            if not isinstance(other, LRI):\n                return other == self\n            return super(LRI, self).__eq__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))\n\n\nclass LRU(LRI):\n    \"\"\"The ``LRU`` is :class:`dict` subtype implementation of the\n    *Least-Recently Used* caching strategy.\n\n    Args:\n        max_size (int): Max number of items to cache. Defaults to ``128``.\n        values (iterable): Initial values for the cache. Defaults to ``None``.\n        on_miss (callable): a callable which accepts a single argument, the\n            key not present in the cache, and returns the value to be cached.\n\n    >>> cap_cache = LRU(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n\n    This cache is also instrumented with statistics\n    collection. ``hit_count``, ``miss_count``, and ``soft_miss_count``\n    are all integer members that can be used to introspect the\n    performance of the cache. (\"Soft\" misses are misses that did not\n    raise :exc:`KeyError`, e.g., ``LRU.get()`` or ``on_miss`` was used to\n    cache a default.\n\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n\n    Other than the size-limiting caching behavior and statistics,\n    ``LRU`` acts like its parent class, the built-in Python :class:`dict`.\n    \"\"\"\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n\n### Cached decorator\n# Key-making technique adapted from Python 3.4's functools\n\nclass _HashedKey(list):\n    \"\"\"The _HashedKey guarantees that hash() will be called no more than once\n    per cached function invocation.\n    \"\"\"\n    __slots__ = 'hash_value'\n\n    def __init__(self, key):\n        self[:] = key\n        self.hash_value = hash(tuple(key))\n\n    def __hash__(self):\n        return self.hash_value\n\n    def __repr__(self):\n        return '%s(%s)' % (self.__class__.__name__, list.__repr__(self))\n\n\ndef make_cache_key(args, kwargs, typed=False,\n                   kwarg_mark=_KWARG_MARK,\n                   fasttypes=frozenset([int, str, frozenset, type(None)])):\n    \"\"\"Make a generic key from a function's positional and keyword\n    arguments, suitable for use in caches. Arguments within *args* and\n    *kwargs* must be `hashable`_. If *typed* is ``True``, ``3`` and\n    ``3.0`` will be treated as separate keys.\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    >>> tuple(make_cache_key(('a', 'b'), {'c': ('d')}))\n    ('a', 'b', _KWARG_MARK, ('c', 'd'))\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n    \"\"\"\n\n    # key = [func_name] if func_name else []\n    # key.extend(args)\n    key = list(args)\n    if kwargs:\n        sorted_items = sorted(kwargs.items())\n        key.append(kwarg_mark)\n        key.extend(sorted_items)\n    if typed:\n        key.extend([type(v) for v in args])\n        if kwargs:\n            key.extend([type(v) for k, v in sorted_items])\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedKey(key)\n\n# for backwards compatibility in case someone was importing it\n_make_cache_key = make_cache_key\n\n\nclass CachedFunction(object):\n    \"\"\"This type is used by :func:`cached`, below. Instances of this\n    class are used to wrap functions in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        if callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be a dict-like object,'\n                            ' or callable returning a dict-like object, not %r'\n                            % cache)\n        else:\n            def _get_cache():\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n\n    def __call__(self, *args, **kwargs):\n        cache = self.get_cache()\n        key = self.key_func(args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)\n\n\nclass CachedMethod(object):\n    \"\"\"Similar to :class:`CachedFunction`, this type is used by\n    :func:`cachedmethod` to wrap methods in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        if isinstance(cache, basestring):\n            self.get_cache = attrgetter(cache)\n        elif callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be an attribute name,'\n                            ' dict-like object, or callable returning'\n                            ' a dict-like object, not %r' % cache)\n        else:\n            def _get_cache(obj):\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n        self.bound_to = None\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        cls = self.__class__\n        ret = cls(self.func, self.get_cache, typed=self.typed,\n                  scoped=self.scoped, key=self.key_func)\n        ret.bound_to = obj\n        return ret\n\n    def __call__(self, *args, **kwargs):\n        obj = args[0] if self.bound_to is None else self.bound_to\n        cache = self.get_cache(obj)\n        key_args = (self.bound_to, self.func) + args if self.scoped else args\n        key = self.key_func(key_args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            if self.bound_to is not None:\n                args = (self.bound_to,) + args\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        args = (cn, self.func, self.scoped, self.typed)\n        if self.bound_to is not None:\n            args += (self.bound_to,)\n            return ('<%s func=%r scoped=%r typed=%r bound_to=%r>' % args)\n        return (\"%s(func=%r, scoped=%r, typed=%r)\" % args)\n\n\ndef cached(cache, scoped=True, typed=False, key=None):\n    \"\"\"Cache any function with the cache object of your choosing. Note\n    that the function wrapped should take only `hashable`_ arguments.\n\n    Args:\n        cache (Mapping): Any :class:`dict`-like object suitable for\n            use as a cache. Instances of the :class:`LRU` and\n            :class:`LRI` are good choices, but a plain :class:`dict`\n            can work in some cases, as well. This argument can also be\n            a callable which accepts no arguments and returns a mapping.\n        scoped (bool): Whether the function itself is part of the\n            cache key.  ``True`` by default, different functions will\n            not read one another's cache entries, but can evict one\n            another's results. ``False`` can be useful for certain\n            shared cache use cases. More advanced behavior can be\n            produced through the *key* argument.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n\n    >>> my_cache = LRU()\n    >>> @cached(my_cache)\n    ... def cached_lower(x):\n    ...     return x.lower()\n    ...\n    >>> cached_lower(\"CaChInG's FuN AgAiN!\")\n    \"caching's fun again!\"\n    >>> len(my_cache)\n    1\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n\n    \"\"\"\n    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator\n\n\ndef cachedmethod(cache, scoped=True, typed=False, key=None):\n    \"\"\"Similar to :func:`cached`, ``cachedmethod`` is used to cache\n    methods based on their arguments, using any :class:`dict`-like\n    *cache* object.\n\n    Args:\n        cache (str/Mapping/callable): Can be the name of an attribute\n            on the instance, any Mapping/:class:`dict`-like object, or\n            a callable which returns a Mapping.\n        scoped (bool): Whether the method itself and the object it is\n            bound to are part of the cache keys. ``True`` by default,\n            different methods will not read one another's cache\n            results. ``False`` can be useful for certain shared cache\n            use cases. More advanced behavior can be produced through\n            the *key* arguments.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n        key (callable): A callable with a signature that matches\n            :func:`make_cache_key` that returns a tuple of hashable\n            values to be used as the key in the cache.\n\n    >>> class Lowerer(object):\n    ...     def __init__(self):\n    ...         self.cache = LRI()\n    ...\n    ...     @cachedmethod('cache')\n    ...     def lower(self, text):\n    ...         return text.lower()\n    ...\n    >>> lowerer = Lowerer()\n    >>> lowerer.lower('WOW WHO COULD GUESS CACHING COULD BE SO NEAT')\n    'wow who could guess caching could be so neat'\n    >>> len(lowerer.cache)\n    1\n\n    \"\"\"\n    def cached_method_decorator(func):\n        return CachedMethod(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_method_decorator\n\n\nclass cachedproperty(object):\n    \"\"\"The ``cachedproperty`` is used similar to :class:`property`, except\n    that the wrapped method is only called once. This is commonly used\n    to implement lazy attributes.\n\n    After the property has been accessed, the value is stored on the\n    instance itself, using the same name as the cachedproperty. This\n    allows the cache to be cleared with :func:`delattr`, or through\n    manipulating the object's ``__dict__``.\n    \"\"\"\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        self.func = func\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        return '<%s func=%s>' % (cn, self.func)\n\n\nclass ThresholdCounter(object):\n    \"\"\"A **bounded** dict-like Mapping from keys to counts. The\n    ThresholdCounter automatically compacts after every (1 /\n    *threshold*) additions, maintaining exact counts for any keys\n    whose count represents at least a *threshold* ratio of the total\n    data. In other words, if a particular key is not present in the\n    ThresholdCounter, its count represents less than *threshold* of\n    the total data.\n\n    >>> tc = ThresholdCounter(threshold=0.1)\n    >>> tc.add(1)\n    >>> tc.items()\n    [(1, 1)]\n    >>> tc.update([2] * 10)\n    >>> tc.get(1)\n    0\n    >>> tc.add(5)\n    >>> 5 in tc\n    True\n    >>> len(list(tc.elements()))\n    11\n\n    As you can see above, the API is kept similar to\n    :class:`collections.Counter`. The most notable feature omissions\n    being that counted items cannot be set directly, uncounted, or\n    removed, as this would disrupt the math.\n\n    Use the ThresholdCounter when you need best-effort long-lived\n    counts for dynamically-keyed data. Without a bounded datastructure\n    such as this one, the dynamic keys often represent a memory leak\n    and can impact application reliability. The ThresholdCounter's\n    item replacement strategy is fully deterministic and can be\n    thought of as *Amortized Least Relevant*. The absolute upper bound\n    of keys it will store is *(2/threshold)*, but realistically\n    *(1/threshold)* is expected for uniformly random datastreams, and\n    one or two orders of magnitude better for real-world data.\n\n    This algorithm is an implementation of the Lossy Counting\n    algorithm described in \"Approximate Frequency Counts over Data\n    Streams\" by Manku & Motwani. Hat tip to Kurt Rose for discovery\n    and initial implementation.\n\n    \"\"\"\n    # TODO: hit_count/miss_count?\n    def __init__(self, threshold=0.001):\n        if not 0 < threshold < 1:\n            raise ValueError('expected threshold between 0 and 1, not: %r'\n                             % threshold)\n\n        self.total = 0\n        self._count_map = {}\n        self._threshold = threshold\n        self._thresh_count = int(1 / threshold)\n        self._cur_bucket = 1\n\n    @property\n    def threshold(self):\n        return self._threshold\n\n    def add(self, key):\n        \"\"\"Increment the count of *key* by 1, automatically adding it if it\n        does not exist.\n\n        Cache compaction is triggered every *1/threshold* additions.\n        \"\"\"\n        self.total += 1\n        try:\n            self._count_map[key][0] += 1\n        except KeyError:\n            self._count_map[key] = [1, self._cur_bucket - 1]\n\n        if self.total % self._thresh_count == 0:\n            self._count_map = dict([(k, v) for k, v in self._count_map.items()\n                                    if sum(v) > self._cur_bucket])\n            self._cur_bucket += 1\n        return\n\n    def elements(self):\n        \"\"\"Return an iterator of all the common elements tracked by the\n        counter. Yields each key as many times as it has been seen.\n        \"\"\"\n        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)\n\n    def most_common(self, n=None):\n        \"\"\"Get the top *n* keys and counts as tuples. If *n* is omitted,\n        returns all the pairs.\n        \"\"\"\n        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]\n\n    def get_common_count(self):\n        \"\"\"Get the sum of counts for keys exceeding the configured data\n        threshold.\n        \"\"\"\n        return sum([count for count, _ in self._count_map.values()])\n\n    def get_uncommon_count(self):\n        \"\"\"Get the sum of counts for keys that were culled because the\n        associated counts represented less than the configured\n        threshold. The long-tail counts.\n        \"\"\"\n        return self.total - self.get_common_count()\n\n    def get_commonality(self):\n        \"\"\"Get a float representation of the effective count accuracy. The\n        higher the number, the less uniform the keys being added, and\n        the higher accuracy and efficiency of the ThresholdCounter.\n\n        If a stronger measure of data cardinality is required,\n        consider using hyperloglog.\n        \"\"\"\n        return float(self.get_common_count()) / self.total\n\n    def __getitem__(self, key):\n        return self._count_map[key][0]\n\n    def __len__(self):\n        return len(self._count_map)\n\n    def __contains__(self, key):\n        return key in self._count_map\n\n    def iterkeys(self):\n        return iter(self._count_map)\n\n    def keys(self):\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield count_map[k][0]\n\n    def values(self):\n        return list(self.itervalues())\n\n    def iteritems(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield (k, count_map[k][0])\n\n    def items(self):\n        return list(self.iteritems())\n\n    def get(self, key, default=0):\n        \"Get count for *key*, defaulting to 0.\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def update(self, iterable, **kwargs):\n        \"\"\"Like dict.update() but add counts instead of replacing them, used\n        to add multiple items in one call.\n\n        Source can be an iterable of keys to add, or a mapping of keys\n        to integer counts.\n        \"\"\"\n        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)\n\n\nclass MinIDMap(object):\n    \"\"\"\n    Assigns arbitrary weakref-able objects the smallest possible unique\n    integer IDs, such that no two objects have the same ID at the same\n    time.\n\n    Maps arbitrary hashable objects to IDs.\n\n    Based on https://gist.github.com/kurtbrose/25b48114de216a5e55df\n    \"\"\"\n    def __init__(self):\n        self.mapping = weakref.WeakKeyDictionary()\n        self.ref_map = {}\n        self.free = []\n\n    def get(self, a):\n        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt\n\n    def drop(self, a):\n        freed, ref = self.mapping[a]\n        del self.mapping[a]\n        del self.ref_map[ref]\n        heapq.heappush(self.free, freed)\n\n    def _clean(self, ref):\n        print(self.ref_map[ref])\n        heapq.heappush(self.free, self.ref_map[ref])\n        del self.ref_map[ref]\n\n    def __contains__(self, a):\n        return a in self.mapping\n\n    def __iter__(self):\n        return iter(self.mapping)\n\n    def __len__(self):\n        return self.mapping.__len__()\n\n    def iteritems(self):\n        return iter((k, self.mapping[k][0]) for k in iter(self.mapping))\n\n\n# end cacheutils.py\n", "prompt": "Please write a python function called 'pop' base the context. Pop the key in the LRI instance and return the corresponding value. If the key is not found and the default value is not passed, the exception is re-raised. This function bypasses the hit count and miss count.\n:param self: LRI, an instance of the LRI class.\n:param key: The key to remove in the instance.\n:param default: The value to return if the key is not found in the instance. Defaults to _UNSET.\n:return: The value corresponding to the key.\n.\n        The context you need to refer to is as follows: # -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"``cacheutils`` contains consistent implementations of fundamental\ncache types. Currently there are two to choose from:\n\n  * :class:`LRI` - Least-recently inserted\n  * :class:`LRU` - Least-recently used\n\nBoth caches are :class:`dict` subtypes, designed to be as\ninterchangeable as possible, to facilitate experimentation. A key\npractice with performance enhancement with caching is ensuring that\nthe caching strategy is working. If the cache is constantly missing,\nit is just adding more overhead and code complexity. The standard\nstatistics are:\n\n  * ``hit_count`` - the number of times the queried key has been in\n    the cache\n  * ``miss_count`` - the number of times a key has been absent and/or\n    fetched by the cache\n  * ``soft_miss_count`` - the number of times a key has been absent,\n    but a default has been provided by the caller, as with\n    :meth:`dict.get` and :meth:`dict.setdefault`. Soft misses are a\n    subset of misses, so this number is always less than or equal to\n    ``miss_count``.\n\nAdditionally, ``cacheutils`` provides :class:`ThresholdCounter`, a\ncache-like bounded counter useful for online statistics collection.\n\nLearn more about `caching algorithms on Wikipedia\n<https://en.wikipedia.org/wiki/Cache_algorithms#Examples>`_.\n\n\"\"\"\n\n# TODO: TimedLRI\n# TODO: support 0 max_size?\n\n\nimport heapq\nimport weakref\nimport itertools\nfrom operator import attrgetter\n\ntry:\n    from threading import RLock\nexcept Exception:\n    class RLock(object):\n        'Dummy reentrant lock for builds without threads'\n        def __enter__(self):\n            pass\n\n        def __exit__(self, exctype, excinst, exctb):\n            pass\n\ntry:\n    from .typeutils import make_sentinel\n    _MISSING = make_sentinel(var_name='_MISSING')\n    _KWARG_MARK = make_sentinel(var_name='_KWARG_MARK')\nexcept ImportError:\n    _MISSING = object()\n    _KWARG_MARK = object()\n\ntry:\n    xrange\nexcept NameError:\n    # py3\n    xrange = range\n    unicode, str, bytes, basestring = str, bytes, bytes, (str, bytes)\n\nPREV, NEXT, KEY, VALUE = range(4)   # names for the link fields\nDEFAULT_MAX_SIZE = 128\n\n\nclass LRI(dict):\n    \"\"\"The ``LRI`` implements the basic *Least Recently Inserted* strategy to\n    caching. One could also think of this as a ``SizeLimitedDefaultDict``.\n\n    *on_miss* is a callable that accepts the missing key (as opposed\n    to :class:`collections.defaultdict`'s \"default_factory\", which\n    accepts no arguments.) Also note that, like the :class:`LRI`,\n    the ``LRI`` is instrumented with statistics tracking.\n\n    >>> cap_cache = LRI(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n    \"\"\"\n    def __init__(self, max_size=DEFAULT_MAX_SIZE, values=None,\n                 on_miss=None):\n        if max_size <= 0:\n            raise ValueError('expected max_size > 0, not %r' % max_size)\n        self.hit_count = self.miss_count = self.soft_miss_count = 0\n        self.max_size = max_size\n        self._lock = RLock()\n        self._init_ll()\n\n        if on_miss is not None and not callable(on_miss):\n            raise TypeError('expected on_miss to be a callable'\n                            ' (or None), not %r' % on_miss)\n        self.on_miss = on_miss\n\n        if values:\n            self.update(values)\n\n    # TODO: fromkeys()?\n\n    # linked list manipulation methods.\n    #\n    # invariants:\n    # 1) 'anchor' is the sentinel node in the doubly linked list.  there is\n    #    always only one, and its KEY and VALUE are both _MISSING.\n    # 2) the most recently accessed node comes immediately before 'anchor'.\n    # 3) the least recently accessed node comes immediately after 'anchor'.\n    def _init_ll(self):\n        anchor = []\n        anchor[:] = [anchor, anchor, _MISSING, _MISSING]\n        # a link lookup table for finding linked list links in O(1)\n        # time.\n        self._link_lookup = {}\n        self._anchor = anchor\n\n    def _print_ll(self):\n        print('***')\n        for (key, val) in self._get_flattened_ll():\n            print(key, val)\n        print('***')\n        return\n\n    def _get_flattened_ll(self):\n        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list\n\n    def _get_link_and_move_to_front_of_ll(self, key):\n        # find what will become the newest link. this may raise a\n        # KeyError, which is useful to __getitem__ and __setitem__\n        newest = self._link_lookup[key]\n\n        # splice out what will become the newest link.\n        newest[PREV][NEXT] = newest[NEXT]\n        newest[NEXT][PREV] = newest[PREV]\n\n        # move what will become the newest link immediately before\n        # anchor (invariant 2)\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        second_newest[NEXT] = anchor[PREV] = newest\n        newest[PREV] = second_newest\n        newest[NEXT] = anchor\n        return newest\n\n    def _set_key_and_add_to_front_of_ll(self, key, value):\n        # create a new link and place it immediately before anchor\n        # (invariant 2).\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        newest = [second_newest, anchor, key, value]\n        second_newest[NEXT] = anchor[PREV] = newest\n        self._link_lookup[key] = newest\n\n    def _set_key_and_evict_last_in_ll(self, key, value):\n        # the link after anchor is the oldest in the linked list\n        # (invariant 3).  the current anchor becomes a link that holds\n        # the newest key, and the oldest link becomes the new anchor\n        # (invariant 1).  now the newest link comes before anchor\n        # (invariant 2).  no links are moved; only their keys\n        # and values are changed.\n        oldanchor = self._anchor\n        oldanchor[KEY] = key\n        oldanchor[VALUE] = value\n\n        self._anchor = anchor = oldanchor[NEXT]\n        evicted = anchor[KEY]\n        anchor[KEY] = anchor[VALUE] = _MISSING\n        del self._link_lookup[evicted]\n        self._link_lookup[key] = oldanchor\n        return evicted\n\n    def _remove_from_ll(self, key):\n        # splice a link out of the list and drop it from our lookup\n        # table.\n        link = self._link_lookup.pop(key)\n        link[PREV][NEXT] = link[NEXT]\n        link[NEXT][PREV] = link[PREV]\n\n    def __setitem__(self, key, value):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if len(self) < self.max_size:\n                    self._set_key_and_add_to_front_of_ll(key, value)\n                else:\n                    evicted = self._set_key_and_evict_last_in_ll(key, value)\n                    super(LRI, self).__delitem__(evicted)\n                super(LRI, self).__setitem__(key, value)\n            else:\n                link[VALUE] = value\n\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            return default\n\n    def __delitem__(self, key):\n        with self._lock:\n            super(LRI, self).__delitem__(key)\n            self._remove_from_ll(key)\n\n###The function: pop###\n    def popitem(self):\n        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item\n\n    def clear(self):\n        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n\n    def copy(self):\n        return self.__class__(max_size=self.max_size, values=self)\n\n    def setdefault(self, key, default=None):\n        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n\n    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__\n        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return\n\n    def __eq__(self, other):\n        with self._lock:\n            if self is other:\n                return True\n            if len(other) != len(self):\n                return False\n            if not isinstance(other, LRI):\n                return other == self\n            return super(LRI, self).__eq__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))\n\n\nclass LRU(LRI):\n    \"\"\"The ``LRU`` is :class:`dict` subtype implementation of the\n    *Least-Recently Used* caching strategy.\n\n    Args:\n        max_size (int): Max number of items to cache. Defaults to ``128``.\n        values (iterable): Initial values for the cache. Defaults to ``None``.\n        on_miss (callable): a callable which accepts a single argument, the\n            key not present in the cache, and returns the value to be cached.\n\n    >>> cap_cache = LRU(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n\n    This cache is also instrumented with statistics\n    collection. ``hit_count``, ``miss_count``, and ``soft_miss_count``\n    are all integer members that can be used to introspect the\n    performance of the cache. (\"Soft\" misses are misses that did not\n    raise :exc:`KeyError`, e.g., ``LRU.get()`` or ``on_miss`` was used to\n    cache a default.\n\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n\n    Other than the size-limiting caching behavior and statistics,\n    ``LRU`` acts like its parent class, the built-in Python :class:`dict`.\n    \"\"\"\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n\n### Cached decorator\n# Key-making technique adapted from Python 3.4's functools\n\nclass _HashedKey(list):\n    \"\"\"The _HashedKey guarantees that hash() will be called no more than once\n    per cached function invocation.\n    \"\"\"\n    __slots__ = 'hash_value'\n\n    def __init__(self, key):\n        self[:] = key\n        self.hash_value = hash(tuple(key))\n\n    def __hash__(self):\n        return self.hash_value\n\n    def __repr__(self):\n        return '%s(%s)' % (self.__class__.__name__, list.__repr__(self))\n\n\ndef make_cache_key(args, kwargs, typed=False,\n                   kwarg_mark=_KWARG_MARK,\n                   fasttypes=frozenset([int, str, frozenset, type(None)])):\n    \"\"\"Make a generic key from a function's positional and keyword\n    arguments, suitable for use in caches. Arguments within *args* and\n    *kwargs* must be `hashable`_. If *typed* is ``True``, ``3`` and\n    ``3.0`` will be treated as separate keys.\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    >>> tuple(make_cache_key(('a', 'b'), {'c': ('d')}))\n    ('a', 'b', _KWARG_MARK, ('c', 'd'))\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n    \"\"\"\n\n    # key = [func_name] if func_name else []\n    # key.extend(args)\n    key = list(args)\n    if kwargs:\n        sorted_items = sorted(kwargs.items())\n        key.append(kwarg_mark)\n        key.extend(sorted_items)\n    if typed:\n        key.extend([type(v) for v in args])\n        if kwargs:\n            key.extend([type(v) for k, v in sorted_items])\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedKey(key)\n\n# for backwards compatibility in case someone was importing it\n_make_cache_key = make_cache_key\n\n\nclass CachedFunction(object):\n    \"\"\"This type is used by :func:`cached`, below. Instances of this\n    class are used to wrap functions in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        if callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be a dict-like object,'\n                            ' or callable returning a dict-like object, not %r'\n                            % cache)\n        else:\n            def _get_cache():\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n\n    def __call__(self, *args, **kwargs):\n        cache = self.get_cache()\n        key = self.key_func(args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)\n\n\nclass CachedMethod(object):\n    \"\"\"Similar to :class:`CachedFunction`, this type is used by\n    :func:`cachedmethod` to wrap methods in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        if isinstance(cache, basestring):\n            self.get_cache = attrgetter(cache)\n        elif callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be an attribute name,'\n                            ' dict-like object, or callable returning'\n                            ' a dict-like object, not %r' % cache)\n        else:\n            def _get_cache(obj):\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n        self.bound_to = None\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        cls = self.__class__\n        ret = cls(self.func, self.get_cache, typed=self.typed,\n                  scoped=self.scoped, key=self.key_func)\n        ret.bound_to = obj\n        return ret\n\n    def __call__(self, *args, **kwargs):\n        obj = args[0] if self.bound_to is None else self.bound_to\n        cache = self.get_cache(obj)\n        key_args = (self.bound_to, self.func) + args if self.scoped else args\n        key = self.key_func(key_args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            if self.bound_to is not None:\n                args = (self.bound_to,) + args\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        args = (cn, self.func, self.scoped, self.typed)\n        if self.bound_to is not None:\n            args += (self.bound_to,)\n            return ('<%s func=%r scoped=%r typed=%r bound_to=%r>' % args)\n        return (\"%s(func=%r, scoped=%r, typed=%r)\" % args)\n\n\ndef cached(cache, scoped=True, typed=False, key=None):\n    \"\"\"Cache any function with the cache object of your choosing. Note\n    that the function wrapped should take only `hashable`_ arguments.\n\n    Args:\n        cache (Mapping): Any :class:`dict`-like object suitable for\n            use as a cache. Instances of the :class:`LRU` and\n            :class:`LRI` are good choices, but a plain :class:`dict`\n            can work in some cases, as well. This argument can also be\n            a callable which accepts no arguments and returns a mapping.\n        scoped (bool): Whether the function itself is part of the\n            cache key.  ``True`` by default, different functions will\n            not read one another's cache entries, but can evict one\n            another's results. ``False`` can be useful for certain\n            shared cache use cases. More advanced behavior can be\n            produced through the *key* argument.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n\n    >>> my_cache = LRU()\n    >>> @cached(my_cache)\n    ... def cached_lower(x):\n    ...     return x.lower()\n    ...\n    >>> cached_lower(\"CaChInG's FuN AgAiN!\")\n    \"caching's fun again!\"\n    >>> len(my_cache)\n    1\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n\n    \"\"\"\n    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator\n\n\ndef cachedmethod(cache, scoped=True, typed=False, key=None):\n    \"\"\"Similar to :func:`cached`, ``cachedmethod`` is used to cache\n    methods based on their arguments, using any :class:`dict`-like\n    *cache* object.\n\n    Args:\n        cache (str/Mapping/callable): Can be the name of an attribute\n            on the instance, any Mapping/:class:`dict`-like object, or\n            a callable which returns a Mapping.\n        scoped (bool): Whether the method itself and the object it is\n            bound to are part of the cache keys. ``True`` by default,\n            different methods will not read one another's cache\n            results. ``False`` can be useful for certain shared cache\n            use cases. More advanced behavior can be produced through\n            the *key* arguments.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n        key (callable): A callable with a signature that matches\n            :func:`make_cache_key` that returns a tuple of hashable\n            values to be used as the key in the cache.\n\n    >>> class Lowerer(object):\n    ...     def __init__(self):\n    ...         self.cache = LRI()\n    ...\n    ...     @cachedmethod('cache')\n    ...     def lower(self, text):\n    ...         return text.lower()\n    ...\n    >>> lowerer = Lowerer()\n    >>> lowerer.lower('WOW WHO COULD GUESS CACHING COULD BE SO NEAT')\n    'wow who could guess caching could be so neat'\n    >>> len(lowerer.cache)\n    1\n\n    \"\"\"\n    def cached_method_decorator(func):\n        return CachedMethod(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_method_decorator\n\n\nclass cachedproperty(object):\n    \"\"\"The ``cachedproperty`` is used similar to :class:`property`, except\n    that the wrapped method is only called once. This is commonly used\n    to implement lazy attributes.\n\n    After the property has been accessed, the value is stored on the\n    instance itself, using the same name as the cachedproperty. This\n    allows the cache to be cleared with :func:`delattr`, or through\n    manipulating the object's ``__dict__``.\n    \"\"\"\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        self.func = func\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        return '<%s func=%s>' % (cn, self.func)\n\n\nclass ThresholdCounter(object):\n    \"\"\"A **bounded** dict-like Mapping from keys to counts. The\n    ThresholdCounter automatically compacts after every (1 /\n    *threshold*) additions, maintaining exact counts for any keys\n    whose count represents at least a *threshold* ratio of the total\n    data. In other words, if a particular key is not present in the\n    ThresholdCounter, its count represents less than *threshold* of\n    the total data.\n\n    >>> tc = ThresholdCounter(threshold=0.1)\n    >>> tc.add(1)\n    >>> tc.items()\n    [(1, 1)]\n    >>> tc.update([2] * 10)\n    >>> tc.get(1)\n    0\n    >>> tc.add(5)\n    >>> 5 in tc\n    True\n    >>> len(list(tc.elements()))\n    11\n\n    As you can see above, the API is kept similar to\n    :class:`collections.Counter`. The most notable feature omissions\n    being that counted items cannot be set directly, uncounted, or\n    removed, as this would disrupt the math.\n\n    Use the ThresholdCounter when you need best-effort long-lived\n    counts for dynamically-keyed data. Without a bounded datastructure\n    such as this one, the dynamic keys often represent a memory leak\n    and can impact application reliability. The ThresholdCounter's\n    item replacement strategy is fully deterministic and can be\n    thought of as *Amortized Least Relevant*. The absolute upper bound\n    of keys it will store is *(2/threshold)*, but realistically\n    *(1/threshold)* is expected for uniformly random datastreams, and\n    one or two orders of magnitude better for real-world data.\n\n    This algorithm is an implementation of the Lossy Counting\n    algorithm described in \"Approximate Frequency Counts over Data\n    Streams\" by Manku & Motwani. Hat tip to Kurt Rose for discovery\n    and initial implementation.\n\n    \"\"\"\n    # TODO: hit_count/miss_count?\n    def __init__(self, threshold=0.001):\n        if not 0 < threshold < 1:\n            raise ValueError('expected threshold between 0 and 1, not: %r'\n                             % threshold)\n\n        self.total = 0\n        self._count_map = {}\n        self._threshold = threshold\n        self._thresh_count = int(1 / threshold)\n        self._cur_bucket = 1\n\n    @property\n    def threshold(self):\n        return self._threshold\n\n    def add(self, key):\n        \"\"\"Increment the count of *key* by 1, automatically adding it if it\n        does not exist.\n\n        Cache compaction is triggered every *1/threshold* additions.\n        \"\"\"\n        self.total += 1\n        try:\n            self._count_map[key][0] += 1\n        except KeyError:\n            self._count_map[key] = [1, self._cur_bucket - 1]\n\n        if self.total % self._thresh_count == 0:\n            self._count_map = dict([(k, v) for k, v in self._count_map.items()\n                                    if sum(v) > self._cur_bucket])\n            self._cur_bucket += 1\n        return\n\n    def elements(self):\n        \"\"\"Return an iterator of all the common elements tracked by the\n        counter. Yields each key as many times as it has been seen.\n        \"\"\"\n        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)\n\n    def most_common(self, n=None):\n        \"\"\"Get the top *n* keys and counts as tuples. If *n* is omitted,\n        returns all the pairs.\n        \"\"\"\n        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]\n\n    def get_common_count(self):\n        \"\"\"Get the sum of counts for keys exceeding the configured data\n        threshold.\n        \"\"\"\n        return sum([count for count, _ in self._count_map.values()])\n\n    def get_uncommon_count(self):\n        \"\"\"Get the sum of counts for keys that were culled because the\n        associated counts represented less than the configured\n        threshold. The long-tail counts.\n        \"\"\"\n        return self.total - self.get_common_count()\n\n    def get_commonality(self):\n        \"\"\"Get a float representation of the effective count accuracy. The\n        higher the number, the less uniform the keys being added, and\n        the higher accuracy and efficiency of the ThresholdCounter.\n\n        If a stronger measure of data cardinality is required,\n        consider using hyperloglog.\n        \"\"\"\n        return float(self.get_common_count()) / self.total\n\n    def __getitem__(self, key):\n        return self._count_map[key][0]\n\n    def __len__(self):\n        return len(self._count_map)\n\n    def __contains__(self, key):\n        return key in self._count_map\n\n    def iterkeys(self):\n        return iter(self._count_map)\n\n    def keys(self):\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield count_map[k][0]\n\n    def values(self):\n        return list(self.itervalues())\n\n    def iteritems(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield (k, count_map[k][0])\n\n    def items(self):\n        return list(self.iteritems())\n\n    def get(self, key, default=0):\n        \"Get count for *key*, defaulting to 0.\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def update(self, iterable, **kwargs):\n        \"\"\"Like dict.update() but add counts instead of replacing them, used\n        to add multiple items in one call.\n\n        Source can be an iterable of keys to add, or a mapping of keys\n        to integer counts.\n        \"\"\"\n        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)\n\n\nclass MinIDMap(object):\n    \"\"\"\n    Assigns arbitrary weakref-able objects the smallest possible unique\n    integer IDs, such that no two objects have the same ID at the same\n    time.\n\n    Maps arbitrary hashable objects to IDs.\n\n    Based on https://gist.github.com/kurtbrose/25b48114de216a5e55df\n    \"\"\"\n    def __init__(self):\n        self.mapping = weakref.WeakKeyDictionary()\n        self.ref_map = {}\n        self.free = []\n\n    def get(self, a):\n        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt\n\n    def drop(self, a):\n        freed, ref = self.mapping[a]\n        del self.mapping[a]\n        del self.ref_map[ref]\n        heapq.heappush(self.free, freed)\n\n    def _clean(self, ref):\n        print(self.ref_map[ref])\n        heapq.heappush(self.free, self.ref_map[ref])\n        del self.ref_map[ref]\n\n    def __contains__(self, a):\n        return a in self.mapping\n\n    def __iter__(self):\n        return iter(self.mapping)\n\n    def __len__(self):\n        return self.mapping.__len__()\n\n    def iteritems(self):\n        return iter((k, self.mapping[k][0]) for k in iter(self.mapping))\n\n\n# end cacheutils.py\n", "test_list": ["def test_lru_basic():\n    lru = LRU(max_size=1)\n    repr(lru)\n    lru['hi'] = 0\n    lru['bye'] = 1\n    assert len(lru) == 1\n    lru['bye']\n    assert lru.get('hi') is None\n    del lru['bye']\n    assert 'bye' not in lru\n    assert len(lru) == 0\n    assert not lru\n    try:\n        lru.pop('bye')\n    except KeyError:\n        pass\n    else:\n        assert False\n    default = object()\n    assert lru.pop('bye', default) is default\n    try:\n        lru.popitem()\n    except KeyError:\n        pass\n    else:\n        assert False\n    lru['another'] = 1\n    assert lru.popitem() == ('another', 1)\n    lru['yet_another'] = 2\n    assert lru.pop('yet_another') == 2\n    lru['yet_another'] = 3\n    assert lru.pop('yet_another', default) == 3\n    lru['yet_another'] = 4\n    lru.clear()\n    assert not lru\n    lru['yet_another'] = 5\n    second_lru = LRU(max_size=1)\n    assert lru.copy() == lru\n    second_lru['yet_another'] = 5\n    assert second_lru == lru\n    assert lru == second_lru\n    lru.update(LRU(max_size=2, values=[('a', 1), ('b', 2)]))\n    assert len(lru) == 1\n    assert 'yet_another' not in lru\n    lru.setdefault('x', 2)\n    assert dict(lru) == {'x': 2}\n    lru.setdefault('x', 3)\n    assert dict(lru) == {'x': 2}\n    assert lru != second_lru\n    assert second_lru != lru"], "requirements": {"Input-Output Conditions": {"requirement": "The 'pop' function should accept a key and an optional default value, returning the value associated with the key if it exists, or the default value if the key is not found.", "unit_test": "def test_pop_with_default():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    assert lri.pop('key1') == 'value1'\n    default = 'default_value'\n    assert lri.pop('non_existent_key', default) == default", "test": "tests/test_cacheutils.py::test_pop_with_default"}, "Exception Handling": {"requirement": "The 'pop' function should raise a KeyError if the key is not found and no default value is provided.", "unit_test": "def test_pop_raises_keyerror():\n    lri = LRI(max_size=2)\n    try:\n        lri.pop('non_existent_key')\n    except KeyError:\n        pass\n    else:\n        assert False, 'Expected KeyError not raised'", "test": "tests/test_cacheutils.py::test_pop_raises_keyerror"}, "Edge Case Handling": {"requirement": "The 'pop' function should handle the case where the cache is empty and a key is requested.", "unit_test": "def test_pop_empty_cache():\n    lri = LRI(max_size=2)\n    default = 'default_value'\n    assert lri.pop('any_key', default) == default", "test": "tests/test_cacheutils.py::test_pop_empty_cache"}, "Functionality Extension": {"requirement": "Extend the 'pop' function to log a message whenever a key is successfully removed from the cache.", "unit_test": "def test_pop_logs_message():\n    import logging\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    with self.assertLogs(level='INFO') as log:\n        lri.pop('key1')\n    assert any('key1 removed' in message for message in log.output)", "test": "tests/test_cacheutils.py::test_pop_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the 'pop' function has complete type annotations for all parameters and return types.", "unit_test": "def test_pop_annotations():\n    from typing import get_type_hints\n    hints = get_type_hints(LRI.pop)\n    assert hints['key'] == str\n    assert hints['default'] == object\n    assert hints['return'] == object", "test": "tests/test_cacheutils.py::test_pop_annotations"}, "Code Complexity": {"requirement": "Ensure that the 'pop' function maintains a cyclomatic complexity of 5 or less.", "unit_test": "def test_pop_cyclomatic_complexity():\n    import radon.complexity as rc\n    code = '''def pop(self, key, default=_UNSET): ... '''\n    complexity = rc.cc_visit_ast(rc.parse(code))\n    assert complexity[0].complexity <= 3", "test": "tests/test_cacheutils.py::test_pop_cyclomatic_complexity"}, "Code Standard": {"requirement": "Ensure that the 'pop' function adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_pop_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_pop_function.py'])\n    assert result.total_errors == 0, 'PEP 8 violations found'", "test": "tests/test_cacheutils.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'pop' function uses the '_lock' attribute to ensure thread safety.", "unit_test": "def test_pop_uses_lock():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    with lri._lock:\n        assert lri.pop('key1') == 'value1'", "test": "tests/test_cacheutils.py::test_pop_uses_lock"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'pop' function correctly uses the '_remove_from_ll' method to maintain the linked list structure.", "unit_test": "def test_pop_correct_ll_removal():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    lri.pop('key1')\n    assert 'key1' not in lri._link_lookup", "test": "tests/test_cacheutils.py::test_pop_correct_ll_removal"}}}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "type": "method", "project_path": "Communications/IMAPClient", "completion_path": "Communications/IMAPClient/imapclient/imapclient.py", "signature_position": [1488, 1488], "body_position": [1521, 1528], "dependency": {"intra_class": ["imapclient.imapclient.IMAPClient._command_and_check", "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "imapclient.imapclient.IMAPClient._imap", "imapclient.imapclient.IMAPClient.use_uid"], "intra_file": ["imapclient.imapclient.join_message_ids"], "cross_file": []}, "requirement": {"Functionality": "This function is used to expunge messages from the selected folder in an IMAP client. If no messages are specified, it removes all messages with the \"\\Deleted\" flag set. If messages are specified, it removes the specified messages with the \"\\Deleted\" flag set. The function returns the server response message followed by a list of expunge responses. The implementation takes into account whether the client is using UIDs or not.", "Arguments": ":param self: IMAPClient. An instance of the IMAPClient class.\n:param messages: List of int or str. The messages to be expunged. Defaults to None.\n:return: Tuple. The server response message followed by a list of expunge responses if no messages are specified. None if messages are specified."}, "tests": ["tests/test_imapclient.py::TestExpunge::test_expunge", "tests/test_imapclient.py::TestExpunge::test_id_expunge"], "indent": 4, "domain": "Communications", "code": "    def expunge(self, messages=None):\n        \"\"\"Use of the *messages* argument is discouraged.\n        Please see the ``uid_expunge`` method instead.\n\n        When, no *messages* are specified, remove all messages\n        from the currently selected folder that have the\n        ``\\\\Deleted`` flag set.\n\n        The return value is the server response message\n        followed by a list of expunge responses. For example::\n\n            ('Expunge completed.',\n             [(2, 'EXPUNGE'),\n              (1, 'EXPUNGE'),\n              (0, 'RECENT')])\n\n        In this case, the responses indicate that the message with\n        sequence numbers 2 and 1 where deleted, leaving no recent\n        messages in the folder.\n\n        See :rfc:`3501#section-6.4.3` section 6.4.3 and\n        :rfc:`3501#section-7.4.1` section 7.4.1 for more details.\n\n        When *messages* are specified, remove the specified messages\n        from the selected folder, provided those messages also have\n        the ``\\\\Deleted`` flag set. The return value is ``None`` in\n        this case.\n\n        Expunging messages by id(s) requires that *use_uid* is\n        ``True`` for the client.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n", "context": "# Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of messages threads from the currently\n        selected folder which match *criteria*.\n\n        Each returned thread is a list of messages ids. An example\n        return value containing three message threads::\n\n            ((1, 2), (3,), (4, 5, 6))\n\n        The optional *algorithm* argument specifies the threading\n        algorithm to use.\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for more details.\n        \"\"\"\n        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n###The function: expunge###\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n", "prompt": "Please write a python function called 'expunge' base the context. This function is used to expunge messages from the selected folder in an IMAP client. If no messages are specified, it removes all messages with the \"\\Deleted\" flag set. If messages are specified, it removes the specified messages with the \"\\Deleted\" flag set. The function returns the server response message followed by a list of expunge responses. The implementation takes into account whether the client is using UIDs or not.:param self: IMAPClient. An instance of the IMAPClient class.\n:param messages: List of int or str. The messages to be expunged. Defaults to None.\n:return: Tuple. The server response message followed by a list of expunge responses if no messages are specified. None if messages are specified..\n        The context you need to refer to is as follows: # Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of messages threads from the currently\n        selected folder which match *criteria*.\n\n        Each returned thread is a list of messages ids. An example\n        return value containing three message threads::\n\n            ((1, 2), (3,), (4, 5, 6))\n\n        The optional *algorithm* argument specifies the threading\n        algorithm to use.\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for more details.\n        \"\"\"\n        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n###The function: expunge###\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n", "test_list": ["def test_expunge(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=sentinel.out)\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge()\n    mockCommand.assert_called_with('EXPUNGE')\n    mockConsume.assert_called_with(sentinel.tag, 'EXPUNGE')\n    self.assertEqual(sentinel.out, result)", "def test_id_expunge(self):\n    self.client._imap.uid.return_value = ('OK', [None])\n    self.assertEqual([None], self.client.expunge(['4', '5', '6']))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'expunge' function should accept a list of message IDs as input and return a tuple containing the server response message and a list of expunge responses. If no message IDs are provided, it should expunge all messages with the '\\Deleted' flag set.", "unit_test": "def test_expunge_with_messages(self):\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)\n\ndef test_expunge_without_messages(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge()\n    self.assertEqual(('OK', ['Expunged']), result)", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_messages"}, "Exception Handling": {"requirement": "The 'expunge' function should raise a ValueError if the input message IDs are not of type list, int, or str.", "unit_test": "def test_expunge_invalid_input(self):\n    with self.assertRaises(ValueError):\n        self.client.expunge(123.45)", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_invalid_input"}, "Edge Case Handling": {"requirement": "The 'expunge' function should handle the edge case where an empty list of message IDs is provided by expunging all messages with the '\\Deleted' flag set.", "unit_test": "def test_expunge_with_empty_list(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge([])\n    self.assertEqual(('OK', ['Expunged']), result)", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_empty_list"}, "Functionality Extension": {"requirement": "Extend the 'expunge' function to optionally log the number of messages expunged when a logger is provided.", "unit_test": "def test_expunge_with_logging(self):\n    logger = Mock()\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    self.client.expunge(['1', '2', '3'], logger=logger)\n    logger.info.assert_called_with('3 messages expunged.')", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_logging"}, "Annotation Coverage": {"requirement": "Ensure that the 'expunge' function has complete type annotations for all parameters and return types.", "unit_test": "def test_expunge_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(self.client.expunge)\n    self.assertEqual(hints['messages'], Optional[List[Union[int, str]]])\n    self.assertEqual(hints['return'], Optional[Tuple[str, List[str]]])", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_annotations"}, "Code Complexity": {"requirement": "The 'expunge' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": "def test_expunge_complexity(self):\n    from radon.complexity import cc_visit\n    with open('imapclient/imapclient.py', 'r') as f:\n        code = f.read()\n    complexity = [c for c in cc_visit(code) if c.name == 'expunge']\n    self.assertTrue(complexity and complexity[0].complexity <= 5)", "test": "tests/test_imapclient.py::TestExpunge::test_code_complexity"}, "Code Standard": {"requirement": "The 'expunge' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_expunge_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['imapclient/imapclient.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/test_imapclient.py::TestExpunge::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'expunge' function should utilize the '_command_and_check' and '_consume_until_tagged_response' methods from the IMAPClient class.", "unit_test": "def test_expunge_context_usage(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=sentinel.out)\n    self.client._command_and_check = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    self.client.expunge()\n    mockCommand.assert_called()\n    mockConsume.assert_called()", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'expunge' function should correctly use the '_command_and_check' and '_consume_until_tagged_response' from the IMAPClient class.", "unit_test": "def test_expunge_uid_usage(self):\n    self.client.use_uid = True\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)\n    self.client.use_uid = False\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)", "test": "tests/test_imapclient.py::TestExpunge::test_expunge_context_correct_usage"}}}
{"namespace": "mopidy.ext.load_extensions", "type": "function", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/ext.py", "signature_position": [210, 210], "body_position": [216, 268], "dependency": {"intra_class": ["mopidy.ext.Extension.dist_name", "mopidy.ext.Extension.ext_name", "mopidy.ext.Extension.get_command", "mopidy.ext.Extension.get_config_schema", "mopidy.ext.Extension.get_default_config", "mopidy.ext.Extension.version"], "intra_file": ["mopidy.ext.Extension", "mopidy.ext.ExtensionData", "mopidy.ext.logger"], "cross_file": []}, "requirement": {"Functionality": "This function finds all installed extensions by iterating through the entry points of the \"mopidy.ext\" package. It loads each entry point, checks if it is a valid extension class, and creates an ExtensionData object with the necessary attributes. The function then appends the ExtensionData object to a list of installed extensions and returns the list.", "Arguments": ":param: No input parameters.\n:return: List[ExtensionData]. A list of installed extensions, where each extension is represented by an ExtensionData object."}, "tests": ["tests/test_ext.py::TestLoadExtensions::test_gets_instance", "tests/test_ext.py::TestLoadExtensions::test_no_extensions", "tests/test_ext.py::TestLoadExtensions::test_gets_wrong_class", "tests/test_ext.py::TestLoadExtensions::test_creating_instance_fails", "tests/test_ext.py::TestLoadExtensions::test_get_default_config_fails"], "indent": 0, "domain": "Multimedia", "code": "def load_extensions() -> List[ExtensionData]:\n    \"\"\"Find all installed extensions.\n\n    :returns: list of installed extensions\n    \"\"\"\n\n    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions\n", "context": "from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n    def get_data_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create data directory for the extension.\n\n        Use this directory to store data that should be persistent.\n\n        :param config: the Mopidy config object\n        :returns: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\n###The function: load_extensions###\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n", "prompt": "Please write a python function called 'load_extensions' base the context. This function finds all installed extensions by iterating through the entry points of the \"mopidy.ext\" package. It loads each entry point, checks if it is a valid extension class, and creates an ExtensionData object with the necessary attributes. The function then appends the ExtensionData object to a list of installed extensions and returns the list.:param: No input parameters.\n:return: List[ExtensionData]. A list of installed extensions, where each extension is represented by an ExtensionData object..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n    def get_data_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create data directory for the extension.\n\n        Use this directory to store data that should be persistent.\n\n        :param config: the Mopidy config object\n        :returns: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\n###The function: load_extensions###\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n", "test_list": ["def test_gets_instance(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_no_extensions(self, iter_entry_points_mock):\n    iter_entry_points_mock.return_value = []\n    assert ext.load_extensions() == []", "def test_gets_wrong_class(self, iter_entry_points_mock):\n\n    class WrongClass:\n        pass\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = WrongClass\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_creating_instance_fails(self, iter_entry_points_mock):\n    mock_extension = mock.Mock(spec=ext.Extension)\n    mock_extension.side_effect = Exception\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = mock_extension\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_get_default_config_fails(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_default_config') as get:\n        get.side_effect = Exception\n        assert ext.load_extensions() == []\n        get.assert_called_once_with()"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'load_extensions' should return a list of ExtensionData objects, ensuring each object is correctly instantiated with valid attributes.", "unit_test": "def test_load_extensions_returns_correct_type(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    result = ext.load_extensions()\n    assert isinstance(result, list)\n    assert all(isinstance(item, ext.ExtensionData) for item in result)", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_returns_correct_type"}, "Exception Handling": {"requirement": "The function 'load_extensions' should handle exceptions raised during the loading of extensions and log appropriate error messages.", "unit_test": "def test_load_extensions_handles_exceptions(self, iter_entry_points_mock, caplog):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.side_effect = Exception('Test exception')\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with caplog.at_level(logging.ERROR):\n        result = ext.load_extensions()\n    assert result == []\n    assert 'Test exception' in caplog.text", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_handles_exceptions"}, "Edge Case Handling": {"requirement": "The function 'load_extensions' should correctly handle the case where no extensions are found and return an empty list.", "unit_test": "def test_load_extensions_no_extensions_found(self, iter_entry_points_mock):\n    iter_entry_points_mock.return_value = []\n    result = ext.load_extensions()\n    assert result == []", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_no_extensions_found"}, "Functionality Extension": {"requirement": "The function 'load_extensions' should support filtering extensions based on a specific criterion, such as version compatibility.", "unit_test": "def test_load_extensions_filters_by_version(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    DummyExtension.version = '2.0.0'\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    result = ext.load_extensions(min_version='2.0.0')\n    assert len(result) == 1\n    assert result[0].extension.version == '2.0.0'", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_filters_by_version"}, "Annotation Coverage": {"requirement": "The function 'load_extensions' should have complete type annotations for all parameters and return types.", "unit_test": "def test_load_extensions_annotations():\n    import inspect\n    signature = inspect.signature(ext.load_extensions)\n    assert signature.return_annotation == 'List[ExtensionData]'", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_annotations"}, "Code Complexity": {"requirement": "The function 'load_extensions' should maintain a cyclomatic complexity of 10 or less.", "unit_test": "def test_load_extensions_cyclomatic_complexity():\n    from radon.complexity import cc_visit\n    with open('path_to_file_containing_load_extensions.py', 'r') as f:\n        code = f.read()\n    complexity = cc_visit(code)\n    load_extensions_complexity = next((c for c in complexity if c.name == 'load_extensions'), None)\n    assert load_extensions_complexity is not None\n    assert load_extensions_complexity.complexity <= 10", "test": "tests/test_ext.py::TestLoadExtensions::test_ext_code_complexity"}, "Code Standard": {"requirement": "The function 'load_extensions' should adhere to PEP 8 coding standards, including proper indentation and spacing.", "unit_test": "def test_load_extensions_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_load_extensions.py'])\n    assert result.total_errors == 0", "test": "tests/test_ext.py::TestLoadExtensions::test_check_ext_code_style"}, "Context Usage Verification": {"requirement": "The function 'load_extensions' should utilize the 'mopidy.ext.Extension.get_config_schema' methods.", "unit_test": "def test_load_extensions_uses_extension_class(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_config_schema') as get_config_schema_mock:\n        ext.load_extensions()\n        get_config_schema_mock.assert_called_once()", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_uses_extension_class"}, "Context Usage Correctness Verification": {"requirement": "The function 'load_extensions' should correctly use the 'mopidy.ext.Extension.get_config_schema' method to retrieve the configuration schema for each extension.", "unit_test": "def test_load_extensions_correctly_uses_get_config_schema(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_config_schema', return_value=config_lib.ConfigSchema('dummy')) as get_config_schema_mock:\n        ext.load_extensions()\n        get_config_schema_mock.assert_called_once()", "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_correctly_uses_get_config_schema"}}}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "type": "method", "project_path": "Communications/Wikipedia-API", "completion_path": "Communications/Wikipedia-API/wikipediaapi/__init__.py", "signature_position": [934, 937], "body_position": [944, 949], "dependency": {"intra_class": ["wikipediaapi.WikipediaPage._called", "wikipediaapi.WikipediaPage._fetch", "wikipediaapi.WikipediaPage._section_mapping"], "intra_file": ["wikipediaapi.WikipediaPageSection"], "cross_file": []}, "requirement": {"Functionality": "This function returns the last section of the current Wikipedia page with the given title. It first checks if the \"extracts\" data has been fetched for the page. If not, it fetches the \"extracts\" data. Then, it retrieves the sections with the given title from the section mapping. If there are sections with the given title, it returns the last section. Otherwise, it returns None.", "Arguments": ":param self: WikipediaPage. An instance of the WikipediaPage class.\n:param title: str. The title of the section to retrieve.\n:return: Optional[WikipediaPageSection]. The last section of the current page with the given title."}, "tests": ["tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsubsection", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsection_by_title_return_last", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_with_erroneous_edit", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsection_by_title_with_multiple_spans", "tests/extract_wiki_format_test.py::TestWikiFormatExtracts::test_subsection_by_title"], "indent": 4, "domain": "Communications", "code": "    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n", "context": "\"\"\"\nWikipedia-API is easy to use wrapper for extracting information from Wikipedia.\n\nIt supports extracting texts, sections, links, categories, translations, etc.\nfrom Wikipedia. Documentation provides code snippets for the most common use\ncases.\n\"\"\"\n\n__version__ = (0, 6, 0)\nfrom collections import defaultdict\nfrom enum import IntEnum\nimport logging\nimport re\nfrom typing import Any, Dict, List, Optional, Union\nfrom urllib import parse\n\nimport requests\n\nUSER_AGENT = (\n    \"Wikipedia-API/\"\n    + \".\".join(str(s) for s in __version__)\n    + \"; https://github.com/martin-majlis/Wikipedia-API/\"\n)\n\nlog = logging.getLogger(__name__)\n\n\n# https://www.mediawiki.org/wiki/API:Main_page\nPagesDict = Dict[str, \"WikipediaPage\"]\n\n\nclass ExtractFormat(IntEnum):\n    \"\"\"Represents extraction format.\"\"\"\n\n    WIKI = 1\n    \"\"\"\n    Allows recognizing subsections\n\n    Example: https://goo.gl/PScNVV\n    \"\"\"\n\n    HTML = 2\n    \"\"\"\n    Alows retrieval of HTML tags\n\n    Example: https://goo.gl/1Jwwpr\n    \"\"\"\n\n    # Plain: https://goo.gl/MAv2qz\n    # Doesn't allow to recognize subsections\n    # PLAIN = 3\n\n\nclass Namespace(IntEnum):\n    \"\"\"\n    Represents namespace in Wikipedia\n\n    You can gen list of possible namespaces here:\n\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace#Programming\n\n    Currently following namespaces are supported:\n    \"\"\"\n\n    MAIN = 0\n    TALK = 1\n    USER = 2\n    USER_TALK = 3\n    WIKIPEDIA = 4\n    WIKIPEDIA_TALK = 5\n    FILE = 6\n    FILE_TALK = 7\n    MEDIAWIKI = 8\n    MEDIAWIKI_TALK = 9\n    TEMPLATE = 10\n    TEMPLATE_TALK = 11\n    HELP = 12\n    HELP_TALK = 13\n    CATEGORY = 14\n    CATEGORY_TALK = 15\n    PORTAL = 100\n    PORTAL_TALK = 101\n    PROJECT = 102\n    PROJECT_TALK = 103\n    REFERENCE = 104\n    REFERENCE_TALK = 105\n    BOOK = 108\n    BOOK_TALK = 109\n    DRAFT = 118\n    DRAFT_TALK = 119\n    EDUCATION_PROGRAM = 446\n    EDUCATION_PROGRAM_TALK = 447\n    TIMED_TEXT = 710\n    TIMED_TEXT_TALK = 711\n    MODULE = 828\n    MODULE_TALK = 829\n    GADGET = 2300\n    GADGET_TALK = 2301\n    GADGET_DEFINITION = 2302\n    GADGET_DEFINITION_TALK = 2303\n\n\nWikiNamespace = Union[Namespace, int]\n\n\ndef namespace2int(namespace: WikiNamespace) -> int:\n    \"\"\"Converts namespace into integer\"\"\"\n    if isinstance(namespace, Namespace):\n        return namespace.value\n\n    return namespace\n\n\nRE_SECTION = {\n    ExtractFormat.WIKI: re.compile(r\"\\n\\n *(==+) (.*?) (==+) *\\n\"),\n    ExtractFormat.HTML: re.compile(\n        r\"\\n? *<h([1-9])[^>]*?>(<span[^>]*></span>)? *\"\n        + \"(<span[^>]*>)? *(<span[^>]*></span>)? *(.*?) *\"\n        + \"(</span>)?(<span>Edit</span>)?</h[1-9]>\\n?\"\n        #                  ^^^^\n        # Example page with 'Edit' erroneous links: https://bit.ly/2ui4FWs\n    ),\n    # ExtractFormat.PLAIN.value: re.compile(r'\\n\\n *(===*) (.*?) (===*) *\\n'),\n}\n\n\nclass Wikipedia:\n    \"\"\"Wikipedia is wrapper for Wikipedia API.\"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        language: str = \"en\",\n        extract_format: ExtractFormat = ExtractFormat.WIKI,\n        headers: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Constructs Wikipedia object for extracting information Wikipedia.\n\n        :param user_agent: HTTP User-Agent used in requests\n                https://meta.wikimedia.org/wiki/User-Agent_policy\n        :param language: Language mutation of Wikipedia -\n                http://meta.wikimedia.org/wiki/List_of_Wikipedias\n        :param extract_format: Format used for extractions\n                :class:`ExtractFormat` object.\n        :param headers:  Headers sent as part of HTTP request\n        :param kwargs: Optional parameters used in -\n                http://docs.python-requests.org/en/master/api/#requests.request\n\n        Examples:\n\n        * Proxy: ``Wikipedia('foo (merlin@example.com)', proxies={'http': 'http://proxy:1234'})``\n        \"\"\"\n        kwargs.setdefault(\"timeout\", 10.0)\n\n        default_headers = {} if headers is None else headers\n        if user_agent:\n            default_headers.setdefault(\n                \"User-Agent\",\n                user_agent,\n            )\n        used_user_agent = default_headers.get(\"User-Agent\")\n        if not (used_user_agent and len(used_user_agent) > 5):\n            raise AssertionError(\n                \"Please, be nice to Wikipedia and specify user agent - \"\n                + \"https://meta.wikimedia.org/wiki/User-Agent_policy. Current user_agent: '\"\n                + str(used_user_agent)\n                + \"' is not sufficient.\"\n            )\n        default_headers[\"User-Agent\"] += \" (\" + USER_AGENT + \")\"\n\n        self.language = language.strip().lower()\n        if not self.language:\n            raise AssertionError(\n                \"Specify language. Current language: '\"\n                + str(self.language)\n                + \"' is not sufficient.\"\n            )\n        self.extract_format = extract_format\n\n        log.info(\n            \"Wikipedia: language=%s, user_agent: %s, extract_format=%s\",\n            self.language,\n            default_headers[\"User-Agent\"],\n            self.extract_format,\n        )\n\n        self._session = requests.Session()\n        self._session.headers.update(default_headers)\n        self._request_kwargs = kwargs\n\n    def __del__(self) -> None:\n        \"\"\"Closes session.\"\"\"\n        if hasattr(self, \"_session\") and self._session:\n            self._session.close()\n\n    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        Creating `WikipediaPage` object is always the first step for extracting\n        any information.\n\n        Example::\n\n            wiki_wiki = wikipediaapi.Wikipedia('en')\n            page_py = wiki_wiki.page('Python_(programming_language)')\n            print(page_py.title)\n            # Python (programming language)\n\n            wiki_hi = wikipediaapi.Wikipedia('hi')\n\n            page_hi_py = wiki_hi.article(\n                title='%E0%A4%AA%E0%A4%BE%E0%A4%87%E0%A4%A5%E0%A4%A8',\n                unquote=True,\n            )\n            print(page_hi_py.title)\n            # \u092a\u093e\u0907\u0925\u0928\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n\n    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        This function is an alias for :func:`page`\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n\n    def extracts(self, page: \"WikipediaPage\", **kwargs) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"extracts\",\n            \"titles\": page.title,\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params[\"explaintext\"] = 1\n            params[\"exsectionformat\"] = \"wiki\"\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return \"\"\n            return self._build_extracts(v, page)\n        return \"\"\n\n    def info(self, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"\n        https://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\n        https://www.mediawiki.org/wiki/API:Info\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"info\",\n            \"titles\": page.title,\n            \"inprop\": \"|\".join(\n                [\n                    \"protection\",\n                    \"talkid\",\n                    \"watched\",\n                    \"watchers\",\n                    \"visitingwatchers\",\n                    \"notificationtimestamp\",\n                    \"subjectid\",\n                    \"url\",\n                    \"readable\",\n                    \"preload\",\n                    \"displaytitle\",\n                ]\n            ),\n        }\n        raw = self._query(page, params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return page\n\n            return self._build_info(v, page)\n        return page\n\n    def langlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns langlinks of the page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        - https://www.mediawiki.org/wiki/API:Langlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to pages in other languages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"langlinks\",\n            \"titles\": page.title,\n            \"lllimit\": 500,\n            \"llprop\": \"url\",\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_langlinks(v, page)\n        return {}\n\n    def links(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns links to other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        - https://www.mediawiki.org/wiki/API:Links\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to linked pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"links\",\n            \"titles\": page.title,\n            \"pllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n\n            while \"continue\" in raw:\n                params[\"plcontinue\"] = raw[\"continue\"][\"plcontinue\"]\n                raw = self._query(page, params)\n                v[\"links\"] += raw[\"query\"][\"pages\"][k][\"links\"]\n\n            return self._build_links(v, page)\n        return {}\n\n    def backlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns backlinks from other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        - https://www.mediawiki.org/wiki/API:Backlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: backlinks from other pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"backlinks\",\n            \"bltitle\": page.title,\n            \"bllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"blcontinue\"] = raw[\"continue\"][\"blcontinue\"]\n            raw = self._query(page, params)\n            v[\"backlinks\"] += raw[\"query\"][\"backlinks\"]\n        return self._build_backlinks(v, page)\n\n    def categories(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns categories for page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        - https://www.mediawiki.org/wiki/API:Categories\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: categories for page\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"categories\",\n            \"titles\": page.title,\n            \"cllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_categories(v, page)\n        return {}\n\n    def categorymembers(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns pages in given category with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        - https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: pages in given category\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"categorymembers\",\n            \"cmtitle\": page.title,\n            \"cmlimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"cmcontinue\"] = raw[\"continue\"][\"cmcontinue\"]\n            raw = self._query(page, params)\n            v[\"categorymembers\"] += raw[\"query\"][\"categorymembers\"]\n\n        return self._build_categorymembers(v, page)\n\n    def _query(self, page: \"WikipediaPage\", params: Dict[str, Any]):\n        \"\"\"Queries Wikimedia API to fetch content.\"\"\"\n        base_url = \"https://\" + page.language + \".wikipedia.org/w/api.php\"\n        log.info(\n            \"Request URL: %s\",\n            base_url + \"?\" + \"&\".join([k + \"=\" + str(v) for k, v in params.items()]),\n        )\n        params[\"format\"] = \"json\"\n        params[\"redirects\"] = 1\n        r = self._session.get(base_url, params=params, **self._request_kwargs)\n        return r.json()\n\n    def _build_extracts(self, extract, page: \"WikipediaPage\") -> str:\n        \"\"\"Constructs summary of given page.\"\"\"\n        page._summary = \"\"\n        page._section_mapping = defaultdict(list)\n\n        self._common_attributes(extract, page)\n\n        section_stack = [page]\n        section = None\n        prev_pos = 0\n\n        for match in re.finditer(RE_SECTION[self.extract_format], extract[\"extract\"]):\n            if len(page._section_mapping) == 0:\n                page._summary = extract[\"extract\"][0 : match.start()].strip()\n            elif section is not None:\n                section._text = (extract[\"extract\"][prev_pos : match.start()]).strip()\n\n            section = self._create_section(match)\n            sec_level = section.level + 1\n\n            if sec_level > len(section_stack):\n                section_stack.append(section)\n            elif sec_level == len(section_stack):\n                section_stack.pop()\n                section_stack.append(section)\n            else:\n                for _ in range(len(section_stack) - sec_level + 1):\n                    section_stack.pop()\n                section_stack.append(section)\n\n            section_stack[len(section_stack) - 2]._section.append(section)\n            # section_stack[sec_level - 1]._section.append(section)\n\n            prev_pos = match.end()\n            page._section_mapping[section.title].append(section)\n\n        # pages without sections have only summary\n        if page._summary == \"\":\n            page._summary = extract[\"extract\"].strip()\n\n        if prev_pos > 0 and section is not None:\n            section._text = extract[\"extract\"][prev_pos:]\n\n        return page._summary\n\n    def _create_section(self, match):\n        \"\"\"Creates section.\"\"\"\n        sec_title = \"\"\n        sec_level = 2\n        if self.extract_format == ExtractFormat.WIKI:\n            sec_title = match.group(2).strip()\n            sec_level = len(match.group(1))\n        elif self.extract_format == ExtractFormat.HTML:\n            sec_title = match.group(5).strip()\n            sec_level = int(match.group(1).strip())\n\n        section = WikipediaPageSection(self, sec_title, sec_level - 1)\n        return section\n\n    def _build_info(self, extract, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"Builds page from API call info.\"\"\"\n        self._common_attributes(extract, page)\n        for k, v in extract.items():\n            page._attributes[k] = v\n\n        return page\n\n    def _build_langlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call langlinks.\"\"\"\n        page._langlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for langlink in extract.get(\"langlinks\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=langlink[\"*\"],\n                ns=Namespace.MAIN,\n                language=langlink[\"lang\"],\n                url=langlink[\"url\"],\n            )\n            page._langlinks[p.language] = p\n\n        return page._langlinks\n\n    def _build_links(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call links.\"\"\"\n        page._links = {}\n\n        self._common_attributes(extract, page)\n\n        for link in extract.get(\"links\", []):\n            page._links[link[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=link[\"title\"],\n                ns=int(link[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._links\n\n    def _build_backlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call backlinks.\"\"\"\n        page._backlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for backlink in extract.get(\"backlinks\", []):\n            page._backlinks[backlink[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=backlink[\"title\"],\n                ns=int(backlink[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._backlinks\n\n    def _build_categories(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categories.\"\"\"\n        page._categories = {}\n\n        self._common_attributes(extract, page)\n\n        for category in extract.get(\"categories\", []):\n            page._categories[category[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=category[\"title\"],\n                ns=int(category[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._categories\n\n    def _build_categorymembers(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categorymembers.\"\"\"\n        page._categorymembers = {}\n\n        self._common_attributes(extract, page)\n\n        for member in extract.get(\"categorymembers\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=member[\"title\"],\n                ns=int(member[\"ns\"]),\n                language=page.language,\n            )\n            p.pageid = member[\"pageid\"]  # type: ignore\n\n            page._categorymembers[member[\"title\"]] = p\n\n        return page._categorymembers\n\n    @staticmethod\n    def _common_attributes(extract, page: \"WikipediaPage\"):\n        \"\"\"Fills in common attributes for page.\"\"\"\n        common_attributes = [\"title\", \"pageid\", \"ns\", \"redirects\"]\n\n        for attr in common_attributes:\n            if attr in extract:\n                page._attributes[attr] = extract[attr]\n\n\nclass WikipediaPageSection:\n    \"\"\"WikipediaPageSection represents section in the page.\"\"\"\n\n    def __init__(\n        self, wiki: Wikipedia, title: str, level: int = 0, text: str = \"\"\n    ) -> None:\n        \"\"\"Constructs WikipediaPageSection.\"\"\"\n        self.wiki = wiki\n        self._title = title\n        self._level = level\n        self._text = text\n        self._section = []  # type: List['WikipediaPageSection']\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current section.\n\n        :return: title of the current section\n        \"\"\"\n        return self._title\n\n    @property\n    def level(self) -> int:\n        \"\"\"\n        Returns indentation level of the current section.\n\n        :return: indentation level of the current section\n        \"\"\"\n        return self._level\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current section.\n\n        :return: text of the current section\n        \"\"\"\n        return self._text\n\n    @property\n    def sections(self) -> List[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section.\n\n        :return: subsections of the current section\n        \"\"\"\n        return self._section\n\n    def section_by_title(self, title: str) -> Optional[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section with given title.\n\n        :param title: title of the subsection\n        :return: subsection if it exists\n        \"\"\"\n        sections = [s for s in self._section if s.title == title]\n        if sections:\n            return sections[-1]\n        return None\n\n    def full_text(self, level: int = 1) -> str:\n        \"\"\"\n        Returns text of the current section as well as all its subsections.\n\n        :param level: indentation level\n        :return: text of the current section as well as all its subsections\n        \"\"\"\n        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += f\"<h{level}>{self.title}</h{level}>\"\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += sec.full_text(level + 1)\n        return res\n\n    def __repr__(self):\n        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )\n\n\nclass WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n###The function: section_by_title###\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "prompt": "Please write a python function called 'section_by_title' base the context. This function returns the last section of the current Wikipedia page with the given title. It first checks if the \"extracts\" data has been fetched for the page. If not, it fetches the \"extracts\" data. Then, it retrieves the sections with the given title from the section mapping. If there are sections with the given title, it returns the last section. Otherwise, it returns None.:param self: WikipediaPage. An instance of the WikipediaPage class.\n:param title: str. The title of the section to retrieve.\n:return: Optional[WikipediaPageSection]. The last section of the current page with the given title..\n        The context you need to refer to is as follows: \"\"\"\nWikipedia-API is easy to use wrapper for extracting information from Wikipedia.\n\nIt supports extracting texts, sections, links, categories, translations, etc.\nfrom Wikipedia. Documentation provides code snippets for the most common use\ncases.\n\"\"\"\n\n__version__ = (0, 6, 0)\nfrom collections import defaultdict\nfrom enum import IntEnum\nimport logging\nimport re\nfrom typing import Any, Dict, List, Optional, Union\nfrom urllib import parse\n\nimport requests\n\nUSER_AGENT = (\n    \"Wikipedia-API/\"\n    + \".\".join(str(s) for s in __version__)\n    + \"; https://github.com/martin-majlis/Wikipedia-API/\"\n)\n\nlog = logging.getLogger(__name__)\n\n\n# https://www.mediawiki.org/wiki/API:Main_page\nPagesDict = Dict[str, \"WikipediaPage\"]\n\n\nclass ExtractFormat(IntEnum):\n    \"\"\"Represents extraction format.\"\"\"\n\n    WIKI = 1\n    \"\"\"\n    Allows recognizing subsections\n\n    Example: https://goo.gl/PScNVV\n    \"\"\"\n\n    HTML = 2\n    \"\"\"\n    Alows retrieval of HTML tags\n\n    Example: https://goo.gl/1Jwwpr\n    \"\"\"\n\n    # Plain: https://goo.gl/MAv2qz\n    # Doesn't allow to recognize subsections\n    # PLAIN = 3\n\n\nclass Namespace(IntEnum):\n    \"\"\"\n    Represents namespace in Wikipedia\n\n    You can gen list of possible namespaces here:\n\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace#Programming\n\n    Currently following namespaces are supported:\n    \"\"\"\n\n    MAIN = 0\n    TALK = 1\n    USER = 2\n    USER_TALK = 3\n    WIKIPEDIA = 4\n    WIKIPEDIA_TALK = 5\n    FILE = 6\n    FILE_TALK = 7\n    MEDIAWIKI = 8\n    MEDIAWIKI_TALK = 9\n    TEMPLATE = 10\n    TEMPLATE_TALK = 11\n    HELP = 12\n    HELP_TALK = 13\n    CATEGORY = 14\n    CATEGORY_TALK = 15\n    PORTAL = 100\n    PORTAL_TALK = 101\n    PROJECT = 102\n    PROJECT_TALK = 103\n    REFERENCE = 104\n    REFERENCE_TALK = 105\n    BOOK = 108\n    BOOK_TALK = 109\n    DRAFT = 118\n    DRAFT_TALK = 119\n    EDUCATION_PROGRAM = 446\n    EDUCATION_PROGRAM_TALK = 447\n    TIMED_TEXT = 710\n    TIMED_TEXT_TALK = 711\n    MODULE = 828\n    MODULE_TALK = 829\n    GADGET = 2300\n    GADGET_TALK = 2301\n    GADGET_DEFINITION = 2302\n    GADGET_DEFINITION_TALK = 2303\n\n\nWikiNamespace = Union[Namespace, int]\n\n\ndef namespace2int(namespace: WikiNamespace) -> int:\n    \"\"\"Converts namespace into integer\"\"\"\n    if isinstance(namespace, Namespace):\n        return namespace.value\n\n    return namespace\n\n\nRE_SECTION = {\n    ExtractFormat.WIKI: re.compile(r\"\\n\\n *(==+) (.*?) (==+) *\\n\"),\n    ExtractFormat.HTML: re.compile(\n        r\"\\n? *<h([1-9])[^>]*?>(<span[^>]*></span>)? *\"\n        + \"(<span[^>]*>)? *(<span[^>]*></span>)? *(.*?) *\"\n        + \"(</span>)?(<span>Edit</span>)?</h[1-9]>\\n?\"\n        #                  ^^^^\n        # Example page with 'Edit' erroneous links: https://bit.ly/2ui4FWs\n    ),\n    # ExtractFormat.PLAIN.value: re.compile(r'\\n\\n *(===*) (.*?) (===*) *\\n'),\n}\n\n\nclass Wikipedia:\n    \"\"\"Wikipedia is wrapper for Wikipedia API.\"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        language: str = \"en\",\n        extract_format: ExtractFormat = ExtractFormat.WIKI,\n        headers: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Constructs Wikipedia object for extracting information Wikipedia.\n\n        :param user_agent: HTTP User-Agent used in requests\n                https://meta.wikimedia.org/wiki/User-Agent_policy\n        :param language: Language mutation of Wikipedia -\n                http://meta.wikimedia.org/wiki/List_of_Wikipedias\n        :param extract_format: Format used for extractions\n                :class:`ExtractFormat` object.\n        :param headers:  Headers sent as part of HTTP request\n        :param kwargs: Optional parameters used in -\n                http://docs.python-requests.org/en/master/api/#requests.request\n\n        Examples:\n\n        * Proxy: ``Wikipedia('foo (merlin@example.com)', proxies={'http': 'http://proxy:1234'})``\n        \"\"\"\n        kwargs.setdefault(\"timeout\", 10.0)\n\n        default_headers = {} if headers is None else headers\n        if user_agent:\n            default_headers.setdefault(\n                \"User-Agent\",\n                user_agent,\n            )\n        used_user_agent = default_headers.get(\"User-Agent\")\n        if not (used_user_agent and len(used_user_agent) > 5):\n            raise AssertionError(\n                \"Please, be nice to Wikipedia and specify user agent - \"\n                + \"https://meta.wikimedia.org/wiki/User-Agent_policy. Current user_agent: '\"\n                + str(used_user_agent)\n                + \"' is not sufficient.\"\n            )\n        default_headers[\"User-Agent\"] += \" (\" + USER_AGENT + \")\"\n\n        self.language = language.strip().lower()\n        if not self.language:\n            raise AssertionError(\n                \"Specify language. Current language: '\"\n                + str(self.language)\n                + \"' is not sufficient.\"\n            )\n        self.extract_format = extract_format\n\n        log.info(\n            \"Wikipedia: language=%s, user_agent: %s, extract_format=%s\",\n            self.language,\n            default_headers[\"User-Agent\"],\n            self.extract_format,\n        )\n\n        self._session = requests.Session()\n        self._session.headers.update(default_headers)\n        self._request_kwargs = kwargs\n\n    def __del__(self) -> None:\n        \"\"\"Closes session.\"\"\"\n        if hasattr(self, \"_session\") and self._session:\n            self._session.close()\n\n    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        Creating `WikipediaPage` object is always the first step for extracting\n        any information.\n\n        Example::\n\n            wiki_wiki = wikipediaapi.Wikipedia('en')\n            page_py = wiki_wiki.page('Python_(programming_language)')\n            print(page_py.title)\n            # Python (programming language)\n\n            wiki_hi = wikipediaapi.Wikipedia('hi')\n\n            page_hi_py = wiki_hi.article(\n                title='%E0%A4%AA%E0%A4%BE%E0%A4%87%E0%A4%A5%E0%A4%A8',\n                unquote=True,\n            )\n            print(page_hi_py.title)\n            # \u092a\u093e\u0907\u0925\u0928\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n\n    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        This function is an alias for :func:`page`\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n\n    def extracts(self, page: \"WikipediaPage\", **kwargs) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"extracts\",\n            \"titles\": page.title,\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params[\"explaintext\"] = 1\n            params[\"exsectionformat\"] = \"wiki\"\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return \"\"\n            return self._build_extracts(v, page)\n        return \"\"\n\n    def info(self, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"\n        https://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\n        https://www.mediawiki.org/wiki/API:Info\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"info\",\n            \"titles\": page.title,\n            \"inprop\": \"|\".join(\n                [\n                    \"protection\",\n                    \"talkid\",\n                    \"watched\",\n                    \"watchers\",\n                    \"visitingwatchers\",\n                    \"notificationtimestamp\",\n                    \"subjectid\",\n                    \"url\",\n                    \"readable\",\n                    \"preload\",\n                    \"displaytitle\",\n                ]\n            ),\n        }\n        raw = self._query(page, params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return page\n\n            return self._build_info(v, page)\n        return page\n\n    def langlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns langlinks of the page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        - https://www.mediawiki.org/wiki/API:Langlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to pages in other languages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"langlinks\",\n            \"titles\": page.title,\n            \"lllimit\": 500,\n            \"llprop\": \"url\",\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_langlinks(v, page)\n        return {}\n\n    def links(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns links to other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        - https://www.mediawiki.org/wiki/API:Links\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to linked pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"links\",\n            \"titles\": page.title,\n            \"pllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n\n            while \"continue\" in raw:\n                params[\"plcontinue\"] = raw[\"continue\"][\"plcontinue\"]\n                raw = self._query(page, params)\n                v[\"links\"] += raw[\"query\"][\"pages\"][k][\"links\"]\n\n            return self._build_links(v, page)\n        return {}\n\n    def backlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns backlinks from other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        - https://www.mediawiki.org/wiki/API:Backlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: backlinks from other pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"backlinks\",\n            \"bltitle\": page.title,\n            \"bllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"blcontinue\"] = raw[\"continue\"][\"blcontinue\"]\n            raw = self._query(page, params)\n            v[\"backlinks\"] += raw[\"query\"][\"backlinks\"]\n        return self._build_backlinks(v, page)\n\n    def categories(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns categories for page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        - https://www.mediawiki.org/wiki/API:Categories\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: categories for page\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"categories\",\n            \"titles\": page.title,\n            \"cllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_categories(v, page)\n        return {}\n\n    def categorymembers(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns pages in given category with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        - https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: pages in given category\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"categorymembers\",\n            \"cmtitle\": page.title,\n            \"cmlimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"cmcontinue\"] = raw[\"continue\"][\"cmcontinue\"]\n            raw = self._query(page, params)\n            v[\"categorymembers\"] += raw[\"query\"][\"categorymembers\"]\n\n        return self._build_categorymembers(v, page)\n\n    def _query(self, page: \"WikipediaPage\", params: Dict[str, Any]):\n        \"\"\"Queries Wikimedia API to fetch content.\"\"\"\n        base_url = \"https://\" + page.language + \".wikipedia.org/w/api.php\"\n        log.info(\n            \"Request URL: %s\",\n            base_url + \"?\" + \"&\".join([k + \"=\" + str(v) for k, v in params.items()]),\n        )\n        params[\"format\"] = \"json\"\n        params[\"redirects\"] = 1\n        r = self._session.get(base_url, params=params, **self._request_kwargs)\n        return r.json()\n\n    def _build_extracts(self, extract, page: \"WikipediaPage\") -> str:\n        \"\"\"Constructs summary of given page.\"\"\"\n        page._summary = \"\"\n        page._section_mapping = defaultdict(list)\n\n        self._common_attributes(extract, page)\n\n        section_stack = [page]\n        section = None\n        prev_pos = 0\n\n        for match in re.finditer(RE_SECTION[self.extract_format], extract[\"extract\"]):\n            if len(page._section_mapping) == 0:\n                page._summary = extract[\"extract\"][0 : match.start()].strip()\n            elif section is not None:\n                section._text = (extract[\"extract\"][prev_pos : match.start()]).strip()\n\n            section = self._create_section(match)\n            sec_level = section.level + 1\n\n            if sec_level > len(section_stack):\n                section_stack.append(section)\n            elif sec_level == len(section_stack):\n                section_stack.pop()\n                section_stack.append(section)\n            else:\n                for _ in range(len(section_stack) - sec_level + 1):\n                    section_stack.pop()\n                section_stack.append(section)\n\n            section_stack[len(section_stack) - 2]._section.append(section)\n            # section_stack[sec_level - 1]._section.append(section)\n\n            prev_pos = match.end()\n            page._section_mapping[section.title].append(section)\n\n        # pages without sections have only summary\n        if page._summary == \"\":\n            page._summary = extract[\"extract\"].strip()\n\n        if prev_pos > 0 and section is not None:\n            section._text = extract[\"extract\"][prev_pos:]\n\n        return page._summary\n\n    def _create_section(self, match):\n        \"\"\"Creates section.\"\"\"\n        sec_title = \"\"\n        sec_level = 2\n        if self.extract_format == ExtractFormat.WIKI:\n            sec_title = match.group(2).strip()\n            sec_level = len(match.group(1))\n        elif self.extract_format == ExtractFormat.HTML:\n            sec_title = match.group(5).strip()\n            sec_level = int(match.group(1).strip())\n\n        section = WikipediaPageSection(self, sec_title, sec_level - 1)\n        return section\n\n    def _build_info(self, extract, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"Builds page from API call info.\"\"\"\n        self._common_attributes(extract, page)\n        for k, v in extract.items():\n            page._attributes[k] = v\n\n        return page\n\n    def _build_langlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call langlinks.\"\"\"\n        page._langlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for langlink in extract.get(\"langlinks\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=langlink[\"*\"],\n                ns=Namespace.MAIN,\n                language=langlink[\"lang\"],\n                url=langlink[\"url\"],\n            )\n            page._langlinks[p.language] = p\n\n        return page._langlinks\n\n    def _build_links(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call links.\"\"\"\n        page._links = {}\n\n        self._common_attributes(extract, page)\n\n        for link in extract.get(\"links\", []):\n            page._links[link[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=link[\"title\"],\n                ns=int(link[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._links\n\n    def _build_backlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call backlinks.\"\"\"\n        page._backlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for backlink in extract.get(\"backlinks\", []):\n            page._backlinks[backlink[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=backlink[\"title\"],\n                ns=int(backlink[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._backlinks\n\n    def _build_categories(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categories.\"\"\"\n        page._categories = {}\n\n        self._common_attributes(extract, page)\n\n        for category in extract.get(\"categories\", []):\n            page._categories[category[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=category[\"title\"],\n                ns=int(category[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._categories\n\n    def _build_categorymembers(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categorymembers.\"\"\"\n        page._categorymembers = {}\n\n        self._common_attributes(extract, page)\n\n        for member in extract.get(\"categorymembers\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=member[\"title\"],\n                ns=int(member[\"ns\"]),\n                language=page.language,\n            )\n            p.pageid = member[\"pageid\"]  # type: ignore\n\n            page._categorymembers[member[\"title\"]] = p\n\n        return page._categorymembers\n\n    @staticmethod\n    def _common_attributes(extract, page: \"WikipediaPage\"):\n        \"\"\"Fills in common attributes for page.\"\"\"\n        common_attributes = [\"title\", \"pageid\", \"ns\", \"redirects\"]\n\n        for attr in common_attributes:\n            if attr in extract:\n                page._attributes[attr] = extract[attr]\n\n\nclass WikipediaPageSection:\n    \"\"\"WikipediaPageSection represents section in the page.\"\"\"\n\n    def __init__(\n        self, wiki: Wikipedia, title: str, level: int = 0, text: str = \"\"\n    ) -> None:\n        \"\"\"Constructs WikipediaPageSection.\"\"\"\n        self.wiki = wiki\n        self._title = title\n        self._level = level\n        self._text = text\n        self._section = []  # type: List['WikipediaPageSection']\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current section.\n\n        :return: title of the current section\n        \"\"\"\n        return self._title\n\n    @property\n    def level(self) -> int:\n        \"\"\"\n        Returns indentation level of the current section.\n\n        :return: indentation level of the current section\n        \"\"\"\n        return self._level\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current section.\n\n        :return: text of the current section\n        \"\"\"\n        return self._text\n\n    @property\n    def sections(self) -> List[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section.\n\n        :return: subsections of the current section\n        \"\"\"\n        return self._section\n\n    def section_by_title(self, title: str) -> Optional[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section with given title.\n\n        :param title: title of the subsection\n        :return: subsection if it exists\n        \"\"\"\n        sections = [s for s in self._section if s.title == title]\n        if sections:\n            return sections[-1]\n        return None\n\n    def full_text(self, level: int = 1) -> str:\n        \"\"\"\n        Returns text of the current section as well as all its subsections.\n\n        :param level: indentation level\n        :return: text of the current section as well as all its subsections\n        \"\"\"\n        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += f\"<h{level}>{self.title}</h{level}>\"\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += sec.full_text(level + 1)\n        return res\n\n    def __repr__(self):\n        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )\n\n\nclass WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n###The function: section_by_title###\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "test_list": ["def test_subsubsection(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 4.2.2')\n    self.assertEqual(section.title, 'Section 4.2.2')\n    self.assertEqual(section.text, '<p><b>Text for section 4.2.2</b>\\n\\n\\n</p>')\n    self.assertEqual(repr(section), 'Section: Section 4.2.2 (3):\\n' + '<p><b>Text for section 4.2.2</b>\\n\\n\\n</p>\\n' + 'Subsections (0):\\n')\n    self.assertEqual(len(section.sections), 0)", "def test_subsection_by_title_return_last(self):\n    page = self.wiki.page('Test_Nested')\n    section = page.section_by_title('Subsection B')\n    self.assertEqual(section.title, 'Subsection B')\n    self.assertEqual(section.text, '<p><b>Text for section 3.B</b>\\n\\n\\n</p>')\n    self.assertEqual(len(section.sections), 0)", "def test_with_erroneous_edit(self):\n    page = self.wiki.page('Test_Edit')\n    self.maxDiff = None\n    section = page.section_by_title('Section with Edit')\n    self.assertEqual(section.title, 'Section with Edit')\n    self.assertEqual(page.text, '<p><b>Summary</b> text\\n\\n</p>\\n\\n' + '<h2>Section 1</h2>\\n' + '<p>Text for section 1</p>\\n\\n<h3>Section with Edit</h3>\\n' + '<p>Text for section with edit\\n\\n\\n</p>')", "def test_subsection_by_title_with_multiple_spans(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 5')\n    self.assertEqual(section.title, 'Section 5')", "def test_subsection_by_title(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 4')\n    self.assertEqual(section.title, 'Section 4')\n    self.assertEqual(section.level, 1)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'section_by_title' should return None if the title does not exist in the section mapping.", "unit_test": "def test_section_by_title_nonexistent(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Nonexistent Section')\n    self.assertIsNone(section)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_nonexistent"}, "Exception Handling": {"requirement": "The function 'section_by_title' should raise a TypeError if the title parameter is not a string.", "unit_test": "def test_section_by_title_invalid_type(self):\n    page = self.wiki.page('Test_1')\n    with self.assertRaises(TypeError):\n        page.section_by_title(123)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_invalid_type"}, "Edge Case Handling": {"requirement": "The function 'section_by_title' should handle pages with no sections gracefully, returning None.", "unit_test": "def test_section_by_title_no_sections(self):\n    page = self.wiki.page('Empty_Page')\n    section = page.section_by_title('Any Section')\n    self.assertIsNone(section)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_no_sections"}, "Functionality Extension": {"requirement": "Extend the 'section_by_title' function to return a list of all sections with the given title if a parameter 'all' is set to True.", "unit_test": "def test_section_by_title_return_all(self):\n    page = self.wiki.page('Test_Multiple')\n    sections = page.section_by_title('Repeated Section', all=True)\n    self.assertEqual(len(sections), 3)\n    self.assertEqual(sections[0].title, 'Repeated Section')", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_return_all"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types in the 'section_by_title' function are annotated with type hints.", "unit_test": "def test_section_by_title_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(WikipediaPage.section_by_title)\n    self.assertEqual(hints['title'], str)\n    self.assertEqual(hints['return'], Optional[WikipediaPageSection])", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_annotations"}, "Code Complexity": {"requirement": "The 'section_by_title' function should have a cyclomatic complexity of 3 or less.", "unit_test": "def test_section_by_title_complexity(self):\n    import radon.complexity as cc\n    source = inspect.getsource(WikipediaPage.section_by_title)\n    complexity = cc.cc_visit(source)\n    self.assertLessEqual(complexity[0].complexity, 3)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_code_complexity"}, "Code Standard": {"requirement": "The 'section_by_title' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_section_by_title_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['wikipediaapi.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'section_by_title' function should utilize the '_section_mapping' attribute of the WikipediaPage class.", "unit_test": "def test_section_by_title_context_usage(self):\n    page = self.wiki.page('Test_1')\n    page._section_mapping = {'Test Section': [WikipediaPageSection(self.wiki, 'Test Section')]}\n    section = page.section_by_title('Test Section')\n    self.assertIsNotNone(section)", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'section_by_title' function should correctly access the '_section_mapping' attribute to retrieve sections by title.", "unit_test": "def test_section_by_title_context_correctness(self):\n    page = self.wiki.page('Test_1')\n    page._section_mapping = {'Correct Section': [WikipediaPageSection(self.wiki, 'Correct Section')]}\n    section = page.section_by_title('Correct Section')\n    self.assertEqual(section.title, 'Correct Section')", "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_context_correctness"}}}
{"namespace": "csvkit.cli.CSVKitUtility.run", "type": "method", "project_path": "Scientific-Engineering/csvkit", "completion_path": "Scientific-Engineering/csvkit/csvkit/cli.py", "signature_position": [106, 106], "body_position": [111, 122], "dependency": {"intra_class": ["csvkit.cli.CSVKitUtility._open_input_file", "csvkit.cli.CSVKitUtility.args", "csvkit.cli.CSVKitUtility.input_file", "csvkit.cli.CSVKitUtility.main", "csvkit.cli.CSVKitUtility.override_flags"], "intra_file": ["csvkit.cli.LazyFile.close"], "cross_file": []}, "requirement": {"Functionality": "This function is a wrapper around the main loop of a utility. It handles opening and closing files. It first checks if the 'f' flag is not present in the override flags. If not present, it opens the input file. Then, it executes the main loop of the utility, ignoring warnings related to column names if the 'no_header_row' option is present. Finally, it closes the input file if the 'f' flag is not present in the override flags.", "Arguments": ":param self: CSVKitUtility. An instance of the CSVKitUtility class.\n:return: No return values."}, "tests": ["tests/test_utilities/test_csvsql.py::TestCSVSQL::test_before_after_insert", "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_duplicate_keys"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def run(self):\n        \"\"\"\n        A wrapper around the main loop of the utility which handles opening and\n        closing files.\n        \"\"\"\n        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        try:\n            with warnings.catch_warnings():\n                if getattr(self.args, 'no_header_row', None):\n                    warnings.filterwarnings(action='ignore', message='Column names not specified', module='agate')\n\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()\n", "context": "#!/usr/bin/env python\n\nimport argparse\nimport bz2\nimport datetime\nimport decimal\nimport gzip\nimport itertools\nimport lzma\nimport sys\nimport warnings\nfrom os.path import splitext\n\nimport agate\nfrom agate.data_types.base import DEFAULT_NULL_VALUES\n\nfrom csvkit.exceptions import ColumnIdentifierError, RequiredHeaderError\n\n\nclass LazyFile:\n    \"\"\"\n    A proxy for a File object that delays opening it until\n    a read method is called.\n\n    Currently this implements only the minimum methods to be useful,\n    but it could easily be expanded.\n    \"\"\"\n\n    def __init__(self, init, *args, **kwargs):\n        self.init = init\n        self.f = None\n        self._is_lazy_opened = False\n\n        self._lazy_args = args\n        self._lazy_kwargs = kwargs\n\n    def __getattr__(self, name):\n        self._open()\n        return getattr(self.f, name)\n\n    def __iter__(self):\n        return self\n\n    def close(self):\n        if self._is_lazy_opened:\n            self.f.close()\n            self.f = None\n            self._is_lazy_opened = False\n\n    def __next__(self):\n        self._open()\n        return next(self.f).replace('\\0', '')\n\n    def _open(self):\n        if not self._is_lazy_opened:\n            self.f = self.init(*self._lazy_args, **self._lazy_kwargs)\n            self._is_lazy_opened = True\n\n\nclass CSVKitUtility:\n    description = ''\n    epilog = ''\n    override_flags = ''\n\n    def __init__(self, args=None, output_file=None):\n        \"\"\"\n        Perform argument processing and other setup for a CSVKitUtility.\n        \"\"\"\n        self._init_common_parser()\n        self.add_arguments()\n        self.args = self.argparser.parse_args(args)\n        # Output file is only set during testing.\n        if output_file is None:\n            self.output_file = sys.stdout\n        else:\n            self.output_file = output_file\n\n        self.reader_kwargs = self._extract_csv_reader_kwargs()\n        self.writer_kwargs = self._extract_csv_writer_kwargs()\n\n        self._install_exception_handler()\n\n        # Ensure SIGPIPE doesn't throw an exception\n        # Prevents [Errno 32] Broken pipe errors, e.g. when piping to 'head'\n        # To test from the shell:\n        #  python -c \"for i in range(5000): print('a,b,c')\" | csvlook | head\n        # Without this fix you will see at the end:\n        #  [Errno 32] Broken pipe\n        # With this fix, there should be no error\n        # For details on Python and SIGPIPE, see https://bugs.python.org/issue1652\n        try:\n            import signal\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        except (ImportError, AttributeError):\n            # Do nothing on platforms that don't have signals or don't have SIGPIPE\n            pass\n\n    def add_arguments(self):\n        \"\"\"\n        Called upon initialization once the parser for common arguments has been constructed.\n\n        Should be overriden by individual utilities.\n        \"\"\"\n        raise NotImplementedError('add_arguments must be provided by each subclass of CSVKitUtility.')\n\n###The function: run###\n    def main(self):\n        \"\"\"\n        Main loop of the utility.\n\n        Should be overriden by individual utilities and explicitly called by the executing script.\n        \"\"\"\n        raise NotImplementedError(' must be provided by each subclass of CSVKitUtility.')\n\n    def _init_common_parser(self):\n        \"\"\"\n        Prepare a base argparse argument parser so that flags are consistent across different shell command tools.\n        If you want to constrain which common args are present, you can pass a string for 'omitflags'. Any argument\n        whose single-letter form is contained in 'omitflags' will be left out of the configured parser. Use 'f' for\n        file.\n        \"\"\"\n        self.argparser = argparse.ArgumentParser(description=self.description, epilog=self.epilog)\n\n        # Input\n        if 'f' not in self.override_flags:\n            self.argparser.add_argument(\n                metavar='FILE', nargs='?', dest='input_path',\n                help='The CSV file to operate on. If omitted, will accept input as piped data via STDIN.')\n        if 'd' not in self.override_flags:\n            self.argparser.add_argument(\n                '-d', '--delimiter', dest='delimiter',\n                help='Delimiting character of the input CSV file.')\n        if 't' not in self.override_flags:\n            self.argparser.add_argument(\n                '-t', '--tabs', dest='tabs', action='store_true',\n                help='Specify that the input CSV file is delimited with tabs. Overrides \"-d\".')\n        if 'q' not in self.override_flags:\n            self.argparser.add_argument(\n                '-q', '--quotechar', dest='quotechar',\n                help='Character used to quote strings in the input CSV file.')\n        if 'u' not in self.override_flags:\n            self.argparser.add_argument(\n                '-u', '--quoting', dest='quoting', type=int, choices=[0, 1, 2, 3],\n                help='Quoting style used in the input CSV file. 0 = Quote Minimal, 1 = Quote All, '\n                     '2 = Quote Non-numeric, 3 = Quote None.')\n        if 'b' not in self.override_flags:\n            self.argparser.add_argument(\n                '-b', '--no-doublequote', dest='doublequote', action='store_false',\n                help='Whether or not double quotes are doubled in the input CSV file.')\n        if 'p' not in self.override_flags:\n            self.argparser.add_argument(\n                '-p', '--escapechar', dest='escapechar',\n                help='Character used to escape the delimiter if --quoting 3 (\"Quote None\") is specified and to escape '\n                     'the QUOTECHAR if --no-doublequote is specified.')\n        if 'z' not in self.override_flags:\n            self.argparser.add_argument(\n                '-z', '--maxfieldsize', dest='field_size_limit', type=int,\n                help='Maximum length of a single field in the input CSV file.')\n        if 'e' not in self.override_flags:\n            self.argparser.add_argument(\n                '-e', '--encoding', dest='encoding', default='utf-8-sig',\n                help='Specify the encoding of the input CSV file.')\n        if 'L' not in self.override_flags:\n            self.argparser.add_argument(\n                '-L', '--locale', dest='locale', default='en_US',\n                help='Specify the locale (en_US) of any formatted numbers.')\n        if 'S' not in self.override_flags:\n            self.argparser.add_argument(\n                '-S', '--skipinitialspace', dest='skipinitialspace', action='store_true',\n                help='Ignore whitespace immediately following the delimiter.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--blanks', dest='blanks', action='store_true',\n                help='Do not convert \"\", \"na\", \"n/a\", \"none\", \"null\", \".\" to NULL.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--null-value', dest='null_values', nargs='+', default=[],\n                help='Convert this value to NULL. --null-value can be specified multiple times.')\n        if 'date-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--date-format', dest='date_format',\n                help='Specify a strptime date format string like \"%%m/%%d/%%Y\".')\n        if 'datetime-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--datetime-format', dest='datetime_format',\n                help='Specify a strptime datetime format string like \"%%m/%%d/%%Y %%I:%%M %%p\".')\n        if 'H' not in self.override_flags:\n            self.argparser.add_argument(\n                '-H', '--no-header-row', dest='no_header_row', action='store_true',\n                help='Specify that the input CSV file has no header row. Will create default headers (a,b,c,...).')\n        if 'K' not in self.override_flags:\n            self.argparser.add_argument(\n                '-K', '--skip-lines', dest='skip_lines', type=int, default=0,\n                help='Specify the number of initial lines to skip before the header row (e.g. comments, copyright '\n                     'notices, empty rows).')\n        if 'v' not in self.override_flags:\n            self.argparser.add_argument(\n                '-v', '--verbose', dest='verbose', action='store_true',\n                help='Print detailed tracebacks when errors occur.')\n\n        # Output\n        if 'l' not in self.override_flags:\n            self.argparser.add_argument(\n                '-l', '--linenumbers', dest='line_numbers', action='store_true',\n                help='Insert a column of line numbers at the front of the output. Useful when piping to grep or as a '\n                     'simple primary key.')\n\n        # Input/Output\n        if 'zero' not in self.override_flags:\n            self.argparser.add_argument(\n                '--zero', dest='zero_based', action='store_true',\n                help='When interpreting or displaying column numbers, use zero-based numbering instead of the default '\n                     '1-based numbering.')\n\n        self.argparser.add_argument(\n            '-V', '--version', action='version', version='%(prog)s 1.3.0',\n            help='Display version information and exit.')\n\n    def _open_input_file(self, path, opened=False):\n        \"\"\"\n        Open the input file specified on the command line.\n        \"\"\"\n        if not path or path == '-':\n            # \"UnsupportedOperation: It is not possible to set the encoding or newline of stream after the first read\"\n            if not opened:\n                sys.stdin.reconfigure(encoding=self.args.encoding)\n            f = sys.stdin\n        else:\n            extension = splitext(path)[1]\n\n            if extension == '.gz':\n                func = gzip.open\n            elif extension == '.bz2':\n                func = bz2.open\n            elif extension == \".xz\":\n                func = lzma.open\n            else:\n                func = open\n\n            f = LazyFile(func, path, mode='rt', encoding=self.args.encoding)\n\n        return f\n\n    def _extract_csv_reader_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the input CSV reader(s).\n        \"\"\"\n        kwargs = {}\n\n        if self.args.tabs:\n            kwargs['delimiter'] = '\\t'\n        elif self.args.delimiter:\n            kwargs['delimiter'] = self.args.delimiter\n\n        for arg in ('quotechar', 'quoting', 'doublequote', 'escapechar', 'field_size_limit', 'skipinitialspace'):\n            value = getattr(self.args, arg)\n            if value is not None:\n                kwargs[arg] = value\n\n        if getattr(self.args, 'no_header_row', None):\n            kwargs['header'] = not self.args.no_header_row\n\n        return kwargs\n\n    def _extract_csv_writer_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the output CSV writer.\n        \"\"\"\n        kwargs = {}\n\n        if getattr(self.args, 'line_numbers', None):\n            kwargs['line_numbers'] = True\n\n        return kwargs\n\n    def _install_exception_handler(self):\n        \"\"\"\n        Installs a replacement for sys.excepthook, which handles pretty-printing uncaught exceptions.\n        \"\"\"\n        def handler(t, value, traceback):\n            if self.args.verbose:\n                sys.__excepthook__(t, value, traceback)\n            else:\n                # Special case handling for Unicode errors, which behave very strangely\n                # when cast with unicode()\n                if t == UnicodeDecodeError:\n                    sys.stderr.write('Your file is not \"%s\" encoded. Please specify the correct encoding with the -e '\n                                     'flag or with the PYTHONIOENCODING environment variable. Use the -v flag to see '\n                                     'the complete error.\\n' % self.args.encoding)\n                else:\n                    sys.stderr.write(f'{t.__name__}: {str(value)}\\n')\n\n        sys.excepthook = handler\n\n    def get_column_types(self):\n        if getattr(self.args, 'blanks', None):\n            type_kwargs = {'null_values': []}\n        else:\n            type_kwargs = {'null_values': list(DEFAULT_NULL_VALUES)}\n        for null_value in getattr(self.args, 'null_values', []):\n            type_kwargs['null_values'].append(null_value)\n\n        text_type = agate.Text(**type_kwargs)\n\n        if self.args.no_inference:\n            types = [text_type]\n        else:\n            number_type = agate.Number(locale=self.args.locale, **type_kwargs)\n\n            # See the order in the `agate.TypeTester` class.\n            types = [\n                agate.Boolean(**type_kwargs),\n                agate.TimeDelta(**type_kwargs),\n                agate.Date(date_format=self.args.date_format, **type_kwargs),\n                agate.DateTime(datetime_format=self.args.datetime_format, **type_kwargs),\n                text_type,\n            ]\n\n            # In order to parse dates like \"20010101\".\n            if self.args.date_format or self.args.datetime_format:\n                types.insert(-1, number_type)\n            else:\n                types.insert(1, number_type)\n\n        return agate.TypeTester(types=types)\n\n    def get_column_offset(self):\n        if self.args.zero_based:\n            return 0\n        return 1\n\n    def skip_lines(self):\n        if isinstance(self.args.skip_lines, int):\n            while self.args.skip_lines > 0:\n                self.input_file.readline()\n                self.args.skip_lines -= 1\n        else:\n            raise ValueError('skip_lines argument must be an int')\n\n        return self.input_file\n\n    def get_rows_and_column_names_and_column_ids(self, **kwargs):\n        rows = agate.csv.reader(self.skip_lines(), **kwargs)\n\n        try:\n            next_row = next(rows)\n        except StopIteration:\n            return iter([]), [], []\n\n        if self.args.no_header_row:\n            # Peek at a row to get the number of columns.\n            row = next_row\n            rows = itertools.chain([row], rows)\n            column_names = make_default_headers(len(row))\n        else:\n            column_names = next_row\n\n        column_offset = self.get_column_offset()\n        if kwargs.get('line_numbers'):\n            column_offset -= 1\n\n        column_ids = parse_column_identifiers(\n            self.args.columns,\n            column_names,\n            column_offset,\n            getattr(self.args, 'not_columns', None),\n        )\n\n        return rows, column_names, column_ids\n\n    def print_column_names(self):\n        \"\"\"\n        Pretty-prints the names and indices of all columns to a file-like object (usually sys.stdout).\n        \"\"\"\n        if getattr(self.args, 'no_header_row', None):\n            raise RequiredHeaderError('You cannot use --no-header-row with the -n or --names options.')\n\n        if getattr(self.args, 'zero_based', None):\n            start = 0\n        else:\n            start = 1\n\n        rows = agate.csv.reader(self.skip_lines(), **self.reader_kwargs)\n        column_names = next(rows)\n\n        for i, c in enumerate(column_names, start):\n            self.output_file.write('%3i: %s\\n' % (i, c))\n\n    def additional_input_expected(self):\n        return isatty(sys.stdin) and not self.args.input_path\n\n\ndef isatty(f):\n    try:\n        return f.isatty()\n    except ValueError:  # I/O operation on closed file\n        return False\n\n\ndef default_str_decimal(obj):\n    if isinstance(obj, (datetime.date, datetime.datetime)):\n        return obj.isoformat()\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError(f'{repr(obj)} is not JSON serializable')\n\n\ndef default_float_decimal(obj):\n    if isinstance(obj, decimal.Decimal):\n        return float(obj)\n    return default_str_decimal(obj)\n\n\ndef make_default_headers(n):\n    \"\"\"\n    Make a set of simple, default headers for files that are missing them.\n    \"\"\"\n    return tuple(agate.utils.letter_name(i) for i in range(n))\n\n\ndef match_column_identifier(column_names, c, column_offset=1):\n    \"\"\"\n    Determine what column a single column id (name or index) matches in a series of column names.\n    Note that integer values are *always* treated as positional identifiers. If you happen to have\n    column names which are also integers, you must specify them using a positional index.\n    \"\"\"\n    if isinstance(c, str) and not c.isdigit() and c in column_names:\n        return column_names.index(c)\n\n    try:\n        c = int(c) - column_offset\n    # Fail out if neither a column name nor an integer\n    except ValueError:\n        raise ColumnIdentifierError(\"Column '%s' is invalid. It is neither an integer nor a column name. \"\n                                    \"Column names are: %s\" % (c, repr(column_names)[1:-1]))\n\n    # Fail out if index is 0-based\n    if c < 0:\n        raise ColumnIdentifierError(\"Column %i is invalid. Columns are 1-based.\" % (c + column_offset))\n\n    # Fail out if index is out of range\n    if c >= len(column_names):\n        raise ColumnIdentifierError(\"Column %i is invalid. The last column is '%s' at index %i.\" % (\n            c + column_offset, column_names[-1], len(column_names) - 1 + column_offset))\n\n    return c\n\n\ndef parse_column_identifiers(ids, column_names, column_offset=1, excluded_columns=None):\n    \"\"\"\n    Parse a comma-separated list of column indices AND/OR names into a list of integer indices.\n    Ranges of integers can be specified with two integers separated by a '-' or ':' character.\n    Ranges of non-integers (e.g. column names) are not supported.\n    Note: Column indices are 1-based.\n    \"\"\"\n    if not column_names:\n        return []\n\n    if not ids and not excluded_columns:\n        return range(len(column_names))\n\n    if ids:\n        columns = []\n\n        for c in ids.split(','):\n            try:\n                columns.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names) + 1\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    columns.append(match_column_identifier(column_names, x, column_offset))\n    else:\n        columns = range(len(column_names))\n\n    excludes = []\n\n    if excluded_columns:\n        for c in excluded_columns.split(','):\n            try:\n                excludes.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names)\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    excludes.append(match_column_identifier(column_names, x, column_offset))\n\n    return [c for c in columns if c not in excludes]\n", "prompt": "Please write a python function called 'run' base the context. This function is a wrapper around the main loop of a utility. It handles opening and closing files. It first checks if the 'f' flag is not present in the override flags. If not present, it opens the input file. Then, it executes the main loop of the utility, ignoring warnings related to column names if the 'no_header_row' option is present. Finally, it closes the input file if the 'f' flag is not present in the override flags.:param self: CSVKitUtility. An instance of the CSVKitUtility class.\n:return: No return values..\n        The context you need to refer to is as follows: #!/usr/bin/env python\n\nimport argparse\nimport bz2\nimport datetime\nimport decimal\nimport gzip\nimport itertools\nimport lzma\nimport sys\nimport warnings\nfrom os.path import splitext\n\nimport agate\nfrom agate.data_types.base import DEFAULT_NULL_VALUES\n\nfrom csvkit.exceptions import ColumnIdentifierError, RequiredHeaderError\n\n\nclass LazyFile:\n    \"\"\"\n    A proxy for a File object that delays opening it until\n    a read method is called.\n\n    Currently this implements only the minimum methods to be useful,\n    but it could easily be expanded.\n    \"\"\"\n\n    def __init__(self, init, *args, **kwargs):\n        self.init = init\n        self.f = None\n        self._is_lazy_opened = False\n\n        self._lazy_args = args\n        self._lazy_kwargs = kwargs\n\n    def __getattr__(self, name):\n        self._open()\n        return getattr(self.f, name)\n\n    def __iter__(self):\n        return self\n\n    def close(self):\n        if self._is_lazy_opened:\n            self.f.close()\n            self.f = None\n            self._is_lazy_opened = False\n\n    def __next__(self):\n        self._open()\n        return next(self.f).replace('\\0', '')\n\n    def _open(self):\n        if not self._is_lazy_opened:\n            self.f = self.init(*self._lazy_args, **self._lazy_kwargs)\n            self._is_lazy_opened = True\n\n\nclass CSVKitUtility:\n    description = ''\n    epilog = ''\n    override_flags = ''\n\n    def __init__(self, args=None, output_file=None):\n        \"\"\"\n        Perform argument processing and other setup for a CSVKitUtility.\n        \"\"\"\n        self._init_common_parser()\n        self.add_arguments()\n        self.args = self.argparser.parse_args(args)\n        # Output file is only set during testing.\n        if output_file is None:\n            self.output_file = sys.stdout\n        else:\n            self.output_file = output_file\n\n        self.reader_kwargs = self._extract_csv_reader_kwargs()\n        self.writer_kwargs = self._extract_csv_writer_kwargs()\n\n        self._install_exception_handler()\n\n        # Ensure SIGPIPE doesn't throw an exception\n        # Prevents [Errno 32] Broken pipe errors, e.g. when piping to 'head'\n        # To test from the shell:\n        #  python -c \"for i in range(5000): print('a,b,c')\" | csvlook | head\n        # Without this fix you will see at the end:\n        #  [Errno 32] Broken pipe\n        # With this fix, there should be no error\n        # For details on Python and SIGPIPE, see https://bugs.python.org/issue1652\n        try:\n            import signal\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        except (ImportError, AttributeError):\n            # Do nothing on platforms that don't have signals or don't have SIGPIPE\n            pass\n\n    def add_arguments(self):\n        \"\"\"\n        Called upon initialization once the parser for common arguments has been constructed.\n\n        Should be overriden by individual utilities.\n        \"\"\"\n        raise NotImplementedError('add_arguments must be provided by each subclass of CSVKitUtility.')\n\n###The function: run###\n    def main(self):\n        \"\"\"\n        Main loop of the utility.\n\n        Should be overriden by individual utilities and explicitly called by the executing script.\n        \"\"\"\n        raise NotImplementedError(' must be provided by each subclass of CSVKitUtility.')\n\n    def _init_common_parser(self):\n        \"\"\"\n        Prepare a base argparse argument parser so that flags are consistent across different shell command tools.\n        If you want to constrain which common args are present, you can pass a string for 'omitflags'. Any argument\n        whose single-letter form is contained in 'omitflags' will be left out of the configured parser. Use 'f' for\n        file.\n        \"\"\"\n        self.argparser = argparse.ArgumentParser(description=self.description, epilog=self.epilog)\n\n        # Input\n        if 'f' not in self.override_flags:\n            self.argparser.add_argument(\n                metavar='FILE', nargs='?', dest='input_path',\n                help='The CSV file to operate on. If omitted, will accept input as piped data via STDIN.')\n        if 'd' not in self.override_flags:\n            self.argparser.add_argument(\n                '-d', '--delimiter', dest='delimiter',\n                help='Delimiting character of the input CSV file.')\n        if 't' not in self.override_flags:\n            self.argparser.add_argument(\n                '-t', '--tabs', dest='tabs', action='store_true',\n                help='Specify that the input CSV file is delimited with tabs. Overrides \"-d\".')\n        if 'q' not in self.override_flags:\n            self.argparser.add_argument(\n                '-q', '--quotechar', dest='quotechar',\n                help='Character used to quote strings in the input CSV file.')\n        if 'u' not in self.override_flags:\n            self.argparser.add_argument(\n                '-u', '--quoting', dest='quoting', type=int, choices=[0, 1, 2, 3],\n                help='Quoting style used in the input CSV file. 0 = Quote Minimal, 1 = Quote All, '\n                     '2 = Quote Non-numeric, 3 = Quote None.')\n        if 'b' not in self.override_flags:\n            self.argparser.add_argument(\n                '-b', '--no-doublequote', dest='doublequote', action='store_false',\n                help='Whether or not double quotes are doubled in the input CSV file.')\n        if 'p' not in self.override_flags:\n            self.argparser.add_argument(\n                '-p', '--escapechar', dest='escapechar',\n                help='Character used to escape the delimiter if --quoting 3 (\"Quote None\") is specified and to escape '\n                     'the QUOTECHAR if --no-doublequote is specified.')\n        if 'z' not in self.override_flags:\n            self.argparser.add_argument(\n                '-z', '--maxfieldsize', dest='field_size_limit', type=int,\n                help='Maximum length of a single field in the input CSV file.')\n        if 'e' not in self.override_flags:\n            self.argparser.add_argument(\n                '-e', '--encoding', dest='encoding', default='utf-8-sig',\n                help='Specify the encoding of the input CSV file.')\n        if 'L' not in self.override_flags:\n            self.argparser.add_argument(\n                '-L', '--locale', dest='locale', default='en_US',\n                help='Specify the locale (en_US) of any formatted numbers.')\n        if 'S' not in self.override_flags:\n            self.argparser.add_argument(\n                '-S', '--skipinitialspace', dest='skipinitialspace', action='store_true',\n                help='Ignore whitespace immediately following the delimiter.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--blanks', dest='blanks', action='store_true',\n                help='Do not convert \"\", \"na\", \"n/a\", \"none\", \"null\", \".\" to NULL.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--null-value', dest='null_values', nargs='+', default=[],\n                help='Convert this value to NULL. --null-value can be specified multiple times.')\n        if 'date-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--date-format', dest='date_format',\n                help='Specify a strptime date format string like \"%%m/%%d/%%Y\".')\n        if 'datetime-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--datetime-format', dest='datetime_format',\n                help='Specify a strptime datetime format string like \"%%m/%%d/%%Y %%I:%%M %%p\".')\n        if 'H' not in self.override_flags:\n            self.argparser.add_argument(\n                '-H', '--no-header-row', dest='no_header_row', action='store_true',\n                help='Specify that the input CSV file has no header row. Will create default headers (a,b,c,...).')\n        if 'K' not in self.override_flags:\n            self.argparser.add_argument(\n                '-K', '--skip-lines', dest='skip_lines', type=int, default=0,\n                help='Specify the number of initial lines to skip before the header row (e.g. comments, copyright '\n                     'notices, empty rows).')\n        if 'v' not in self.override_flags:\n            self.argparser.add_argument(\n                '-v', '--verbose', dest='verbose', action='store_true',\n                help='Print detailed tracebacks when errors occur.')\n\n        # Output\n        if 'l' not in self.override_flags:\n            self.argparser.add_argument(\n                '-l', '--linenumbers', dest='line_numbers', action='store_true',\n                help='Insert a column of line numbers at the front of the output. Useful when piping to grep or as a '\n                     'simple primary key.')\n\n        # Input/Output\n        if 'zero' not in self.override_flags:\n            self.argparser.add_argument(\n                '--zero', dest='zero_based', action='store_true',\n                help='When interpreting or displaying column numbers, use zero-based numbering instead of the default '\n                     '1-based numbering.')\n\n        self.argparser.add_argument(\n            '-V', '--version', action='version', version='%(prog)s 1.3.0',\n            help='Display version information and exit.')\n\n    def _open_input_file(self, path, opened=False):\n        \"\"\"\n        Open the input file specified on the command line.\n        \"\"\"\n        if not path or path == '-':\n            # \"UnsupportedOperation: It is not possible to set the encoding or newline of stream after the first read\"\n            if not opened:\n                sys.stdin.reconfigure(encoding=self.args.encoding)\n            f = sys.stdin\n        else:\n            extension = splitext(path)[1]\n\n            if extension == '.gz':\n                func = gzip.open\n            elif extension == '.bz2':\n                func = bz2.open\n            elif extension == \".xz\":\n                func = lzma.open\n            else:\n                func = open\n\n            f = LazyFile(func, path, mode='rt', encoding=self.args.encoding)\n\n        return f\n\n    def _extract_csv_reader_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the input CSV reader(s).\n        \"\"\"\n        kwargs = {}\n\n        if self.args.tabs:\n            kwargs['delimiter'] = '\\t'\n        elif self.args.delimiter:\n            kwargs['delimiter'] = self.args.delimiter\n\n        for arg in ('quotechar', 'quoting', 'doublequote', 'escapechar', 'field_size_limit', 'skipinitialspace'):\n            value = getattr(self.args, arg)\n            if value is not None:\n                kwargs[arg] = value\n\n        if getattr(self.args, 'no_header_row', None):\n            kwargs['header'] = not self.args.no_header_row\n\n        return kwargs\n\n    def _extract_csv_writer_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the output CSV writer.\n        \"\"\"\n        kwargs = {}\n\n        if getattr(self.args, 'line_numbers', None):\n            kwargs['line_numbers'] = True\n\n        return kwargs\n\n    def _install_exception_handler(self):\n        \"\"\"\n        Installs a replacement for sys.excepthook, which handles pretty-printing uncaught exceptions.\n        \"\"\"\n        def handler(t, value, traceback):\n            if self.args.verbose:\n                sys.__excepthook__(t, value, traceback)\n            else:\n                # Special case handling for Unicode errors, which behave very strangely\n                # when cast with unicode()\n                if t == UnicodeDecodeError:\n                    sys.stderr.write('Your file is not \"%s\" encoded. Please specify the correct encoding with the -e '\n                                     'flag or with the PYTHONIOENCODING environment variable. Use the -v flag to see '\n                                     'the complete error.\\n' % self.args.encoding)\n                else:\n                    sys.stderr.write(f'{t.__name__}: {str(value)}\\n')\n\n        sys.excepthook = handler\n\n    def get_column_types(self):\n        if getattr(self.args, 'blanks', None):\n            type_kwargs = {'null_values': []}\n        else:\n            type_kwargs = {'null_values': list(DEFAULT_NULL_VALUES)}\n        for null_value in getattr(self.args, 'null_values', []):\n            type_kwargs['null_values'].append(null_value)\n\n        text_type = agate.Text(**type_kwargs)\n\n        if self.args.no_inference:\n            types = [text_type]\n        else:\n            number_type = agate.Number(locale=self.args.locale, **type_kwargs)\n\n            # See the order in the `agate.TypeTester` class.\n            types = [\n                agate.Boolean(**type_kwargs),\n                agate.TimeDelta(**type_kwargs),\n                agate.Date(date_format=self.args.date_format, **type_kwargs),\n                agate.DateTime(datetime_format=self.args.datetime_format, **type_kwargs),\n                text_type,\n            ]\n\n            # In order to parse dates like \"20010101\".\n            if self.args.date_format or self.args.datetime_format:\n                types.insert(-1, number_type)\n            else:\n                types.insert(1, number_type)\n\n        return agate.TypeTester(types=types)\n\n    def get_column_offset(self):\n        if self.args.zero_based:\n            return 0\n        return 1\n\n    def skip_lines(self):\n        if isinstance(self.args.skip_lines, int):\n            while self.args.skip_lines > 0:\n                self.input_file.readline()\n                self.args.skip_lines -= 1\n        else:\n            raise ValueError('skip_lines argument must be an int')\n\n        return self.input_file\n\n    def get_rows_and_column_names_and_column_ids(self, **kwargs):\n        rows = agate.csv.reader(self.skip_lines(), **kwargs)\n\n        try:\n            next_row = next(rows)\n        except StopIteration:\n            return iter([]), [], []\n\n        if self.args.no_header_row:\n            # Peek at a row to get the number of columns.\n            row = next_row\n            rows = itertools.chain([row], rows)\n            column_names = make_default_headers(len(row))\n        else:\n            column_names = next_row\n\n        column_offset = self.get_column_offset()\n        if kwargs.get('line_numbers'):\n            column_offset -= 1\n\n        column_ids = parse_column_identifiers(\n            self.args.columns,\n            column_names,\n            column_offset,\n            getattr(self.args, 'not_columns', None),\n        )\n\n        return rows, column_names, column_ids\n\n    def print_column_names(self):\n        \"\"\"\n        Pretty-prints the names and indices of all columns to a file-like object (usually sys.stdout).\n        \"\"\"\n        if getattr(self.args, 'no_header_row', None):\n            raise RequiredHeaderError('You cannot use --no-header-row with the -n or --names options.')\n\n        if getattr(self.args, 'zero_based', None):\n            start = 0\n        else:\n            start = 1\n\n        rows = agate.csv.reader(self.skip_lines(), **self.reader_kwargs)\n        column_names = next(rows)\n\n        for i, c in enumerate(column_names, start):\n            self.output_file.write('%3i: %s\\n' % (i, c))\n\n    def additional_input_expected(self):\n        return isatty(sys.stdin) and not self.args.input_path\n\n\ndef isatty(f):\n    try:\n        return f.isatty()\n    except ValueError:  # I/O operation on closed file\n        return False\n\n\ndef default_str_decimal(obj):\n    if isinstance(obj, (datetime.date, datetime.datetime)):\n        return obj.isoformat()\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError(f'{repr(obj)} is not JSON serializable')\n\n\ndef default_float_decimal(obj):\n    if isinstance(obj, decimal.Decimal):\n        return float(obj)\n    return default_str_decimal(obj)\n\n\ndef make_default_headers(n):\n    \"\"\"\n    Make a set of simple, default headers for files that are missing them.\n    \"\"\"\n    return tuple(agate.utils.letter_name(i) for i in range(n))\n\n\ndef match_column_identifier(column_names, c, column_offset=1):\n    \"\"\"\n    Determine what column a single column id (name or index) matches in a series of column names.\n    Note that integer values are *always* treated as positional identifiers. If you happen to have\n    column names which are also integers, you must specify them using a positional index.\n    \"\"\"\n    if isinstance(c, str) and not c.isdigit() and c in column_names:\n        return column_names.index(c)\n\n    try:\n        c = int(c) - column_offset\n    # Fail out if neither a column name nor an integer\n    except ValueError:\n        raise ColumnIdentifierError(\"Column '%s' is invalid. It is neither an integer nor a column name. \"\n                                    \"Column names are: %s\" % (c, repr(column_names)[1:-1]))\n\n    # Fail out if index is 0-based\n    if c < 0:\n        raise ColumnIdentifierError(\"Column %i is invalid. Columns are 1-based.\" % (c + column_offset))\n\n    # Fail out if index is out of range\n    if c >= len(column_names):\n        raise ColumnIdentifierError(\"Column %i is invalid. The last column is '%s' at index %i.\" % (\n            c + column_offset, column_names[-1], len(column_names) - 1 + column_offset))\n\n    return c\n\n\ndef parse_column_identifiers(ids, column_names, column_offset=1, excluded_columns=None):\n    \"\"\"\n    Parse a comma-separated list of column indices AND/OR names into a list of integer indices.\n    Ranges of integers can be specified with two integers separated by a '-' or ':' character.\n    Ranges of non-integers (e.g. column names) are not supported.\n    Note: Column indices are 1-based.\n    \"\"\"\n    if not column_names:\n        return []\n\n    if not ids and not excluded_columns:\n        return range(len(column_names))\n\n    if ids:\n        columns = []\n\n        for c in ids.split(','):\n            try:\n                columns.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names) + 1\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    columns.append(match_column_identifier(column_names, x, column_offset))\n    else:\n        columns = range(len(column_names))\n\n    excludes = []\n\n    if excluded_columns:\n        for c in excluded_columns.split(','):\n            try:\n                excludes.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names)\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    excludes.append(match_column_identifier(column_names, x, column_offset))\n\n    return [c for c in columns if c not in excludes]\n", "test_list": ["def test_before_after_insert(self):\n    self.get_output(['--db', 'sqlite:///' + self.db_file, '--insert', 'examples/dummy.csv', '--before-insert', 'SELECT 1; CREATE TABLE foobar (date DATE)', '--after-insert', 'INSERT INTO dummy VALUES (0, 5, 6)'])\n    output_file = io.StringIO()\n    utility = SQL2CSV(['--db', 'sqlite:///' + self.db_file, '--query', 'SELECT * FROM foobar'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'date\\n')\n    output_file = io.StringIO()\n    utility = SQL2CSV(['--db', 'sqlite:///' + self.db_file, '--query', 'SELECT * FROM dummy'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'a,b,c\\n1,2.0,3.0\\n0,5.0,6.0\\n')", "def test_duplicate_keys(self):\n    output_file = io.StringIO()\n    utility = CSVJSON(['-k', 'a', 'examples/dummy3.csv'], output_file)\n    self.assertRaisesRegex(ValueError, 'Value True is not unique in the key column.', utility.run)\n    output_file.close()"], "requirements": {"Input-Output Conditions": {"requirement": "The 'run' function should ensure that the input file is opened correctly when the 'f' flag is not present and that the file is closed after processing. Additionally, it should verify that the input file is of a valid CSV format.", "unit_test": "def test_input_file_handling(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertTrue(utility.input_file.closed)\n    with self.assertRaises(ValueError):\n        utility = CSVKitUtility(['examples/invalid.csv'])\n        utility.run()", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_input_file_handling"}, "Exception Handling": {"requirement": "The 'run' function should handle exceptions gracefully, providing meaningful error messages when the input file cannot be opened or processed.", "unit_test": "def test_exception_handling(self):\n    utility = CSVKitUtility(['non_existent_file.csv'])\n    with self.assertRaises(FileNotFoundError):\n        utility.run()", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_exception_handling"}, "Edge Case Handling": {"requirement": "The 'run' function should handle edge cases such as empty input files or files with only headers but no data rows.", "unit_test": "def test_edge_case_handling(self):\n    utility = CSVKitUtility(['examples/empty.csv'])\n    utility.run()\n    output_file = io.StringIO()\n    utility = CSVKitUtility(['examples/only_headers.csv'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'a,b,c\\n')", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'run' function to support additional file formats such as JSON and XML, ensuring compatibility with the existing CSV processing logic.", "unit_test": "def test_functionality_extension(self):\n    utility = CSVKitUtility(['examples/data.json'])\n    utility.run()\n    output_file = io.StringIO()\n    utility = CSVKitUtility(['examples/data.xml'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertIn('<data>', output)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'run' function and related methods have comprehensive docstrings and type annotations for all parameters and return types.", "unit_test": "def test_annotation_coverage(self):\n    self.assertTrue(hasattr(CSVKitUtility.run, '__annotations__'))\n    self.assertIn('self', CSVKitUtility.run.__annotations__)\n    self.assertIn('return', CSVKitUtility.run.__annotations__)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'run' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": "def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    with open('csvkit/cli.py') as f:\n        code = f.read()\n    complexity = cc_visit(code)\n    run_complexity = next((c for c in complexity if c.name == 'run'), None)\n    self.assertIsNotNone(run_complexity)\n    self.assertLessEqual(run_complexity.complexity, 10)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_code_complexity"}, "Code Standard": {"requirement": "The 'run' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_code_standard(self):\n    import subprocess\n    result = subprocess.run(['flake8', 'csvkit/cli.py'], capture_output=True, text=True)\n    self.assertEqual(result.returncode, 0, msg=result.stdout)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'run' function should utilize the necessary context, such as 'CSVKitUtility.args' and 'CSVKitUtility.input_file', to manage input and output operations.", "unit_test": "def test_context_usage(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertIsNotNone(utility.args)\n    self.assertIsNotNone(utility.input_file)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'run' function should correctly use the context, ensuring that 'CSVKitUtility.args' is parsed correctly and 'CSVKitUtility.input_file' is opened and closed as expected.", "unit_test": "def test_context_usage_correctness(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertEqual(utility.args.input_path, 'examples/valid.csv')\n    self.assertTrue(utility.input_file.closed)", "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_context_usage_correctness"}}}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "type": "method", "project_path": "System/prometheus-client", "completion_path": "System/prometheus-client/prometheus_client/mmap_dict.py", "signature_position": [127, 127], "body_position": [128, 131], "dependency": {"intra_class": ["prometheus_client.mmap_dict.MmapedDict._init_value", "prometheus_client.mmap_dict.MmapedDict._m", "prometheus_client.mmap_dict.MmapedDict._positions"], "intra_file": ["prometheus_client.mmap_dict._pack_two_doubles"], "cross_file": []}, "requirement": {"Functionality": "This function writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, it initializes the key and then writes the value and timestamp to the corresponding position in the memory-mapped file.", "Arguments": ":param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to write the value to.\n:param value: The value to be written.\n:param timestamp: The timestamp associated with the value.\n:return: No return values."}, "tests": ["tests/test_multiprocess.py::TestMmapedDict::test_corruption_detected", "tests/test_multiprocess.py::TestMmapedDict::test_multi_expansion", "tests/test_multiprocess.py::TestMmapedDict::test_expansion", "tests/test_multiprocess.py::TestMmapedDict::test_process_restart"], "indent": 4, "domain": "System", "code": "    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "context": "import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n\n###The function: write_value###\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "prompt": "Please write a python function called 'write_value' base the context. This function writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, it initializes the key and then writes the value and timestamp to the corresponding position in the memory-mapped file.:param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to write the value to.\n:param value: The value to be written.\n:param timestamp: The timestamp associated with the value.\n:return: No return values..\n        The context you need to refer to is as follows: import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n\n###The function: write_value###\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "test_list": ["def test_corruption_detected(self):\n    self.d.write_value('abc', 42.0, 987.0)\n    self.d._m[8:16] = b'somejunk'\n    with self.assertRaises(RuntimeError):\n        list(self.d.read_all_values())", "def test_multi_expansion(self):\n    key = 'a' * mmap_dict._INITIAL_MMAP_SIZE * 4\n    self.d.write_value('abc', 42.0, 987.0)\n    self.d.write_value(key, 123.0, 876.0)\n    self.d.write_value('def', 17.0, 765.0)\n    self.assertEqual([('abc', 42.0, 987.0), (key, 123.0, 876.0), ('def', 17.0, 765.0)], list(self.d.read_all_values()))", "def test_expansion(self):\n    key = 'a' * mmap_dict._INITIAL_MMAP_SIZE\n    self.d.write_value(key, 123.0, 987.0)\n    self.assertEqual([(key, 123.0, 987.0)], list(self.d.read_all_values()))", "def test_process_restart(self):\n    self.d.write_value('abc', 123.0, 987.0)\n    self.d.close()\n    self.d = mmap_dict.MmapedDict(self.tempfile)\n    self.assertEqual((123, 987.0), self.d.read_value('abc'))\n    self.assertEqual([('abc', 123.0, 987.0)], list(self.d.read_all_values()))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'write_value' function should correctly write a float value and a timestamp to the specified key in the MmapedDict instance, and the key should be a string.", "unit_test": "def test_write_value_correctness(self):\n    self.d.write_value('test_key', 100.0, 1609459200.0)\n    value, timestamp = self.d.read_value('test_key')\n    self.assertEqual(value, 100.0)\n    self.assertEqual(timestamp, 1609459200.0)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_correctness"}, "Exception Handling": {"requirement": "The 'write_value' function should raise a TypeError if the key is not a string or if the value or timestamp is not a float.", "unit_test": "def test_write_value_type_error(self):\n    with self.assertRaises(TypeError):\n        self.d.write_value(123, 100.0, 1609459200.0)\n    with self.assertRaises(TypeError):\n        self.d.write_value('test_key', '100', 1609459200.0)\n    with self.assertRaises(TypeError):\n        self.d.write_value('test_key', 100.0, '1609459200')", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_type_error"}, "Edge Case Handling": {"requirement": "The 'write_value' function should handle writing to a key with an empty string and ensure it does not corrupt the data.", "unit_test": "def test_write_value_empty_key(self):\n    self.d.write_value('', 50.0, 1609459200.0)\n    value, timestamp = self.d.read_value('')\n    self.assertEqual(value, 50.0)\n    self.assertEqual(timestamp, 1609459200.0)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_empty_key"}, "Functionality Extension": {"requirement": "Extend the 'write_value' function to return a boolean indicating whether the key was newly initialized.", "unit_test": "def test_write_value_initialization_flag(self):\n    is_new = self.d.write_value('new_key', 200.0, 1609459200.0)\n    self.assertTrue(is_new)\n    is_new = self.d.write_value('new_key', 300.0, 1609459300.0)\n    self.assertFalse(is_new)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_initialization_flag"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'write_value' function are properly annotated with type hints.", "unit_test": "def test_write_value_annotations(self):\n    annotations = self.d.write_value.__annotations__\n    self.assertEqual(annotations['key'], str)\n    self.assertEqual(annotations['value'], float)\n    self.assertEqual(annotations['timestamp'], float)\n    self.assertEqual(annotations['return'], bool)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_annotations"}, "Code Complexity": {"requirement": "The 'write_value' function should maintain a cyclomatic complexity of no more than 5.", "unit_test": "def test_write_value_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(self.d.write_value)\n    complexity = cc_visit(source_code)\n    self.assertTrue(all(c.cyclomatic_complexity <= 5 for c in complexity))", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_complexity"}, "Code Standard": {"requirement": "The 'write_value' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_write_value_code_style(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['mmap_dict.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_code_style"}, "Context Usage Verification": {"requirement": "The 'write_value' function should utilize the '_m', and '_positions' attributes of the MmapedDict class.", "unit_test": "def test_write_value_context_usage(self):\n    self.d.write_value('context_key', 150.0, 1609459200.0)\n    self.assertIn('context_key', self.d._positions)\n    pos = self.d._positions['context_key']\n    value, timestamp = _unpack_two_doubles(self.d._m, pos)\n    self.assertEqual(value, 150.0)\n    self.assertEqual(timestamp, 1609459200.0)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'write_value' function correctly updates the '_positions' dictionary and writes the value and timestamp to the correct position in the '_m' memory map.", "unit_test": "def test_write_value_correct_position_update(self):\n    self.d.write_value('correct_key', 250.0, 1609459200.0)\n    pos = self.d._positions['correct_key']\n    value, timestamp = _unpack_two_doubles(self.d._m, pos)\n    self.assertEqual(value, 250.0)\n    self.assertEqual(timestamp, 1609459200.0)", "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_correct_position_update"}}}
{"namespace": "mopidy.config.types.LogLevel.serialize", "type": "method", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/config/types.py", "signature_position": [372, 372], "body_position": [373, 376], "dependency": {"intra_class": ["mopidy.config.types.LogLevel.levels"], "intra_file": ["mopidy.config.types.encode"], "cross_file": []}, "requirement": {"Functionality": "Serialize a value based on the LogLevel instance. It looks up the value in the levels dictionary and returns the corresponding key. If the value is not found, it returns an empty string.", "Arguments": ":param self: LogLevel. An instance of the LogLevel class.\n:param value: The value to be serialized.\n:param display: Bool. Whether to display the serialized value. Defaults to False.\n:return: String. The serialized value or an empty string if the value is not found."}, "tests": ["tests/config/test_types.py::TestLogLevel::test_serialize", "tests/config/test_types.py::TestLogLevel::test_serialize_ignores_unknown_level"], "indent": 4, "domain": "Multimedia", "code": "    def serialize(self, value, display=False):\n        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"\n", "context": "import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n    def deserialize(self, value):\n        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n###The function: serialize###\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n", "prompt": "Please write a python function called 'serialize' base the context. Serialize a value based on the LogLevel instance. It looks up the value in the levels dictionary and returns the corresponding key. If the value is not found, it returns an empty string.:param self: LogLevel. An instance of the LogLevel class.\n:param value: The value to be serialized.\n:param display: Bool. Whether to display the serialized value. Defaults to False.\n:return: String. The serialized value or an empty string if the value is not found..\n        The context you need to refer to is as follows: import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n    def deserialize(self, value):\n        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n###The function: serialize###\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n", "test_list": ["def test_serialize(self):\n    cv = types.LogLevel()\n    for name, level in self.levels.items():\n        assert cv.serialize(level) == name", "def test_serialize_ignores_unknown_level(self):\n    cv = types.LogLevel()\n    assert cv.serialize(1337) == ''"], "requirements": {"Input-Output Conditions": {"requirement": "The 'serialize' function should return the correct string representation of a log level when given a valid integer log level value.", "unit_test": "def test_serialize_valid_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize(logging.INFO) == 'info'\n    assert cv.serialize(logging.DEBUG) == 'debug'", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_valid_input"}, "Exception Handling": {"requirement": "The 'serialize' function should handle non-integer inputs gracefully by returning an empty string.", "unit_test": "def test_serialize_non_integer_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize('not_an_integer') == ''\n    assert cv.serialize(None) == ''", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_non_integer_input"}, "Edge Case Handling": {"requirement": "The 'serialize' function should return an empty string for integer values that are not defined in the levels dictionary.", "unit_test": "def test_serialize_edge_case_unknown_level(self):\n    cv = types.LogLevel()\n    assert cv.serialize(-1) == ''\n    assert cv.serialize(9999) == ''", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_edge_case_unknown_level"}, "Functionality Extension": {"requirement": "Extend the 'serialize' function to accept log level names as input and return the corresponding integer value.", "unit_test": "def test_serialize_with_name_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize('info') == logging.INFO\n    assert cv.serialize('debug') == logging.DEBUG", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_with_name_input"}, "Annotation Coverage": {"requirement": "Ensure that the 'serialize' function has complete type annotations for all parameters and return types.", "unit_test": "def test_serialize_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(types.LogLevel.serialize)\n    assert hints['value'] == int\n    assert hints['display'] == bool\n    assert hints['return'] == str", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_annotations"}, "Code Complexity": {"requirement": "The 'serialize' function should maintain a cyclomatic complexity of 3, indicating a simple function structure.", "unit_test": "def test_serialize_cyclomatic_complexity(self):\n    import radon.complexity as cc\n    complexity = cc.cc_visit_func(types.LogLevel.serialize)\n    assert complexity[0].complexity == 1", "test": "tests/config/test_types.py::TestLogLevel::test_code_complexity"}, "Code Standard": {"requirement": "The 'serialize' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_serialize_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/your/module.py'])\n    assert result.total_errors == 0", "test": "tests/config/test_types.py::TestLogLevel::test_code_style"}, "Context Usage Verification": {"requirement": "The 'serialize' function should utilize the 'levels' dictionary from the 'LogLevel' class context.", "unit_test": "def test_serialize_uses_levels_dict(self):\n    cv = types.LogLevel()\n    assert 'levels' in cv.__class__.__dict__", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_log_level"}, "Context Usage Correctness Verification": {"requirement": "The 'serialize' function should correctly map integer log levels to their string representations using the 'levels' dictionary.", "unit_test": "def test_serialize_correct_mapping(self):\n    cv = types.LogLevel()\n    for name, level in cv.levels.items():\n        assert cv.serialize(level) == name", "test": "tests/config/test_types.py::TestLogLevel::test_serialize_maps_level_to_string"}}}
{"namespace": "playhouse.kv.KeyValue.pop", "type": "method", "project_path": "Software-Development/peewee", "completion_path": "Software-Development/peewee/playhouse/kv.py", "signature_position": [163, 163], "body_position": [164, 173], "dependency": {"intra_class": ["playhouse.kv.KeyValue._database"], "intra_file": ["playhouse.kv.Sentinel"], "cross_file": []}, "requirement": {"Functionality": "This function removes the specified key from the KeyValue instance and returns the corresponding value. If the key is not found and no default value is provided, an exception is raised. The function also ensures that the operation is atomic by using a database transaction.", "Arguments": ":param self: KeyValue. An instance of the KeyValue class.\n:param key: The key to be removed from the instance.\n:param default: Optional. The value to be returned if the key is not found. Defaults to Sentinel.\n:return: The value corresponding to the key, or the default value if provided."}, "tests": ["tests/kv.py::TestKeyValue::test_basic_apis"], "indent": 4, "domain": "Software-Development", "code": "    def pop(self, key, default=Sentinel):\n        with self._database.atomic():\n            try:\n                result = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default\n            del self[key]\n\n        return result\n", "context": "import operator\n\nfrom peewee import *\nfrom peewee import sqlite3\nfrom peewee import Expression\nfrom playhouse.fields import PickleField\ntry:\n    from playhouse.sqlite_ext import CSqliteExtDatabase as SqliteExtDatabase\nexcept ImportError:\n    from playhouse.sqlite_ext import SqliteExtDatabase\n\n\nSentinel = type('Sentinel', (object,), {})\n\n\nclass KeyValue(object):\n    \"\"\"\n    Persistent dictionary.\n\n    :param Field key_field: field to use for key. Defaults to CharField.\n    :param Field value_field: field to use for value. Defaults to PickleField.\n    :param bool ordered: data should be returned in key-sorted order.\n    :param Database database: database where key/value data is stored.\n    :param str table_name: table name for data.\n    \"\"\"\n    def __init__(self, key_field=None, value_field=None, ordered=False,\n                 database=None, table_name='keyvalue'):\n        if key_field is None:\n            key_field = CharField(max_length=255, primary_key=True)\n        if not key_field.primary_key:\n            raise ValueError('key_field must have primary_key=True.')\n\n        if value_field is None:\n            value_field = PickleField()\n\n        self._key_field = key_field\n        self._value_field = value_field\n        self._ordered = ordered\n        self._database = database or SqliteExtDatabase(':memory:')\n        self._table_name = table_name\n        support_on_conflict = (isinstance(self._database, PostgresqlDatabase) or\n                              (isinstance(self._database, SqliteDatabase) and\n                               self._database.server_version >= (3, 24)))\n        if support_on_conflict:\n            self.upsert = self._postgres_upsert\n            self.update = self._postgres_update\n        else:\n            self.upsert = self._upsert\n            self.update = self._update\n\n        self.model = self.create_model()\n        self.key = self.model.key\n        self.value = self.model.value\n\n        # Ensure table exists.\n        self.model.create_table()\n\n    def create_model(self):\n        class KeyValue(Model):\n            key = self._key_field\n            value = self._value_field\n            class Meta:\n                database = self._database\n                table_name = self._table_name\n        return KeyValue\n\n    def query(self, *select):\n        query = self.model.select(*select).tuples()\n        if self._ordered:\n            query = query.order_by(self.key)\n        return query\n\n    def convert_expression(self, expr):\n        if not isinstance(expr, Expression):\n            return (self.key == expr), True\n        return expr, False\n\n    def __contains__(self, key):\n        expr, _ = self.convert_expression(key)\n        return self.model.select().where(expr).exists()\n\n    def __len__(self):\n        return len(self.model)\n\n    def __getitem__(self, expr):\n        converted, is_single = self.convert_expression(expr)\n        query = self.query(self.value).where(converted)\n        item_getter = operator.itemgetter(0)\n        result = [item_getter(row) for row in query]\n        if len(result) == 0 and is_single:\n            raise KeyError(expr)\n        elif is_single:\n            return result[0]\n        return result\n\n    def _upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict('replace')\n         .execute())\n\n    def _postgres_upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict(conflict_target=[self.key],\n                      preserve=[self.value])\n         .execute())\n\n    def __setitem__(self, expr, value):\n        if isinstance(expr, Expression):\n            self.model.update(value=value).where(expr).execute()\n        else:\n            self.upsert(expr, value)\n\n    def __delitem__(self, expr):\n        converted, _ = self.convert_expression(expr)\n        self.model.delete().where(converted).execute()\n\n    def __iter__(self):\n        return iter(self.query().execute())\n\n    def keys(self):\n        return map(operator.itemgetter(0), self.query(self.key))\n\n    def values(self):\n        return map(operator.itemgetter(0), self.query(self.value))\n\n    def items(self):\n        return iter(self.query().execute())\n\n    def _update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict('replace')\n                .execute())\n\n    def _postgres_update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict(conflict_target=[self.key],\n                             preserve=[self.value])\n                .execute())\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n###The function: pop###\n    def clear(self):\n        self.model.delete().execute()\n", "prompt": "Please write a python function called 'pop' base the context. This function removes the specified key from the KeyValue instance and returns the corresponding value. If the key is not found and no default value is provided, an exception is raised. The function also ensures that the operation is atomic by using a database transaction.:param self: KeyValue. An instance of the KeyValue class.\n:param key: The key to be removed from the instance.\n:param default: Optional. The value to be returned if the key is not found. Defaults to Sentinel.\n:return: The value corresponding to the key, or the default value if provided..\n        The context you need to refer to is as follows: import operator\n\nfrom peewee import *\nfrom peewee import sqlite3\nfrom peewee import Expression\nfrom playhouse.fields import PickleField\ntry:\n    from playhouse.sqlite_ext import CSqliteExtDatabase as SqliteExtDatabase\nexcept ImportError:\n    from playhouse.sqlite_ext import SqliteExtDatabase\n\n\nSentinel = type('Sentinel', (object,), {})\n\n\nclass KeyValue(object):\n    \"\"\"\n    Persistent dictionary.\n\n    :param Field key_field: field to use for key. Defaults to CharField.\n    :param Field value_field: field to use for value. Defaults to PickleField.\n    :param bool ordered: data should be returned in key-sorted order.\n    :param Database database: database where key/value data is stored.\n    :param str table_name: table name for data.\n    \"\"\"\n    def __init__(self, key_field=None, value_field=None, ordered=False,\n                 database=None, table_name='keyvalue'):\n        if key_field is None:\n            key_field = CharField(max_length=255, primary_key=True)\n        if not key_field.primary_key:\n            raise ValueError('key_field must have primary_key=True.')\n\n        if value_field is None:\n            value_field = PickleField()\n\n        self._key_field = key_field\n        self._value_field = value_field\n        self._ordered = ordered\n        self._database = database or SqliteExtDatabase(':memory:')\n        self._table_name = table_name\n        support_on_conflict = (isinstance(self._database, PostgresqlDatabase) or\n                              (isinstance(self._database, SqliteDatabase) and\n                               self._database.server_version >= (3, 24)))\n        if support_on_conflict:\n            self.upsert = self._postgres_upsert\n            self.update = self._postgres_update\n        else:\n            self.upsert = self._upsert\n            self.update = self._update\n\n        self.model = self.create_model()\n        self.key = self.model.key\n        self.value = self.model.value\n\n        # Ensure table exists.\n        self.model.create_table()\n\n    def create_model(self):\n        class KeyValue(Model):\n            key = self._key_field\n            value = self._value_field\n            class Meta:\n                database = self._database\n                table_name = self._table_name\n        return KeyValue\n\n    def query(self, *select):\n        query = self.model.select(*select).tuples()\n        if self._ordered:\n            query = query.order_by(self.key)\n        return query\n\n    def convert_expression(self, expr):\n        if not isinstance(expr, Expression):\n            return (self.key == expr), True\n        return expr, False\n\n    def __contains__(self, key):\n        expr, _ = self.convert_expression(key)\n        return self.model.select().where(expr).exists()\n\n    def __len__(self):\n        return len(self.model)\n\n    def __getitem__(self, expr):\n        converted, is_single = self.convert_expression(expr)\n        query = self.query(self.value).where(converted)\n        item_getter = operator.itemgetter(0)\n        result = [item_getter(row) for row in query]\n        if len(result) == 0 and is_single:\n            raise KeyError(expr)\n        elif is_single:\n            return result[0]\n        return result\n\n    def _upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict('replace')\n         .execute())\n\n    def _postgres_upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict(conflict_target=[self.key],\n                      preserve=[self.value])\n         .execute())\n\n    def __setitem__(self, expr, value):\n        if isinstance(expr, Expression):\n            self.model.update(value=value).where(expr).execute()\n        else:\n            self.upsert(expr, value)\n\n    def __delitem__(self, expr):\n        converted, _ = self.convert_expression(expr)\n        self.model.delete().where(converted).execute()\n\n    def __iter__(self):\n        return iter(self.query().execute())\n\n    def keys(self):\n        return map(operator.itemgetter(0), self.query(self.key))\n\n    def values(self):\n        return map(operator.itemgetter(0), self.query(self.value))\n\n    def items(self):\n        return iter(self.query().execute())\n\n    def _update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict('replace')\n                .execute())\n\n    def _postgres_update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict(conflict_target=[self.key],\n                             preserve=[self.value])\n                .execute())\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n###The function: pop###\n    def clear(self):\n        self.model.delete().execute()\n", "test_list": ["def test_basic_apis(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    KV['k2'] = [0, 1, 2]\n    self.assertEqual(KV['k1'], 'v1')\n    self.assertEqual(KV['k2'], [0, 1, 2])\n    self.assertRaises(KeyError, lambda: KV['k3'])\n    self.assertTrue((KV.key < 'k2') in KV)\n    self.assertFalse((KV.key > 'k2') in KV)\n    del KV['k1']\n    KV['k3'] = 'v3'\n    self.assertFalse('k1' in KV)\n    self.assertTrue('k3' in KV)\n    self.assertEqual(sorted(KV.keys()), ['k2', 'k3'])\n    self.assertEqual(len(KV), 2)\n    data = dict(KV)\n    self.assertEqual(data, {'k2': [0, 1, 2], 'k3': 'v3'})\n    self.assertEqual(dict(KV), dict(KV.items()))\n    self.assertEqual(KV.pop('k2'), [0, 1, 2])\n    self.assertRaises(KeyError, lambda: KV['k2'])\n    self.assertRaises(KeyError, KV.pop, 'k2')\n    self.assertEqual(KV.get('k3'), 'v3')\n    self.assertTrue(KV.get('kx') is None)\n    self.assertEqual(KV.get('kx', 'vx'), 'vx')\n    self.assertTrue(KV.get('k4') is None)\n    self.assertEqual(KV.setdefault('k4', 'v4'), 'v4')\n    self.assertEqual(KV.get('k4'), 'v4')\n    self.assertEqual(KV.get('k4', 'v5'), 'v4')\n    KV.clear()\n    self.assertEqual(len(KV), 0)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'pop' function should correctly remove the specified key from the KeyValue instance and return the corresponding value. If a default value is provided and the key is not found, the default value should be returned.", "unit_test": "def test_pop_with_default(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    self.assertEqual(KV.pop('k1'), 'v1')\n    self.assertEqual(KV.pop('k2', 'default'), 'default')\n    self.assertRaises(KeyError, KV.pop, 'k2')", "test": "tests/kv.py::TestKeyValue::test_pop_with_default"}, "Exception Handling": {"requirement": "The 'pop' function should raise a KeyError exception with an appropriate error message: 'non_existent_key' if the specified key is not found and no default value is provided.", "unit_test": "def test_pop_key_error(self):\n    KV = self.create_kv()\n    with self.assertRaises(KeyError) as context:\n        KV.pop('non_existent_key')\n    self.assertEqual(str(context.exception), \"'non_existent_key'\")", "test": "tests/kv.py::TestKeyValue::test_pop_key_error"}, "Edge Case Handling": {"requirement": "The 'pop' function should handle edge cases such as popping from an empty KeyValue instance and ensure that the operation behaves as expected.", "unit_test": "def test_pop_from_empty(self):\n    KV = self.create_kv()\n    self.assertRaises(KeyError, KV.pop, 'any_key')\n    self.assertEqual(KV.pop('any_key', 'default'), 'default')", "test": "tests/kv.py::TestKeyValue::test_pop_from_empty"}, "Functionality Extension": {"requirement": "Extend the 'pop' function to allow popping multiple keys at once, returning a dictionary of key-value pairs for the keys that were successfully removed.", "unit_test": "def test_pop_multiple_keys(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    KV['k2'] = 'v2'\n    KV['k3'] = 'v3'\n    result = KV.pop(['k1', 'k2'])\n    self.assertEqual(result, {'k1': 'v1', 'k2': 'v2'})\n    self.assertFalse('k1' in KV)\n    self.assertFalse('k2' in KV)\n    self.assertTrue('k3' in KV)", "test": "tests/kv.py::TestKeyValue::test_pop_multiple_keys"}, "Annotation Coverage": {"requirement": "Ensure that the 'pop' function has complete annotation coverage, including parameter types and return type.", "unit_test": "def test_pop_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(KeyValue.pop)\n    self.assertEqual(hints['key'], 'str')\n    self.assertEqual(hints['default'], 'Optional[Any]')\n    self.assertEqual(hints['return'], 'Any')", "test": "tests/kv.py::TestKeyValue::test_pop_annotations"}, "Code Complexity": {"requirement": "The 'pop' function should maintain a cyclomatic complexity of 3 or less to ensure readability and maintainability.", "unit_test": "def test_pop_complexity(self):\n    import radon.complexity as cc\n    source = inspect.getsource(KeyValue.pop)\n    complexity = cc.cc_visit(source)\n    self.assertTrue(all(block.complexity <= 3 for block in complexity))", "test": "tests/kv.py::TestKeyValue::test_code_complexity"}, "Code Standard": {"requirement": "The 'pop' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_pop_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_keyvalue_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/kv.py::TestKeyValue::test_code_style"}, "Context Usage Verification": {"requirement": "The 'pop' function should utilize the database transaction context to ensure atomicity of the operation.", "unit_test": "def test_pop_transaction_usage(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    with KV._database.atomic() as txn:\n        KV.pop('k1')\n    self.assertFalse('k1' in KV)", "test": "tests/kv.py::TestKeyValue::test_pop_transaction_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'pop' function correctly uses the database context to ensure that the operation is atomic and consistent.", "unit_test": "def test_pop_atomicity(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    with KV._database.atomic() as txn:\n        KV.pop('k1')\n        self.assertFalse('k1' in KV)\n    self.assertRaises(KeyError, lambda: KV['k1'])", "test": "tests/kv.py::TestKeyValue::test_pop_atomicity"}}}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "type": "method", "project_path": "Security/asyncssh", "completion_path": "Security/asyncssh/asyncssh/public_key.py", "signature_position": [624, 624], "body_position": [634, 637], "dependency": {"intra_class": ["asyncssh.public_key.SSHKey._comment", "asyncssh.public_key.SSHKey._filename", "asyncssh.public_key.SSHKey.public_data", "asyncssh.public_key.SSHKey.set_comment", "asyncssh.public_key.SSHKey.set_filename"], "intra_file": ["asyncssh.public_key.decode_ssh_public_key"], "cross_file": []}, "requirement": {"Functionality": "This method converts an SSHKey object that contains a private key into one that contains only the corresponding public key. It first decodes asymmetric encryption. Once decrypted, it proceeds to assign a relevant comment and filename to the associated key. Upon completion of these steps, the method returns the processed data as its final output.", "Arguments": ":param self: SSHKey. An instance of the SSHKey class.\n:return: SSHKey. The SSHKey object that contains only the corresponding public key."}, "tests": ["tests/test_agent.py::_TestAgent::test_lock", "tests/test_public_key.py::_TestPublicKeyTopLevel::test_rsa_encrypt_error", "tests/test_public_key.py::_TestPublicKeyTopLevel::test_generate_errors", "tests/test_agent.py::_TestAgent::test_reconnect", "tests/test_agent.py::_TestAgent::test_sign"], "indent": 4, "domain": "Security", "code": "    def convert_to_public(self) -> 'SSHKey':\n        \"\"\"Return public key corresponding to this key\n\n           This method converts an :class:`SSHKey` object which contains\n           a private key into one which contains only the corresponding\n           public key. If it is called on something which is already\n           a public key, it has no effect.\n\n        \"\"\"\n\n        result = decode_ssh_public_key(self.public_data)\n        result.set_comment(self._comment)\n        result.set_filename(self._filename)\n        return result\n", "context": "# Copyright (c) 2013-2023 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH asymmetric encryption handlers\"\"\"\n\nimport binascii\nimport os\nimport re\nimport time\n\nfrom datetime import datetime\nfrom hashlib import md5, sha1, sha256, sha384, sha512\nfrom pathlib import Path, PurePath\nfrom typing import Callable, Dict, List, Mapping, Optional, Sequence, Set\nfrom typing import Tuple, Type, Union, cast\nfrom typing_extensions import Protocol\n\nfrom .crypto import ed25519_available, ed448_available\nfrom .encryption import Encryption\nfrom .sk import sk_available\n\ntry:\n    # pylint: disable=unused-import\n    from .crypto import X509Certificate\n    from .crypto import generate_x509_certificate, import_x509_certificate\n    _x509_available = True\nexcept ImportError: # pragma: no cover\n    _x509_available = False\n\ntry:\n    import bcrypt\n    _bcrypt_available = hasattr(bcrypt, 'kdf')\nexcept ImportError: # pragma: no cover\n    _bcrypt_available = False\n\nfrom .asn1 import ASN1DecodeError, BitString, ObjectIdentifier\nfrom .asn1 import der_encode, der_decode, der_decode_partial\nfrom .crypto import CryptoKey, PyCAKey\nfrom .encryption import get_encryption_params, get_encryption\nfrom .misc import BytesOrStr, DefTuple, FilePath, IPNetwork\nfrom .misc import ip_network, read_file, write_file, parse_time_interval\nfrom .packet import NameList, String, UInt32, UInt64\nfrom .packet import PacketDecodeError, SSHPacket\nfrom .pbe import KeyEncryptionError, pkcs1_encrypt, pkcs8_encrypt\nfrom .pbe import pkcs1_decrypt, pkcs8_decrypt\nfrom .sk import SSH_SK_USER_PRESENCE_REQD, sk_get_resident\n\n\n_Comment = Optional[BytesOrStr]\n_CertPrincipals = Union[str, Sequence[str]]\n_Time = Union[int, float, datetime, str]\n\n_PubKeyAlgMap = Dict[bytes, Type['SSHKey']]\n_CertAlgMap = Dict[bytes, Tuple[Optional[Type['SSHKey']],\n                                Type['SSHCertificate']]]\n_CertSigAlgMap = Dict[bytes, bytes]\n_CertVersionMap = Dict[Tuple[bytes, int],\n                       Tuple[bytes, Type['SSHOpenSSHCertificate']]]\n\n_PEMMap = Dict[bytes, Type['SSHKey']]\n_PKCS8OIDMap = Dict[ObjectIdentifier, Type['SSHKey']]\n_SKAlgMap = Dict[int, Tuple[Type['SSHKey'], Tuple[object, ...]]]\n\n_OpenSSHCertOptions = Dict[str, object]\n_OpenSSHCertParams = Tuple[object, int, int, bytes, bytes,\n                           int, int, bytes, bytes]\n_OpenSSHCertEncoders = Sequence[Tuple[str, Callable[[object], bytes]]]\n_OpenSSHCertDecoders = Dict[bytes, Callable[[SSHPacket], object]]\n\nX509CertPurposes = Union[None, str, Sequence[str]]\n\n_IdentityArg = Union[bytes, FilePath, 'SSHKey', 'SSHCertificate']\nIdentityListArg = Union[_IdentityArg, Sequence[_IdentityArg]]\n_KeyArg = Union[bytes, FilePath, 'SSHKey']\nKeyListArg = Union[FilePath, Sequence[_KeyArg]]\n_CertArg = Union[bytes, FilePath, 'SSHCertificate']\nCertListArg = Union[_CertArg, Sequence[_CertArg]]\n_KeyPairArg = Union['SSHKeyPair', _KeyArg, Tuple[_KeyArg, _CertArg]]\nKeyPairListArg = Union[_KeyPairArg, Sequence[_KeyPairArg]]\n\n\n# Default file names in .ssh directory to read private keys from\n_DEFAULT_KEY_FILES = (\n    ('id_ed25519_sk', ed25519_available and sk_available),\n    ('id_ecdsa_sk', sk_available),\n    ('id_ed448', ed448_available),\n    ('id_ed25519', ed25519_available),\n    ('id_ecdsa', True),\n    ('id_rsa', True),\n    ('id_dsa', True)\n)\n\n# Default directories and file names to read host keys from\n_DEFAULT_HOST_KEY_DIRS = ('/opt/local/etc', '/opt/local/etc/ssh',\n                          '/usr/local/etc', '/usr/local/etc/ssh',\n                          '/etc', '/etc/ssh')\n_DEFAULT_HOST_KEY_FILES = ('ssh_host_ed448_key', 'ssh_host_ed25519_key',\n                           'ssh_host_ecdsa_key', 'ssh_host_rsa_key',\n                           'ssh_host_dsa_key')\n\n_hashes = {'md5': md5, 'sha1': sha1, 'sha256': sha256,\n           'sha384': sha384, 'sha512': sha512}\n\n_public_key_algs: List[bytes] = []\n_default_public_key_algs: List[bytes] = []\n\n_certificate_algs: List[bytes] = []\n_default_certificate_algs: List[bytes] = []\n\n_x509_certificate_algs: List[bytes] = []\n_default_x509_certificate_algs: List[bytes] = []\n\n_public_key_alg_map: _PubKeyAlgMap = {}\n_certificate_alg_map: _CertAlgMap = {}\n_certificate_sig_alg_map: _CertSigAlgMap = {}\n_certificate_version_map: _CertVersionMap = {}\n_pem_map: _PEMMap = {}\n_pkcs8_oid_map: _PKCS8OIDMap = {}\n_sk_alg_map: _SKAlgMap = {}\n\n_abs_date_pattern = re.compile(r'\\d{8}')\n_abs_time_pattern = re.compile(r'\\d{14}')\n\n_subject_pattern = re.compile(r'(?:Distinguished[ -_]?Name|Subject|DN)[=:]?\\s?',\n                              re.IGNORECASE)\n\n# SSH certificate types\nCERT_TYPE_USER = 1\nCERT_TYPE_HOST = 2\n\n# Flag to omit second argument in alg_params\nOMIT = object()\n\n_OPENSSH_KEY_V1 = b'openssh-key-v1\\0'\n_OPENSSH_SALT_LEN = 16\n_OPENSSH_WRAP_LEN = 70\n\n\ndef _parse_time(t: _Time) -> int:\n    \"\"\"Parse a time value\"\"\"\n\n    if isinstance(t, int):\n        return t\n    elif isinstance(t, float):\n        return int(t)\n    elif isinstance(t, datetime):\n        return int(t.timestamp())\n    elif isinstance(t, str):\n        if t == 'now':\n            return int(time.time())\n\n        match = _abs_date_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d').timestamp())\n\n        match = _abs_time_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d%H%M%S').timestamp())\n\n        try:\n            return int(time.time() + parse_time_interval(t))\n        except ValueError:\n            pass\n\n    raise ValueError('Unrecognized time value')\n\n\ndef _wrap_base64(data: bytes, wrap: int = 64) -> bytes:\n    \"\"\"Break a Base64 value into multiple lines.\"\"\"\n\n    data = binascii.b2a_base64(data)[:-1]\n    return b'\\n'.join(data[i:i+wrap]\n                      for i in range(0, len(data), wrap)) + b'\\n'\n\n\nclass KeyGenerationError(ValueError):\n    \"\"\"Key generation error\n\n       This exception is raised by :func:`generate_private_key`,\n       :meth:`generate_user_certificate() <SSHKey.generate_user_certificate>`\n       or :meth:`generate_host_certificate()\n       <SSHKey.generate_host_certificate>` when the requested parameters are\n       unsupported.\n\n    \"\"\"\n\n\nclass KeyImportError(ValueError):\n    \"\"\"Key import error\n\n       This exception is raised by key import functions when the\n       data provided cannot be imported as a valid key.\n\n    \"\"\"\n\n\nclass KeyExportError(ValueError):\n    \"\"\"Key export error\n\n       This exception is raised by key export functions when the\n       requested format is unknown or encryption is requested for a\n       format which doesn't support it.\n\n    \"\"\"\n\n\nclass SigningKey(Protocol):\n    \"\"\"Protocol for signing a block of data\"\"\"\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with a private key\"\"\"\n\n\nclass VerifyingKey(Protocol):\n    \"\"\"Protocol for verifying a signature on a block of data\"\"\"\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify a signature on a block of data with a public key\"\"\"\n\n\nclass SSHKey:\n    \"\"\"Parent class which holds an asymmetric encryption key\"\"\"\n\n    algorithm: bytes = b''\n    sig_algorithms: Sequence[bytes] = ()\n    cert_algorithms: Sequence[bytes] = ()\n    x509_algorithms: Sequence[bytes] = ()\n    all_sig_algorithms: Set[bytes] = set()\n    default_x509_hash: str = ''\n    pem_name: bytes = b''\n    pkcs8_oid: Optional[ObjectIdentifier] = None\n    use_executor: bool = False\n\n    def __init__(self, key: Optional[CryptoKey] = None):\n        self._key = key\n        self._comment: Optional[bytes] = None\n        self._filename: Optional[bytes] = None\n        self._touch_required = False\n\n    @classmethod\n    def generate(cls, algorithm: bytes, **kwargs) -> 'SSHKey':\n        \"\"\"Generate a new SSH private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_private(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_public(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a public key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def decode_pkcs1_private(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs1_public(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_private(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_public(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    @classmethod\n    def decode_ssh_private(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format private key\"\"\"\n\n    @classmethod\n    def decode_ssh_public(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format public key\"\"\"\n\n    @property\n    def private_data(self) -> bytes:\n        \"\"\"Return private key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_private()\n\n    @property\n    def public_data(self) -> bytes:\n        \"\"\"Return public key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_public()\n\n    @property\n    def pyca_key(self) -> PyCAKey:\n        \"\"\"Return PyCA key for use in X.509 module\"\"\"\n\n        assert self._key is not None\n        return self._key.pyca_key\n\n    def _generate_certificate(self, key: 'SSHKey', version: int, serial: int,\n                              cert_type: int, key_id: str,\n                              principals: _CertPrincipals,\n                              valid_after: _Time, valid_before: _Time,\n                              cert_options: _OpenSSHCertOptions,\n                              sig_alg_name: DefTuple[str],\n                              comment: DefTuple[_Comment]) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        if isinstance(principals, str):\n            principals = [p.strip() for p in principals.split(',')]\n        else:\n            principals = list(principals)\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if sig_alg_name == ():\n            sig_alg = self.sig_algorithms[0]\n        else:\n            sig_alg = cast(str, sig_alg_name).encode()\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        comment: _Comment\n\n        try:\n            algorithm, cert_handler = _certificate_version_map[key.algorithm,\n                                                               version]\n        except KeyError:\n            raise KeyGenerationError('Unknown certificate version') from None\n\n        return cert_handler.generate(self, algorithm, key, serial, cert_type,\n                                     key_id, principals, valid_after,\n                                     valid_before, cert_options,\n                                     sig_alg, comment)\n\n    def _generate_x509_certificate(self, key: 'SSHKey', subject: str,\n                                   issuer: Optional[str],\n                                   serial: Optional[int],\n                                   valid_after: _Time, valid_before: _Time,\n                                   ca: bool, ca_path_len: Optional[int],\n                                   purposes: X509CertPurposes,\n                                   user_principals: _CertPrincipals,\n                                   host_principals: _CertPrincipals,\n                                   hash_name: DefTuple[str],\n                                   comment: DefTuple[_Comment]) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if hash_name == ():\n            hash_name = key.default_x509_hash\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        hash_name: str\n        comment: _Comment\n\n        return SSHX509Certificate.generate(self, key, subject, issuer,\n                                           serial, valid_after, valid_before,\n                                           ca, ca_path_len, purposes,\n                                           user_principals, host_principals,\n                                           hash_name, comment)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this key\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key as a byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key as a Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def get_filename(self) -> Optional[bytes]:\n        \"\"\"Return the filename associated with this key\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._filename\n\n    def set_filename(self, filename: Union[None, bytes, FilePath]) -> None:\n        \"\"\"Set the filename associated with this key\n\n           :param filename:\n               The new filename to associate with this key\n           :type filename: `PurePath`, `str`, `bytes`, or `None`\n\n        \"\"\"\n\n        if isinstance(filename, PurePath):\n            filename = str(filename)\n\n        if isinstance(filename, str):\n            filename = filename.encode('utf-8')\n\n        self._filename = filename or None\n\n    def get_fingerprint(self, hash_name: str = 'sha256') -> str:\n        \"\"\"Get the fingerprint of this key\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           :param hash_name: (optional)\n               The hash algorithm to use to construct the fingerprint.\n           :type hash_name: `str`\n\n           :returns: `str`\n\n           :raises: :exc:`ValueError` if the hash name is invalid\n\n        \"\"\"\n\n        try:\n            hash_alg = _hashes[hash_name]\n        except KeyError:\n            raise ValueError('Unknown hash algorithm') from None\n\n        h = hash_alg(self.public_data)\n\n        if hash_name == 'md5':\n            fp = h.hexdigest()\n            fp_text = ':'.join(fp[i:i+2] for i in range(0, len(fp), 2))\n        else:\n            fpb = h.digest()\n            fp_text = binascii.b2a_base64(fpb).decode('ascii')[:-1].strip('=')\n\n        return hash_name.upper() + ':' + fp_text\n\n    def set_touch_required(self, touch_required: bool) -> None:\n        \"\"\"Set whether touch is required when using a security key\"\"\"\n\n        self._touch_required = touch_required\n\n    def sign_raw(self, data: bytes, hash_name: str) -> bytes:\n        \"\"\"Return a raw signature of the specified data\"\"\"\n\n        assert self._key is not None\n        return self._key.sign(data, hash_name)\n\n    def sign_ssh(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Abstract method to compute an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def verify_ssh(self, data: bytes, sig_algorithm: bytes,\n                   packet: SSHPacket) -> bool:\n        \"\"\"Abstract method to verify an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def sign(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Return an SSH-encoded signature of the specified data\"\"\"\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return b''.join((String(sig_algorithm),\n                         self.sign_ssh(data, sig_algorithm)))\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify an SSH signature of the specified data using this key\"\"\"\n\n        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False\n\n    def encode_pkcs1_private(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 private key export not supported')\n\n    def encode_pkcs1_public(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 public key export not supported')\n\n    def encode_pkcs8_private(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 private key export not supported')\n\n    def encode_pkcs8_public(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 public key export not supported')\n\n    def encode_ssh_private(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH private key export not supported')\n\n    def encode_ssh_public(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH public key export not supported')\n\n    def encode_agent_cert_private(self) -> bytes:\n        \"\"\"Encode certificate private key data for agent\"\"\"\n\n        raise NotImplementedError\n\n###The function: convert_to_public###\n    def generate_user_certificate(\n            self, user_key: 'SSHKey', key_id: str, version: int = 1,\n            serial: int = 0, principals: _CertPrincipals = (),\n            valid_after: _Time = 0, valid_before: _Time = 0xffffffffffffffff,\n            force_command: Optional[str] = None,\n            source_address: Optional[Sequence[str]] = None,\n            permit_x11_forwarding: bool = True,\n            permit_agent_forwarding: bool = True,\n            permit_port_forwarding: bool = True, permit_pty: bool = True,\n            permit_user_rc: bool = True, touch_required: bool = True,\n            sig_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH user certificate\n\n           This method returns an SSH user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param force_command: (optional)\n               The command (if any) to force a session to run when this\n               certificate is used.\n           :param source_address: (optional)\n               A list of source addresses and networks for which the\n               certificate is valid, defaulting to all addresses.\n           :param permit_x11_forwarding: (optional)\n               Whether or not to allow this user to use X11 forwarding,\n               defaulting to `True`.\n           :param permit_agent_forwarding: (optional)\n               Whether or not to allow this user to use agent forwarding,\n               defaulting to `True`.\n           :param permit_port_forwarding: (optional)\n               Whether or not to allow this user to use port forwarding,\n               defaulting to `True`.\n           :param permit_pty: (optional)\n               Whether or not to allow this user to allocate a\n               pseudo-terminal, defaulting to `True`.\n           :param permit_user_rc: (optional)\n               Whether or not to run the user rc file when this certificate\n               is used, defaulting to `True`.\n           :param touch_required: (optional)\n               Whether or not to require the user to touch the security key\n               when authenticating with it, defaulting to `True`.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type force_command: `str` or `None`\n           :type source_address: list of ip_address and ip_network values\n           :type permit_x11_forwarding: `bool`\n           :type permit_agent_forwarding: `bool`\n           :type permit_port_forwarding: `bool`\n           :type permit_pty: `bool`\n           :type permit_user_rc: `bool`\n           :type touch_required: `bool`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        cert_options: _OpenSSHCertOptions = {}\n\n        if force_command:\n            cert_options['force-command'] = force_command\n\n        if source_address:\n            cert_options['source-address'] = [ip_network(addr)\n                                              for addr in source_address]\n\n        if permit_x11_forwarding:\n            cert_options['permit-X11-forwarding'] = True\n\n        if permit_agent_forwarding:\n            cert_options['permit-agent-forwarding'] = True\n\n        if permit_port_forwarding:\n            cert_options['permit-port-forwarding'] = True\n\n        if permit_pty:\n            cert_options['permit-pty'] = True\n\n        if permit_user_rc:\n            cert_options['permit-user-rc'] = True\n\n        if not touch_required:\n            cert_options['no-touch-required'] = True\n\n        return self._generate_certificate(user_key, version, serial,\n                                          CERT_TYPE_USER, key_id,\n                                          principals, valid_after,\n                                          valid_before, cert_options,\n                                          sig_alg, comment)\n\n    def generate_host_certificate(self, host_key: 'SSHKey', key_id: str,\n                                  version: int = 1, serial: int = 0,\n                                  principals: _CertPrincipals = (),\n                                  valid_after: _Time = 0,\n                                  valid_before: _Time = 0xffffffffffffffff,\n                                  sig_alg: DefTuple[str] = (),\n                                  comment: DefTuple[_Comment] = ()) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH host certificate\n\n           This method returns an SSH host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        if comment == ():\n            comment = host_key.get_comment_bytes()\n\n        return self._generate_certificate(host_key, version, serial,\n                                          CERT_TYPE_HOST, key_id,\n                                          principals, valid_after,\n                                          valid_before, {}, sig_alg, comment)\n\n    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 user certificate\n\n           This method returns an X.509 user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellClient'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n\n    def generate_x509_host_certificate(\n            self, host_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellServer',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 host certificate\n\n           This method returns an X.509 host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellServer'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(host_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)\n\n    def generate_x509_ca_certificate(self, ca_key: 'SSHKey', subject: str,\n                                     issuer: Optional[str] = None,\n                                     serial: Optional[int] = None,\n                                     valid_after: _Time = 0,\n                                     valid_before: _Time = 0xffffffffffffffff,\n                                     ca_path_len: Optional[int] = None,\n                                     hash_alg: DefTuple[str] = (),\n                                     comment: DefTuple[_Comment] = ()) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 CA certificate\n\n           This method returns an X.509 CA certificate with the requested\n           attributes signed by this private key.\n\n           :param ca_key:\n               The new CA's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param ca_path_len: (optional)\n               The maximum number of levels of intermediate CAs allowed\n               below this new CA or `None` to not enforce a limit,\n               defaulting to no limit.\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               ca_key.\n           :type ca_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type ca_path_len: `int` or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(ca_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, True,\n                                               ca_path_len, None, (), (),\n                                               hash_alg, comment)\n\n    def export_private_key(self, format_name: str = 'openssh',\n                           passphrase: Optional[BytesOrStr] = None,\n                           cipher_name: str = 'aes256-cbc',\n                           hash_name: str = 'sha256',\n                           pbe_version: int = 2, rounds: int = 128,\n                           ignore_few_rounds: bool = False) -> bytes:\n        \"\"\"Export a private key in the requested format\n\n           This method returns this object's private key encoded in the\n           requested format. If a passphrase is specified, the key will\n           be exported in encrypted form.\n\n           Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh\n\n           By default, openssh format will be used.\n\n           Encryption is supported in pkcs1-pem, pkcs8-der, pkcs8-pem,\n           and openssh formats. For pkcs1-pem, only the cipher can be\n           specified. For pkcs8-der and pkcs-8, cipher,  hash and PBE\n           version can be specified. For openssh, cipher and rounds\n           can be specified.\n\n           Available ciphers for pkcs1-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, des-cbc, des3-cbc\n\n           Available ciphers for pkcs8-der and pkcs8-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, blowfish-cbc,\n               cast128-cbc, des-cbc, des2-cbc, des3-cbc, rc4-40, rc4-128\n\n           Available ciphers for openssh format include the following\n           :ref:`encryption algorithms <EncryptionAlgs>`.\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           Available PBE versions include 1 for PBES1 and 2 for PBES2.\n\n           Not all combinations of cipher, hash, and version are supported.\n\n           The default cipher is aes256. In the pkcs8 formats, the default\n           hash is sha256 and default version is PBES2.\n\n           In openssh format, the default number of rounds is 128.\n\n           .. note:: The openssh format uses bcrypt for encryption, but\n                     unlike the traditional bcrypt cost factor used in\n                     password hashing which scales logarithmically, the\n                     encryption strength here scales linearly with the\n                     rounds value. Since the cipher is rekeyed 64 times\n                     per round, the default rounds value of 128 corresponds\n                     to 8192 total iterations, which is the equivalent of\n                     a bcrypt cost factor of 13.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :param passphrase: (optional)\n               A passphrase to encrypt the private key with.\n           :param cipher_name: (optional)\n               The cipher to use for private key encryption.\n           :param hash_name: (optional)\n               The hash to use for private key encryption.\n           :param pbe_version: (optional)\n               The PBE version to use for private key encryption.\n           :param rounds: (optional)\n               The number of KDF rounds to apply to the passphrase.\n           :type format_name: `str`\n           :type passphrase: `str` or `bytes`\n           :type cipher_name: `str`\n           :type hash_name: `str`\n           :type pbe_version: `int`\n           :type rounds: `int`\n\n           :returns: `bytes` representing the exported private key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_private())\n\n            if passphrase is not None:\n                if format_name == 'pkcs1-der':\n                    raise KeyExportError('PKCS#1 DER format does not support '\n                                         'private key encryption')\n\n                alg, iv, data = pkcs1_encrypt(data, cipher_name, passphrase)\n                headers = (b'Proc-Type: 4,ENCRYPTED\\n' +\n                           b'DEK-Info: ' + alg + b',' +\n                           binascii.b2a_hex(iv).upper() + b'\\n\\n')\n            else:\n                headers = b''\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PRIVATE KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        headers + _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_private()\n\n            if alg_params is OMIT:\n                data = der_encode((0, (self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode((0, (self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if passphrase is not None:\n                data = pkcs8_encrypt(data, cipher_name, hash_name,\n                                     pbe_version, passphrase)\n\n            if format_name == 'pkcs8-pem':\n                if passphrase is not None:\n                    keytype = b'ENCRYPTED PRIVATE KEY'\n                else:\n                    keytype = b'PRIVATE KEY'\n\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            check = os.urandom(4)\n            nkeys = 1\n\n            data = b''.join((check, check, self.private_data,\n                             String(self._comment or b'')))\n\n            cipher: Optional[Encryption]\n\n            if passphrase is not None:\n                try:\n                    alg = cipher_name.encode('ascii')\n                    key_size, iv_size, block_size, _, _, _ = \\\n                        get_encryption_params(alg)\n                except (KeyError, UnicodeEncodeError):\n                    raise KeyEncryptionError('Unknown cipher: %s' %\n                                             cipher_name) from None\n\n                if not _bcrypt_available: # pragma: no cover\n                    raise KeyExportError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n                kdf = b'bcrypt'\n                salt = os.urandom(_OPENSSH_SALT_LEN)\n                kdf_data = b''.join((String(salt), UInt32(rounds)))\n\n                if isinstance(passphrase, str):\n                    passphrase = passphrase.encode('utf-8')\n\n                key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                 rounds, ignore_few_rounds)\n\n                cipher = get_encryption(alg, key[:key_size], key[key_size:])\n                block_size = max(block_size, 8)\n            else:\n                cipher = None\n                alg = b'none'\n                kdf = b'none'\n                kdf_data = b''\n                block_size = 8\n                mac = b''\n\n            pad = len(data) % block_size\n            if pad: # pragma: no branch\n                data = data + bytes(range(1, block_size + 1 - pad))\n\n            if cipher:\n                data, mac = cipher.encrypt_packet(0, b'', data)\n            else:\n                mac = b''\n\n            data = b''.join((_OPENSSH_KEY_V1, String(alg), String(kdf),\n                             String(kdf_data), UInt32(nkeys),\n                             String(self.public_data), String(data), mac))\n\n            return (b'-----BEGIN OPENSSH PRIVATE KEY-----\\n' +\n                    _wrap_base64(data, _OPENSSH_WRAP_LEN) +\n                    b'-----END OPENSSH PRIVATE KEY-----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def export_public_key(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a public key in the requested format\n\n           This method returns this object's public key encoded in the\n           requested format. Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh, rfc4716\n\n           By default, openssh format will be used.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported public key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_public())\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PUBLIC KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_public()\n            pkcs8_data = BitString(pkcs8_data)\n\n            if alg_params is OMIT:\n                data = der_encode(((self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode(((self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if format_name == 'pkcs8-pem':\n                data = (b'-----BEGIN PUBLIC KEY-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END PUBLIC KEY-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs))\n\n    def write_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs))\n\n    def append_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs), 'ab')\n\n    def append_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs), 'ab')\n\n\nclass SSHCertificate:\n    \"\"\"Parent class which holds an SSH certificate\"\"\"\n\n    is_x509 = False\n    is_x509_chain = False\n\n    def __init__(self, algorithm: bytes, sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes], key: SSHKey,\n                 public_data: bytes, comment: _Comment):\n        self.algorithm = algorithm\n        self.sig_algorithms = sig_algorithms\n        self.host_key_algorithms = host_key_algorithms\n        self.key = key\n        self.public_data = public_data\n\n        self.set_comment(comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        raise NotImplementedError\n\n    def __eq__(self, other: object) -> bool:\n        return (isinstance(other, type(self)) and\n                self.public_data == other.public_data)\n\n    def __hash__(self) -> int:\n        return hash(self.public_data)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this certificate\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this certificate\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this certificate as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this certificate as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        return self._comment.decode(encoding, errors) if self._comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this certificate\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def export_certificate(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a certificate in the requested format\n\n           This function returns this certificate encoded in the requested\n           format. Available formats include:\n\n               der, pem, openssh, rfc4716\n\n           By default, OpenSSH format will be used.\n\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported certificate\n\n        \"\"\"\n\n        if self.is_x509:\n            if format_name == 'rfc4716':\n                raise KeyExportError('RFC4716 format is not supported for '\n                                     'X.509 certificates')\n        else:\n            if format_name in ('der', 'pem'):\n                raise KeyExportError('DER and PEM formats are not supported '\n                                     'for OpenSSH certificates')\n\n        if format_name == 'der':\n            return self.public_data\n        elif format_name == 'pem':\n            return (b'-----BEGIN CERTIFICATE-----\\n' +\n                    _wrap_base64(self.public_data) +\n                    b'-----END CERTIFICATE-----\\n')\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_certificate(self, filename: FilePath,\n                          format_name: str = 'openssh') -> None:\n        \"\"\"Write a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which writes the exported certificate to a file.\n\n           :param filename:\n               The filename to write the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name))\n\n    def append_certificate(self, filename: FilePath,\n                           format_name: str = 'openssh') -> None:\n        \"\"\"Append a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which appends the exported certificate to an existing file.\n\n           :param filename:\n               The filename to append the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name), 'ab')\n\n\nclass SSHOpenSSHCertificate(SSHCertificate):\n    \"\"\"Class which holds an OpenSSH certificate\"\"\"\n\n    _user_option_encoders: _OpenSSHCertEncoders = ()\n    _user_extension_encoders: _OpenSSHCertEncoders = ()\n    _host_option_encoders: _OpenSSHCertEncoders = ()\n    _host_extension_encoders: _OpenSSHCertEncoders = ()\n\n    _user_option_decoders: _OpenSSHCertDecoders = {}\n    _user_extension_decoders: _OpenSSHCertDecoders = {}\n    _host_option_decoders: _OpenSSHCertDecoders = {}\n    _host_extension_decoders: _OpenSSHCertDecoders = {}\n\n    def __init__(self, algorithm: bytes, key: SSHKey, data: bytes,\n                 principals: Sequence[str], options: _OpenSSHCertOptions,\n                 signing_key: SSHKey, serial: int, cert_type: int,\n                 key_id: str, valid_after: int, valid_before: int,\n                 comment: _Comment):\n        super().__init__(algorithm, key.sig_algorithms,\n                         key.cert_algorithms or (algorithm,),\n                         key, data, comment)\n\n        self.principals = principals\n        self.options = options\n        self.signing_key = signing_key\n\n        self._serial = serial\n        self._cert_type = cert_type\n        self._key_id = key_id\n        self._valid_after = valid_after\n        self._valid_before = valid_before\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', algorithm: bytes, key: 'SSHKey',\n                 serial: int, cert_type: int, key_id: str,\n                 principals: Sequence[str], valid_after: int,\n                 valid_before: int, options: _OpenSSHCertOptions,\n                 sig_alg: bytes, comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        principal_bytes = b''.join(String(p) for p in principals)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._encode_options(options,\n                                               cls._user_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._user_extension_encoders)\n        else:\n            cert_options = cls._encode_options(options,\n                                               cls._host_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._host_extension_encoders)\n\n        key = key.convert_to_public()\n\n        data = b''.join((String(algorithm),\n                         cls._encode(key, serial, cert_type, key_id,\n                                     principal_bytes, valid_after,\n                                     valid_before, cert_options,\n                                     cert_extensions),\n                         String(signing_key.public_data)))\n\n        data += String(signing_key.sign(data, sig_alg))\n\n        signing_key = signing_key.convert_to_public()\n\n        return cls(algorithm, key, data, principals, options, signing_key,\n                   serial, cert_type, key_id, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        assert key_handler is not None\n\n        key_params, serial, cert_type, key_id, \\\n            principals, valid_after, valid_before, \\\n            options, extensions = cls._decode(packet, key_handler)\n\n        signing_key = decode_ssh_public_key(packet.get_string())\n        data = packet.get_consumed_payload()\n        signature = packet.get_string()\n        packet.check_end()\n\n        if not signing_key.verify(data, signature):\n            raise KeyImportError('Invalid certificate signature')\n\n        key = key_handler.make_public(key_params)\n        data = packet.get_consumed_payload()\n\n        try:\n            key_id_bytes = key_id.decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in key ID') from None\n\n        packet = SSHPacket(principals)\n        principals: List[str] = []\n\n        while packet:\n            try:\n                principal = packet.get_string().decode('utf-8')\n            except UnicodeDecodeError:\n                raise KeyImportError('Invalid characters in principal '\n                                     'name') from None\n\n            principals.append(principal)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._decode_options(\n                options, cls._user_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._user_extension_decoders, False))\n        elif cert_type == CERT_TYPE_HOST:\n            cert_options = cls._decode_options(\n                options, cls._host_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._host_extension_decoders, False))\n        else:\n            raise KeyImportError('Unknown certificate type')\n\n        return cls(algorithm, key, data, principals, cert_options, signing_key,\n                   serial, cert_type, key_id_bytes, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n\n        \"\"\"Encode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def _encode_options(options: _OpenSSHCertOptions,\n                        encoders: _OpenSSHCertEncoders) -> bytes:\n        \"\"\"Encode options found in this certificate\"\"\"\n\n        result = []\n\n        for name, encoder in encoders:\n            value = options.get(name)\n            if value:\n                result.append(String(name) + String(encoder(value)))\n\n        return b''.join(result)\n\n    @staticmethod\n    def _encode_bool(_value: object) -> bytes:\n        \"\"\"Encode a boolean option value\"\"\"\n\n        return b''\n\n    @staticmethod\n    def _encode_force_cmd(force_command: object) -> bytes:\n        \"\"\"Encode a force-command option\"\"\"\n\n        return String(cast(BytesOrStr, force_command))\n\n    @staticmethod\n    def _encode_source_addr(source_address: object) -> bytes:\n        \"\"\"Encode a source-address option\"\"\"\n\n        return NameList(str(addr).encode('ascii')\n                        for addr in cast(Sequence[IPNetwork], source_address))\n\n    @staticmethod\n    def _decode_bool(_packet: SSHPacket) -> bool:\n        \"\"\"Decode a boolean option value\"\"\"\n\n        return True\n\n    @staticmethod\n    def _decode_force_cmd(packet: SSHPacket) -> str:\n        \"\"\"Decode a force-command option\"\"\"\n\n        try:\n            return packet.get_string().decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in command') from None\n\n    @staticmethod\n    def _decode_source_addr(packet: SSHPacket) -> Sequence[IPNetwork]:\n        \"\"\"Decode a source-address option\"\"\"\n\n        try:\n            return [ip_network(addr.decode('ascii'))\n                    for addr in packet.get_namelist()]\n        except (UnicodeDecodeError, ValueError):\n            raise KeyImportError('Invalid source address') from None\n\n    @staticmethod\n    def _decode_options(options: bytes, decoders: _OpenSSHCertDecoders,\n                        critical: bool = True) -> _OpenSSHCertOptions:\n        \"\"\"Decode options found in this certificate\"\"\"\n\n        packet = SSHPacket(options)\n        result: _OpenSSHCertOptions = {}\n\n        while packet:\n            name = packet.get_string()\n\n            decoder = decoders.get(name)\n            if decoder:\n                data_packet = SSHPacket(packet.get_string())\n                result[name.decode('ascii')] = decoder(data_packet)\n                data_packet.check_end()\n            elif critical:\n                raise KeyImportError('Unrecognized critical option: %s' %\n                                     name.decode('ascii', errors='replace'))\n\n        return result\n\n    def validate(self, cert_type: int, principal: str) -> None:\n        \"\"\"Validate an OpenSSH certificate\"\"\"\n\n        if self._cert_type != cert_type:\n            raise ValueError('Invalid certificate type')\n\n        now = time.time()\n\n        if now < self._valid_after:\n            raise ValueError('Certificate not yet valid')\n\n        if now >= self._valid_before:\n            raise ValueError('Certificate expired')\n\n        if principal and self.principals and principal not in self.principals:\n            raise ValueError('Certificate principal mismatch')\n\n\nclass SSHOpenSSHCertificateV01(SSHOpenSSHCertificate):\n    \"\"\"Encoder/decoder class for version 01 OpenSSH certificates\"\"\"\n\n    _user_option_encoders = (\n        ('force-command',           SSHOpenSSHCertificate._encode_force_cmd),\n        ('source-address',          SSHOpenSSHCertificate._encode_source_addr)\n    )\n\n    _user_extension_encoders = (\n        ('permit-X11-forwarding',   SSHOpenSSHCertificate._encode_bool),\n        ('permit-agent-forwarding', SSHOpenSSHCertificate._encode_bool),\n        ('permit-port-forwarding',  SSHOpenSSHCertificate._encode_bool),\n        ('permit-pty',              SSHOpenSSHCertificate._encode_bool),\n        ('permit-user-rc',          SSHOpenSSHCertificate._encode_bool),\n        ('no-touch-required',       SSHOpenSSHCertificate._encode_bool)\n    )\n\n    _user_option_decoders = {\n        b'force-command':           SSHOpenSSHCertificate._decode_force_cmd,\n        b'source-address':          SSHOpenSSHCertificate._decode_source_addr\n    }\n\n    _user_extension_decoders = {\n        b'permit-X11-forwarding':   SSHOpenSSHCertificate._decode_bool,\n        b'permit-agent-forwarding': SSHOpenSSHCertificate._decode_bool,\n        b'permit-port-forwarding':  SSHOpenSSHCertificate._decode_bool,\n        b'permit-pty':              SSHOpenSSHCertificate._decode_bool,\n        b'permit-user-rc':          SSHOpenSSHCertificate._decode_bool,\n        b'no-touch-required':       SSHOpenSSHCertificate._decode_bool\n    }\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n        \"\"\"Encode a version 01 SSH certificate\"\"\"\n\n        return b''.join((String(os.urandom(32)), key.encode_ssh_public(),\n                         UInt64(serial), UInt32(cert_type), String(key_id),\n                         String(principals), UInt64(valid_after),\n                         UInt64(valid_before), String(options),\n                         String(extensions), String('')))\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode a version 01 SSH certificate\"\"\"\n\n        _ = packet.get_string()                             # nonce\n        key_params = key_handler.decode_ssh_public(packet)\n        serial = packet.get_uint64()\n        cert_type = packet.get_uint32()\n        key_id = packet.get_string()\n        principals = packet.get_string()\n        valid_after = packet.get_uint64()\n        valid_before = packet.get_uint64()\n        options = packet.get_string()\n        extensions = packet.get_string()\n        _ = packet.get_string()                             # reserved\n\n        return (key_params, serial, cert_type, key_id, principals,\n                valid_after, valid_before, options, extensions)\n\n\nclass SSHX509Certificate(SSHCertificate):\n    \"\"\"Encoder/decoder class for SSH X.509 certificates\"\"\"\n\n    is_x509 = True\n\n    def __init__(self, key: SSHKey, x509_cert: 'X509Certificate',\n                 comment: _Comment = None):\n        super().__init__(b'x509v3-' + key.algorithm, key.x509_algorithms,\n                         key.x509_algorithms, key, x509_cert.data,\n                         x509_cert.comment or comment)\n\n        self.subject = x509_cert.subject\n        self.issuer = x509_cert.issuer\n        self.issuer_hash = x509_cert.issuer_hash\n        self.user_principals = x509_cert.user_principals\n        self.x509_cert = x509_cert\n\n    def _expand_trust_store(self, cert: 'SSHX509Certificate',\n                            trusted_cert_paths: Sequence[FilePath],\n                            trust_store: Set['SSHX509Certificate']) -> None:\n        \"\"\"Look up certificates by issuer hash to build a trust store\"\"\"\n\n        issuer_hash = cert.issuer_hash\n\n        for path in trusted_cert_paths:\n            idx = 0\n\n            try:\n                while True:\n                    cert_path = Path(path, issuer_hash + '.' + str(idx))\n                    idx += 1\n\n                    c = cast('SSHX509Certificate', read_certificate(cert_path))\n\n                    if c.subject != cert.issuer or c in trust_store:\n                        continue\n\n                    trust_store.add(c)\n                    self._expand_trust_store(c, trusted_cert_paths, trust_store)\n            except (OSError, KeyImportError):\n                pass\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        raise RuntimeError # pragma: no cover\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', key: 'SSHKey', subject: str,\n                 issuer: Optional[str], serial: Optional[int],\n                 valid_after: int, valid_before: int, ca: bool,\n                 ca_path_len: Optional[int], purposes: X509CertPurposes,\n                 user_principals: _CertPrincipals,\n                 host_principals: _CertPrincipals, hash_name: str,\n                 comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        key = key.convert_to_public()\n\n        x509_cert = generate_x509_certificate(signing_key.pyca_key,\n                                              key.pyca_key, subject, issuer,\n                                              serial, valid_after, valid_before,\n                                              ca, ca_path_len, purposes,\n                                              user_principals, host_principals,\n                                              hash_name, comment)\n\n        return cls(key, x509_cert)\n\n    @classmethod\n    def construct_from_der(cls, data: bytes,\n                           comment: _Comment = None) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from DER data\"\"\"\n\n        try:\n            x509_cert = import_x509_certificate(data)\n            key = import_public_key(x509_cert.key_data)\n        except ValueError as exc:\n            raise KeyImportError(str(exc)) from None\n\n        return cls(key, x509_cert, comment)\n\n    def validate_chain(self, trust_chain: Sequence['SSHX509Certificate'],\n                       trusted_certs: Sequence['SSHX509Certificate'],\n                       trusted_cert_paths: Sequence[FilePath],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        trust_store = set(c for c in trust_chain if c.subject != c.issuer) | \\\n            set(c for c in trusted_certs)\n\n        if trusted_cert_paths:\n            self._expand_trust_store(self, trusted_cert_paths, trust_store)\n\n            for c in trust_chain:\n                self._expand_trust_store(c, trusted_cert_paths, trust_store)\n\n        self.x509_cert.validate([c.x509_cert for c in trust_store],\n                                purposes, user_principal, host_principal)\n\n\nclass SSHX509CertificateChain(SSHCertificate):\n    \"\"\"Encoder/decoder class for an SSH X.509 certificate chain\"\"\"\n\n    is_x509_chain = True\n\n    def __init__(self, algorithm: bytes, certs: Sequence[SSHCertificate],\n                 ocsp_responses: Sequence[bytes], comment: _Comment):\n        key = certs[0].key\n        data = self._public_data(algorithm, certs, ocsp_responses)\n\n        super().__init__(algorithm, key.x509_algorithms, key.x509_algorithms,\n                         key, data, comment)\n\n        x509_certs = cast(Sequence[SSHX509Certificate], certs)\n        first_cert = x509_certs[0]\n        last_cert = x509_certs[-1]\n\n        self.subject = first_cert.subject\n        self.issuer = last_cert.issuer\n        self.user_principals = first_cert.user_principals\n\n        self._certs = x509_certs\n        self._ocsp_responses = ocsp_responses\n\n    @staticmethod\n    def _public_data(algorithm: bytes, certs: Sequence[SSHCertificate],\n                     ocsp_responses: Sequence[bytes]) -> bytes:\n        \"\"\"Return the X509 chain public data\"\"\"\n\n        return (String(algorithm) + UInt32(len(certs)) +\n                b''.join(String(c.public_data) for c in certs) +\n                UInt32(len(ocsp_responses)) +\n                b''.join(String(resp) for resp in ocsp_responses))\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        cert_count = packet.get_uint32()\n        certs = [import_certificate(packet.get_string())\n                 for _ in range(cert_count)]\n\n        ocsp_resp_count = packet.get_uint32()\n        ocsp_responses = [packet.get_string() for _ in range(ocsp_resp_count)]\n\n        packet.check_end()\n\n        if not certs:\n            raise KeyImportError('No certificates present')\n\n        return cls(algorithm, certs, ocsp_responses, comment)\n\n    @classmethod\n    def construct_from_certs(cls, certs: Sequence['SSHCertificate']) -> \\\n            'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate chain from certificates\"\"\"\n\n        cert = certs[0]\n\n        return cls(cert.algorithm, certs, (), cert.get_comment_bytes())\n\n    def adjust_public_data(self, algorithm: bytes) -> bytes:\n        \"\"\"Adjust public data to reflect chosen signature algorithm\"\"\"\n\n        return self._public_data(algorithm, self._certs, self._ocsp_responses)\n\n    def validate_chain(self, trusted_certs: Sequence[SSHX509Certificate],\n                       trusted_cert_paths: Sequence[FilePath],\n                       revoked_certs: Set[SSHX509Certificate],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        if revoked_certs:\n            for cert in self._certs:\n                if cert in revoked_certs:\n                    raise ValueError('Revoked X.509 certificate in '\n                                     'certificate chain')\n\n        self._certs[0].validate_chain(self._certs[1:], trusted_certs,\n                                      trusted_cert_paths, purposes,\n                                      user_principal, host_principal)\n\n\nclass SSHKeyPair:\n    \"\"\"Parent class which represents an asymmetric key pair\n\n       This is an abstract class which provides a method to sign data\n       with a private key and members to access the corresponding\n       algorithm and public key or certificate information needed to\n       identify what key was used for signing.\n\n    \"\"\"\n\n    _key_type = 'unknown'\n\n    def __init__(self, algorithm: bytes, sig_algorithm: bytes,\n                 sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes],\n                 public_data: bytes, comment: _Comment,\n                 cert: Optional[SSHCertificate] = None,\n                 filename: Optional[bytes] = None,\n                 use_executor: bool = False):\n        self.key_algorithm = algorithm\n        self.key_public_data = public_data\n\n        self.set_comment(comment)\n        self._cert = cert\n        self._filename = filename\n\n        self.use_executor = use_executor\n\n        if cert:\n            if cert.key.public_data != self.key_public_data:\n                raise ValueError('Certificate key mismatch')\n\n            self.algorithm = cert.algorithm\n\n            if cert.is_x509_chain:\n                self.sig_algorithm = cert.algorithm\n            else:\n                self.sig_algorithm = sig_algorithm\n\n            self.sig_algorithms = cert.sig_algorithms\n            self.host_key_algorithms = cert.host_key_algorithms\n            self.public_data = cert.public_data\n        else:\n            self.algorithm = algorithm\n            self.sig_algorithm = algorithm\n            self.sig_algorithms = sig_algorithms\n            self.host_key_algorithms = host_key_algorithms\n            self.public_data = public_data\n\n    def get_key_type(self) -> str:\n        \"\"\"Return what type of key pair this is\n\n           This method returns 'local' for locally loaded keys, and\n           'agent' for keys managed by an SSH agent.\n\n        \"\"\"\n\n        return self._key_type\n\n    @property\n    def has_cert(self) -> bool:\n        \"\"\" Return if this key pair has an associated cert\"\"\"\n\n        return bool(self._cert)\n\n    @property\n    def has_x509_chain(self) -> bool:\n        \"\"\" Return if this key pair has an associated X.509 cert chain\"\"\"\n\n        return self._cert.is_x509_chain if self._cert else False\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key pair\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyImportError('Private key export to agent not supported')\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key pair as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key pair as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key pair\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def set_certificate(self, cert: SSHCertificate) -> None:\n        \"\"\"Set certificate to use with this key\"\"\"\n\n        if cert.key.public_data != self.key_public_data:\n            raise ValueError('Certificate key mismatch')\n\n        self._cert = cert\n        self.algorithm = cert.algorithm\n\n        if cert.is_x509_chain:\n            self.sig_algorithm = cert.algorithm\n        else:\n            self.sig_algorithm = self.key_algorithm\n\n        self.sig_algorithms = cert.sig_algorithms\n        self.host_key_algorithms = cert.host_key_algorithms\n        self.public_data = cert.public_data\n\n    def set_sig_algorithm(self, sig_algorithm: bytes) -> None:\n        \"\"\"Set the signature algorithm to use when signing data\"\"\"\n\n        try:\n            sig_algorithm = _certificate_sig_alg_map[sig_algorithm]\n        except KeyError:\n            pass\n\n        self.sig_algorithm = sig_algorithm\n\n        if not self.has_cert:\n            self.algorithm = sig_algorithm\n        elif self.has_x509_chain:\n            self.algorithm = sig_algorithm\n\n            cert = cast('SSHX509CertificateChain', self._cert)\n            self.public_data = cert.adjust_public_data(sig_algorithm)\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise RuntimeError # pragma: no cover\n\n\nclass SSHLocalKeyPair(SSHKeyPair):\n    \"\"\"Class which holds a local asymmetric key pair\n\n       This class holds a private key and associated public data\n       which can either be the matching public key or a certificate\n       which has signed that public key.\n\n    \"\"\"\n\n    _key_type = 'local'\n\n    def __init__(self, key: SSHKey, pubkey: Optional[SSHKey] = None,\n                 cert: Optional[SSHCertificate] = None):\n        if pubkey and pubkey.public_data != key.public_data:\n            raise ValueError('Public key mismatch')\n\n        if key.has_comment():\n            comment = key.get_comment_bytes()\n        elif cert and cert.has_comment():\n            comment = cert.get_comment_bytes()\n        elif pubkey and pubkey.has_comment():\n            comment = pubkey.get_comment_bytes()\n        else:\n            comment = None\n\n        super().__init__(key.algorithm, key.algorithm, key.sig_algorithms,\n                         key.sig_algorithms, key.public_data, comment, cert,\n                         key.get_filename(), key.use_executor)\n\n        self._key = key\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        if self._cert:\n            data = String(self.public_data) + \\\n                       self._key.encode_agent_cert_private()\n        else:\n            data = self._key.encode_ssh_private()\n\n        return String(self.algorithm) + data\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        return self._key.sign(data, self.sig_algorithm)\n\n\ndef _parse_openssh(data: bytes) -> Tuple[bytes, Optional[bytes], bytes]:\n    \"\"\"Parse an OpenSSH format public key or certificate\"\"\"\n\n    line = data.split(None, 2)\n\n    if len(line) < 2:\n        raise KeyImportError('Invalid OpenSSH public key or certificate')\n    elif len(line) == 2:\n        comment = None\n    else:\n        comment = line[2]\n\n    if (line[0] not in _public_key_alg_map and\n            line[0] not in _certificate_alg_map):\n        raise KeyImportError('Unknown OpenSSH public key algorithm')\n\n    try:\n        return line[0], comment, binascii.a2b_base64(line[1])\n    except binascii.Error:\n        raise KeyImportError('Invalid OpenSSH public key '\n                             'or certificate') from None\n\n\ndef _parse_pem(data: bytes) -> Tuple[Mapping[bytes, bytes], bytes]:\n    \"\"\"Parse a PEM data block\"\"\"\n\n    start = 0\n    end: Optional[int] = None\n    headers: Dict[bytes, bytes] = {}\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if b':' in line:\n            hdr, value = line.split(b':', 1)\n            headers[hdr.strip()] = value.strip()\n        else:\n            break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return headers, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid PEM data') from None\n\n\ndef _parse_rfc4716(data: bytes) -> Tuple[Optional[bytes], bytes]:\n    \"\"\"Parse an RFC 4716 data block\"\"\"\n\n    start = 0\n    end = None\n    hdr = b''\n    comment = None\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if line[-1:] == b'\\\\':\n            hdr += line[:-1]\n        else:\n            hdr += line\n            if b':' in hdr:\n                hdr, value = hdr.split(b':', 1)\n\n                if hdr.strip() == b'Comment':\n                    comment = value.strip()\n                    if comment[:1] == b'\"' and comment[-1:] == b'\"':\n                        comment = comment[1:-1]\n\n                hdr = b''\n            else:\n                break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return comment, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid RFC 4716 data') from None\n\n\ndef _match_block(data: bytes, start: int, header: bytes,\n                 fmt: str) -> Tuple[bytes, int]:\n    \"\"\"Match a block of data wrapped in a header/footer\"\"\"\n\n    match = re.compile(b'^' + header[:5] + b'END' + header[10:] +\n                       rb'[ \\t\\r\\f\\v]*$', re.M).search(data, start)\n\n    if not match:\n        raise KeyImportError('Missing %s footer' % fmt)\n\n    return data[start:match.start()], match.end()\n\n\ndef _match_next(data: bytes, keytype: bytes, public: bool = False) -> \\\n        Tuple[Optional[str], Tuple, Optional[int]]:\n    \"\"\"Find the next key/certificate and call the appropriate decode\"\"\"\n\n    end: Optional[int]\n\n    if data.startswith(b'\\x30'):\n        try:\n            key_data, end = der_decode_partial(data)\n            return 'der', (key_data,), end\n        except ASN1DecodeError:\n            pass\n\n    start = 0\n    end = None\n\n    while end != 0:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if (line.startswith(b'-----BEGIN ') and\n                line.endswith(b' ' + keytype + b'-----')):\n            pem_name = line[11:-(6+len(keytype))].strip()\n            data, end = _match_block(data, end, line, 'PEM')\n            headers, data = _parse_pem(data)\n            return 'pem', (pem_name, headers, data), end\n        elif public:\n            if line == b'---- BEGIN SSH2 PUBLIC KEY ----':\n                data, end = _match_block(data, end, line, 'RFC 4716')\n                return 'rfc4716', _parse_rfc4716(data), end\n            else:\n                try:\n                    cert = _parse_openssh(line)\n                except KeyImportError:\n                    pass\n                else:\n                    return 'openssh', cert, (end if end else len(data))\n\n        start = end\n\n    return None, (), len(data)\n\n\ndef _decode_pkcs1_private(\n        pem_name: bytes, key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_private(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s private key' %\n                             pem_name.decode('ascii'))\n\n    if pem_name == b'RSA':\n        key_params = cast(Tuple, key_params) + \\\n            (unsafe_skip_rsa_key_validation,)\n\n    return handler.make_private(key_params)\n\n\ndef _decode_pkcs1_public(pem_name: bytes, key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_public(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s public key' %\n                             pem_name.decode('ascii'))\n\n    return handler.make_public(key_params)\n\n\ndef _decode_pkcs8_private(\n        key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) >= 3 and\n            key_data[0] in (0, 1) and isinstance(key_data[1], tuple) and\n            1 <= len(key_data[1]) <= 2 and isinstance(key_data[2], bytes)):\n        if len(key_data[1]) == 2:\n            alg, alg_params = key_data[1]\n        else:\n            alg, alg_params = key_data[1][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_private(alg_params, key_data[2])\n        if key_params is None:\n            raise KeyImportError('Invalid %s private key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        if alg == ObjectIdentifier('1.2.840.113549.1.1.1'):\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        return handler.make_private(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 private key')\n\n\ndef _decode_pkcs8_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) == 2 and\n            isinstance(key_data[0], tuple) and 1 <= len(key_data[0]) <= 2 and\n            isinstance(key_data[1], BitString) and key_data[1].unused == 0):\n        if len(key_data[0]) == 2:\n            alg, alg_params = key_data[0]\n        else:\n            alg, alg_params = key_data[0][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_public(alg_params, key_data[1].value)\n        if key_params is None:\n            raise KeyImportError('Invalid %s public key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        return handler.make_public(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 public key')\n\n\ndef _decode_openssh_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode an OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        cipher_name = packet.get_string()\n        kdf = packet.get_string()\n        kdf_data = packet.get_string()\n        nkeys = packet.get_uint32()\n        _ = packet.get_string()                 # public_key\n        key_data = packet.get_string()\n        mac = packet.get_remaining_payload()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if cipher_name != b'none':\n            if passphrase is None:\n                raise KeyImportError('Passphrase must be specified to import '\n                                     'encrypted private keys')\n\n            try:\n                key_size, iv_size, block_size, _, _, _ = \\\n                    get_encryption_params(cipher_name)\n            except KeyError:\n                raise KeyEncryptionError('Unknown cipher: %s' %\n                                         cipher_name.decode('ascii')) from None\n\n            if kdf != b'bcrypt':\n                raise KeyEncryptionError('Unknown kdf: %s' %\n                                         kdf.decode('ascii'))\n\n            if not _bcrypt_available: # pragma: no cover\n                raise KeyEncryptionError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n            packet = SSHPacket(kdf_data)\n            salt = packet.get_string()\n            rounds = packet.get_uint32()\n            packet.check_end()\n\n            if isinstance(passphrase, str):\n                passphrase = passphrase.encode('utf-8')\n\n            try:\n                bcrypt_key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                        rounds, ignore_few_rounds=True)\n            except ValueError:\n                raise KeyEncryptionError('Invalid OpenSSH '\n                                         'private key') from None\n\n            cipher = get_encryption(cipher_name, bcrypt_key[:key_size],\n                                    bcrypt_key[key_size:])\n\n            decrypted_key = cipher.decrypt_packet(0, b'', key_data, 0, mac)\n\n            if decrypted_key is None:\n                raise KeyEncryptionError('Incorrect passphrase')\n\n            key_data = decrypted_key\n            block_size = max(block_size, 8)\n        else:\n            block_size = 8\n\n        packet = SSHPacket(key_data)\n\n        check1 = packet.get_uint32()\n        check2 = packet.get_uint32()\n        if check1 != check2:\n            if cipher_name != b'none':\n                raise KeyEncryptionError('Incorrect passphrase') from None\n            else:\n                raise KeyImportError('Invalid OpenSSH private key')\n\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n        if not handler:\n            raise KeyImportError('Unknown OpenSSH private key algorithm')\n\n        key_params = handler.decode_ssh_private(packet)\n        comment = packet.get_string()\n        pad = packet.get_remaining_payload()\n\n        if len(pad) >= block_size or pad != bytes(range(1, len(pad) + 1)):\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if alg == b'ssh-rsa':\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        key = handler.make_private(key_params)\n        key.set_comment(comment)\n        return key\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_openssh_public(data: bytes) -> SSHKey:\n    \"\"\"Decode public key within OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        _ = packet.get_string()                 # cipher_name\n        _ = packet.get_string()                 # kdf\n        _ = packet.get_string()                 # kdf_data\n        nkeys = packet.get_uint32()\n        pubkey = packet.get_string()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        return decode_ssh_public_key(pubkey)\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_der_private(\n        key_data: object, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a DER format private key\"\"\"\n\n    # First, if there's a passphrase, try to decrypt PKCS#8\n    if passphrase is not None:\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            # Decryption failed - try decoding it as unencrypted\n            pass\n\n    # Then, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_private(pem_name, key_data,\n                                         unsafe_skip_rsa_key_validation)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER private key')\n\n\ndef _decode_der_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a DER format public key\"\"\"\n\n    # First, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_public(key_data)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_public(pem_name, key_data)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER public key')\n\n\ndef _decode_der_certificate(data: bytes,\n                            comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a DER format X.509 certificate\"\"\"\n\n    return SSHX509Certificate.construct_from_der(data, comment)\n\n\ndef _decode_pem_private(\n        pem_name: bytes, headers: Mapping[bytes, bytes],\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PEM format private key\"\"\"\n\n    if pem_name == b'OPENSSH':\n        return _decode_openssh_private(data, passphrase,\n                                       unsafe_skip_rsa_key_validation)\n\n    if headers.get(b'Proc-Type') == b'4,ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        dek_info = headers.get(b'DEK-Info', b'').split(b',')\n        if len(dek_info) != 2:\n            raise KeyImportError('Invalid PEM encryption params')\n\n        alg, iv = dek_info\n        try:\n            iv = binascii.a2b_hex(iv)\n        except binascii.Error:\n            raise KeyImportError('Invalid PEM encryption params') from None\n\n        try:\n            data = pkcs1_decrypt(data, alg, iv, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#1 '\n                                 'private key') from None\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM private key') from None\n\n    if pem_name == b'ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        pem_name = b''\n\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#8 '\n                                 'private key') from None\n\n    if pem_name:\n        return _decode_pkcs1_private(pem_name, key_data,\n                                     unsafe_skip_rsa_key_validation)\n    else:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n\n\ndef _decode_pem_public(pem_name: bytes, data: bytes) -> SSHKey:\n    \"\"\"Decode a PEM format public key\"\"\"\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM public key') from None\n\n    if pem_name:\n        return _decode_pkcs1_public(pem_name, key_data)\n    else:\n        return _decode_pkcs8_public(key_data)\n\n\ndef _decode_pem_certificate(pem_name: bytes, data: bytes) -> SSHCertificate:\n    \"\"\"Decode a PEM format X.509 certificate\"\"\"\n\n    if pem_name == b'TRUSTED':\n        # Strip off OpenSSL trust information\n        try:\n            _, end = der_decode_partial(data)\n            data = data[:end]\n        except ASN1DecodeError:\n            raise KeyImportError('Invalid PEM trusted certificate') from None\n    elif pem_name:\n        raise KeyImportError('Invalid PEM certificate')\n\n    return SSHX509Certificate.construct_from_der(data)\n\n\ndef _decode_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> \\\n            Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a private key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_private(key_info[0], passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    elif fmt == 'pem':\n        pem_name, headers, data = key_info\n        key = _decode_pem_private(pem_name, headers, data, passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    else:\n        key = None\n\n    return key, end\n\n\ndef _decode_public(data: bytes) -> Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a public key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PUBLIC KEY', public=True)\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_public(key_info[0])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        key = _decode_pem_public(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n        key = decode_ssh_public_key(data)\n\n        if algorithm != key.algorithm:\n            raise KeyImportError('Public key algorithm mismatch')\n\n        key.set_comment(comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        key = decode_ssh_public_key(data)\n        key.set_comment(comment)\n    else:\n        fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n        if fmt == 'pem' and key_info[0] == b'OPENSSH':\n            key = _decode_openssh_public(key_info[2])\n        else:\n            key, _ = _decode_private(data, None, False)\n\n            if key:\n                key = key.convert_to_public()\n\n    return key, end\n\n\ndef _decode_certificate(data: bytes) -> \\\n        Tuple[Optional[SSHCertificate], Optional[int]]:\n    \"\"\"Decode a certificate\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'CERTIFICATE', public=True)\n\n    cert: Optional[SSHCertificate]\n\n    if fmt == 'der':\n        cert = _decode_der_certificate(data[:end])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        cert = _decode_pem_certificate(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n\n        if algorithm.startswith(b'x509v3-'):\n            cert = _decode_der_certificate(data, comment)\n        else:\n            cert = decode_ssh_certificate(data, comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        cert = decode_ssh_certificate(data, comment)\n    else:\n        cert = None\n\n    return cert, end\n\n\ndef _decode_private_list(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> Sequence[SSHKey]:\n    \"\"\"Decode a private key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_private(data, passphrase,\n                                   unsafe_skip_rsa_key_validation)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_public_list(data: bytes) -> Sequence[SSHKey]:\n    \"\"\"Decode a public key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_public(data)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_certificate_list(data: bytes) -> Sequence[SSHCertificate]:\n    \"\"\"Decode a certificate list\"\"\"\n\n    certs: List[SSHCertificate] = []\n\n    while data:\n        cert, end = _decode_certificate(data)\n\n        if cert:\n            certs.append(cert)\n\n        data = data[end:]\n\n    return certs\n\n\ndef register_sk_alg(sk_alg: int, handler: Type[SSHKey], *args: object) -> None:\n    \"\"\"Register a new security key algorithm\"\"\"\n\n    _sk_alg_map[sk_alg] = handler, args\n\n\ndef register_public_key_alg(algorithm: bytes, handler: Type[SSHKey],\n                            default: bool,\n                            sig_algorithms: Optional[Sequence[bytes]] = \\\n                                None) -> None:\n    \"\"\"Register a new public key algorithm\"\"\"\n\n    if not sig_algorithms:\n        sig_algorithms = handler.sig_algorithms\n\n    _public_key_algs.extend(sig_algorithms)\n\n    if default:\n        _default_public_key_algs.extend(sig_algorithms)\n\n    _public_key_alg_map[algorithm] = handler\n\n    if handler.pem_name:\n        _pem_map[handler.pem_name] = handler\n\n    if handler.pkcs8_oid: # pragma: no branch\n        _pkcs8_oid_map[handler.pkcs8_oid] = handler\n\n\ndef register_certificate_alg(version: int, algorithm: bytes,\n                             cert_algorithm: bytes,\n                             key_handler: Type[SSHKey],\n                             cert_handler: Type[SSHOpenSSHCertificate],\n                             default: bool) -> None:\n    \"\"\"Register a new certificate algorithm\"\"\"\n\n    _certificate_algs.append(cert_algorithm)\n\n    if default:\n        _default_certificate_algs.append(cert_algorithm)\n\n    _certificate_alg_map[cert_algorithm] = (key_handler, cert_handler)\n\n    _certificate_sig_alg_map[cert_algorithm] = algorithm\n\n    _certificate_version_map[algorithm, version] = \\\n        (cert_algorithm, cert_handler)\n\n\ndef register_x509_certificate_alg(cert_algorithm: bytes, default: bool) -> None:\n    \"\"\"Register a new X.509 certificate algorithm\"\"\"\n\n    if _x509_available: # pragma: no branch\n        _x509_certificate_algs.append(cert_algorithm)\n\n        if default:\n            _default_x509_certificate_algs.append(cert_algorithm)\n\n        _certificate_alg_map[cert_algorithm] = (None, SSHX509CertificateChain)\n\n\ndef get_public_key_algs() -> List[bytes]:\n    \"\"\"Return supported public key algorithms\"\"\"\n\n    return _public_key_algs\n\n\ndef get_default_public_key_algs() -> List[bytes]:\n    \"\"\"Return default public key algorithms\"\"\"\n\n    return _default_public_key_algs\n\n\ndef get_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported certificate-based public key algorithms\"\"\"\n\n    return _certificate_algs\n\n\ndef get_default_certificate_algs() -> List[bytes]:\n    \"\"\"Return default certificate-based public key algorithms\"\"\"\n\n    return _default_certificate_algs\n\n\ndef get_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported X.509 certificate-based public key algorithms\"\"\"\n\n    return _x509_certificate_algs\n\n\ndef get_default_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return default X.509 certificate-based public key algorithms\"\"\"\n\n    return _default_x509_certificate_algs\n\n\ndef decode_ssh_public_key(data: bytes) -> SSHKey:\n    \"\"\"Decode a packetized SSH public key\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n\n        if handler:\n            key_params = handler.decode_ssh_public(packet)\n            packet.check_end()\n\n            key = handler.make_public(key_params)\n            key.algorithm = alg\n            return key\n        else:\n            raise KeyImportError('Unknown key algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except PacketDecodeError:\n        raise KeyImportError('Invalid public key') from None\n\n\ndef decode_ssh_certificate(data: bytes,\n                           comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a packetized SSH certificate\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        key_handler, cert_handler = _certificate_alg_map.get(alg, (None, None))\n\n        if cert_handler:\n            return cert_handler.construct(packet, alg, key_handler, comment)\n        else:\n            raise KeyImportError('Unknown certificate algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except (PacketDecodeError, ValueError):\n        raise KeyImportError('Invalid OpenSSH certificate') from None\n\n\ndef generate_private_key(alg_name: str, comment: _Comment = None,\n                         **kwargs) -> SSHKey:\n    \"\"\"Generate a new private key\n\n       This function generates a new private key of a type matching\n       the requested SSH algorithm. Depending on the algorithm, additional\n       parameters can be passed which affect the generated key.\n\n       Available algorithms include:\n\n           ssh-dss, ssh-rsa, ecdsa-sha2-nistp256, ecdsa-sha2-nistp384,\n           ecdsa-sha2-nistp521, ecdsa-sha2-1.3.132.0.10, ssh-ed25519,\n           ssh-ed448, sk-ecdsa-sha2-nistp256\\\\@openssh.com,\n           sk-ssh-ed25519\\\\@openssh.com\n\n       For dss keys, no parameters are supported. The key size is fixed at\n       1024 bits due to the use of SHA1 signatures.\n\n       For rsa keys, the key size can be specified using the `key_size`\n       parameter, and the RSA public exponent can be changed using the\n       `exponent` parameter. By default, generated keys are 2048 bits\n       with a public exponent of 65537.\n\n       For ecdsa keys, the curve to use is part of the SSH algorithm name\n       and that determines the key size. No other parameters are supported.\n\n       For ed25519 and ed448 keys, no parameters are supported. The key size\n       is fixed by the algorithms at 256 bits and 448 bits, respectively.\n\n       For sk keys, the application name to associate with the generated\n       key can be specified using the `application` parameter. It defaults\n       to `'ssh:'`. The user name to associate with the generated key can\n       be specified using the `user` parameter. It defaults to `'AsyncSSH'`.\n\n       When generating an sk key, a PIN can be provided via the `pin`\n       parameter if the security key requires it.\n\n       The `resident` parameter can be set to `True` to request that a\n       resident key be created on the security key. This allows the key\n       handle and public key information to later be retrieved so that\n       the generated key can be used without having to store any\n       information on the client system. It defaults to `False`.\n\n       You can enable or disable the security key touch requirement by\n       setting the `touch_required` parameter. It defaults to `True`,\n       requiring that the user confirm their presence by touching the\n       security key each time they use it to authenticate.\n\n       :param alg_name:\n           The SSH algorithm name corresponding to the desired type of key.\n       :param comment: (optional)\n           A comment to associate with this key.\n       :param key_size: (optional)\n           The key size in bits for RSA keys.\n       :param exponent: (optional)\n           The public exponent for RSA keys.\n       :param application: (optional)\n           The application name to associate with the generated SK key,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name to associate with the generated SK key, defaulting\n           to `'AsyncSSH'`.\n       :param pin: (optional)\n           The PIN to use to access the security key, defaulting to `None`.\n       :param resident: (optional)\n           Whether or not to create a resident key on the security key,\n           defaulting to `False`.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type alg_name: `str`\n       :type comment: `str`, `bytes`, or `None`\n       :type key_size: `int`\n       :type exponent: `int`\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type resident: `bool`\n       :type touch_required: `bool`\n\n       :returns: An :class:`SSHKey` private key\n\n       :raises: :exc:`KeyGenerationError` if the requested key parameters\n                are unsupported\n    \"\"\"\n\n    algorithm = alg_name.encode('utf-8')\n    handler = _public_key_alg_map.get(algorithm)\n\n    if handler:\n        try:\n            key = handler.generate(algorithm, **kwargs)\n        except (TypeError, ValueError) as exc:\n            raise KeyGenerationError(str(exc)) from None\n    else:\n        raise KeyGenerationError('Unknown algorithm: %s' % alg_name)\n\n    key.set_comment(comment)\n    return key\n\ndef import_private_key(\n        data: BytesOrStr, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Import a private key\n\n       This function imports a private key encoded in PKCS#1 or PKCS#8 DER\n       or PEM format or OpenSSH format. Encrypted private keys can be\n       imported by specifying the passphrase needed to decrypt them.\n\n       :param data:\n           The data to import.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type data: `bytes` or ASCII `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_private(data, passphrase, unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_private_key_and_certs(\n        data: bytes, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Import a private key and optional certificate chain\"\"\"\n\n    key, end = _decode_private(data, passphrase,\n                               unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key, import_certificate_chain(data[end:])\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_public_key(data: BytesOrStr) -> SSHKey:\n    \"\"\"Import a public key\n\n       This function imports a public key encoded in OpenSSH, RFC4716, or\n       PKCS#1 or PKCS#8 DER or PEM format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_public(data)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid public key')\n\n\ndef import_certificate(data: BytesOrStr) -> SSHCertificate:\n    \"\"\"Import a certificate\n\n       This function imports an SSH certificate in DER, PEM, OpenSSH, or\n       RFC4716 format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    cert, _ = _decode_certificate(data)\n\n    if cert:\n        return cert\n    else:\n        raise KeyImportError('Invalid certificate')\n\n\ndef import_certificate_chain(data: bytes) -> Optional[SSHX509CertificateChain]:\n    \"\"\"Import an X.509 certificate chain\"\"\"\n\n    certs = _decode_certificate_list(data)\n\n    chain: Optional[SSHX509CertificateChain]\n\n    if certs:\n        chain = SSHX509CertificateChain.construct_from_certs(certs)\n    else:\n        chain = None\n\n    return chain\n\n\ndef import_certificate_subject(data: str) -> str:\n    \"\"\"Import an X.509 certificate subject name\"\"\"\n\n    try:\n        algorithm, data = data.strip().split(None, 1)\n    except ValueError:\n        raise KeyImportError('Missing certificate subject algorithm') from None\n\n    if algorithm.startswith('x509v3-'):\n        match = _subject_pattern.match(data)\n\n        if match:\n            return data[match.end():]\n\n    raise KeyImportError('Invalid certificate subject')\n\n\ndef read_private_key(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Read a private key from a file\n\n       This function reads a private key from a file. See the function\n       :func:`import_private_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    key = import_private_key(read_file(filename), passphrase,\n                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_private_key_and_certs(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Read a private key and optional certificate chain from a file\"\"\"\n\n    key, cert = import_private_key_and_certs(read_file(filename), passphrase,\n                                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key, cert\n\n\ndef read_public_key(filename: FilePath) -> SSHKey:\n    \"\"\"Read a public key from a file\n\n       This function reads a public key from a file. See the function\n       :func:`import_public_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    key = import_public_key(read_file(filename))\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_certificate(filename: FilePath) -> SSHCertificate:\n    \"\"\"Read a certificate from a file\n\n       This function reads an SSH certificate from a file. See the\n       function :func:`import_certificate` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the certificate from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    return import_certificate(read_file(filename))\n\n\ndef read_private_key_list(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKey]:\n    \"\"\"Read a list of private keys from a file\n\n       This function reads a list of private keys from a file. See the\n       function :func:`import_private_key` for information about the\n       formats supported. If any of the keys are encrypted, they must\n       all be encrypted with the same passphrase.\n\n       :param filename:\n           The file to read the keys from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKey` private keys\n\n    \"\"\"\n\n    keys = _decode_private_list(read_file(filename), passphrase,\n                                unsafe_skip_rsa_key_validation)\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_public_key_list(filename: FilePath) -> Sequence[SSHKey]:\n    \"\"\"Read a list of public keys from a file\n\n       This function reads a list of public keys from a file. See the\n       function :func:`import_public_key` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the keys from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHKey` public keys\n\n    \"\"\"\n\n    keys = _decode_public_list(read_file(filename))\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_certificate_list(filename: FilePath) -> Sequence[SSHCertificate]:\n    \"\"\"Read a list of certificates from a file\n\n       This function reads a list of SSH certificates from a file. See\n       the function :func:`import_certificate` for information about\n       the formats supported.\n\n       :param filename:\n           The file to read the certificates from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHCertificate` certificates\n\n    \"\"\"\n\n    return _decode_certificate_list(read_file(filename))\n\n\ndef load_keypairs(\n        keylist: KeyPairListArg, passphrase: Optional[BytesOrStr] = None,\n        certlist: CertListArg = (), skip_public: bool = False,\n        ignore_encrypted: bool = False,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKeyPair]:\n    \"\"\"Load SSH private keys and optional matching certificates\n\n       This function loads a list of SSH keys and optional matching\n       certificates.\n\n       When certificates are specified, the private key is added to\n       the list both with and without the certificate.\n\n       :param keylist:\n           The list of private keys and certificates to load.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt private keys.\n       :param certlist: (optional)\n           A list of certificates to attempt to pair with the provided\n           list of private keys.\n       :param skip_public: (optional)\n           An internal parameter used to skip public keys and certificates\n           when IdentitiesOnly and IdentityFile are used to specify a\n           mixture of private and public keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type keylist: *see* :ref:`SpecifyingPrivateKeys`\n       :type passphrase: `str` or `bytes`\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n       :type skip_public: `bool`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKeyPair` objects\n\n    \"\"\"\n\n    keys_to_load: Sequence[_KeyPairArg]\n    result: List[SSHKeyPair] = []\n\n    certlist = load_certificates(certlist)\n    certdict = {cert.key.public_data: cert for cert in certlist}\n\n    if isinstance(keylist, (PurePath, str)):\n        try:\n            priv_keys = read_private_key_list(keylist, passphrase,\n                                              unsafe_skip_rsa_key_validation)\n            keys_to_load = [keylist] if len(priv_keys) <= 1 else priv_keys\n        except KeyImportError:\n            keys_to_load = [keylist]\n    elif isinstance(keylist, (tuple, bytes, SSHKey, SSHKeyPair)):\n        keys_to_load = [cast(_KeyPairArg, keylist)]\n    else:\n        keys_to_load = keylist if keylist else []\n\n    for key_to_load in keys_to_load:\n        allow_certs = False\n        key_prefix = None\n        saved_exc = None\n        pubkey_or_certs = None\n        pubkey_to_load: Optional[_KeyArg] = None\n        certs_to_load: Optional[_CertArg] = None\n        key: Union['SSHKey', 'SSHKeyPair']\n\n        if isinstance(key_to_load, (PurePath, str, bytes)):\n            allow_certs = True\n        elif isinstance(key_to_load, tuple):\n            key_to_load, pubkey_or_certs = key_to_load\n\n        try:\n            if isinstance(key_to_load, (PurePath, str)):\n                key_prefix = str(key_to_load)\n\n                if allow_certs:\n                    key, certs_to_load = read_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n\n                    if not certs_to_load:\n                        certs_to_load = key_prefix + '-cert.pub'\n                else:\n                    key = read_private_key(key_to_load, passphrase,\n                                           unsafe_skip_rsa_key_validation)\n\n                pubkey_to_load = key_prefix + '.pub'\n            elif isinstance(key_to_load, bytes):\n                if allow_certs:\n                    key, certs_to_load = import_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n                else:\n                    key = import_private_key(key_to_load, passphrase,\n                                             unsafe_skip_rsa_key_validation)\n            else:\n                key = key_to_load\n        except KeyImportError as exc:\n            if skip_public or \\\n                    (ignore_encrypted and str(exc).startswith('Passphrase')):\n                continue\n\n            raise\n\n        certs: Optional[Sequence[SSHCertificate]]\n\n        if pubkey_or_certs:\n            try:\n                certs = load_certificates(pubkey_or_certs)\n            except (TypeError, OSError, KeyImportError) as exc:\n                saved_exc = exc\n                certs = None\n\n            if not certs:\n                pubkey_to_load = cast(_KeyArg, pubkey_or_certs)\n        elif certs_to_load:\n            try:\n                certs = load_certificates(certs_to_load)\n            except (OSError, KeyImportError):\n                certs = None\n        else:\n            certs = None\n\n        pubkey: Optional[SSHKey]\n\n        if pubkey_to_load:\n            try:\n                if isinstance(pubkey_to_load, (PurePath, str)):\n                    pubkey = read_public_key(pubkey_to_load)\n                elif isinstance(pubkey_to_load, bytes):\n                    pubkey = import_public_key(pubkey_to_load)\n                else:\n                    pubkey = pubkey_to_load\n            except (OSError, KeyImportError):\n                pubkey = None\n            else:\n                saved_exc = None\n        else:\n            pubkey = None\n\n        if saved_exc:\n            raise saved_exc # pylint: disable=raising-bad-type\n\n        if not certs:\n            if isinstance(key, SSHKeyPair):\n                pubdata = key.key_public_data\n            else:\n                pubdata = key.public_data\n\n            cert = certdict.get(pubdata)\n\n            if cert and cert.is_x509:\n                cert = SSHX509CertificateChain.construct_from_certs(certlist)\n        elif len(certs) == 1 and not certs[0].is_x509:\n            cert = certs[0]\n        else:\n            cert = SSHX509CertificateChain.construct_from_certs(certs)\n\n        if isinstance(key, SSHKeyPair):\n            if cert:\n                key.set_certificate(cert)\n\n            result.append(key)\n        else:\n            if cert:\n                result.append(SSHLocalKeyPair(key, pubkey, cert))\n\n            result.append(SSHLocalKeyPair(key, pubkey))\n\n    return result\n\n\ndef load_default_keypairs(passphrase: Optional[BytesOrStr] = None,\n                          certlist: CertListArg = ()) -> \\\n        Sequence[SSHKeyPair]:\n    \"\"\"Return a list of default keys from the user's home directory\"\"\"\n\n    result: List[SSHKeyPair] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                path = Path('~', '.ssh', file).expanduser()\n                result.extend(load_keypairs(path, passphrase, certlist,\n                                            ignore_encrypted=True))\n            except OSError:\n                pass\n\n    return result\n\n\ndef load_public_keys(keylist: KeyListArg) -> Sequence[SSHKey]:\n    \"\"\"Load public keys\n\n       This function loads a list of SSH public keys.\n\n       :param keylist:\n           The list of public keys to load.\n       :type keylist: *see* :ref:`SpecifyingPublicKeys`\n\n       :returns: A list of :class:`SSHKey` objects\n\n    \"\"\"\n\n    if isinstance(keylist, (PurePath, str)):\n        return read_public_key_list(keylist)\n    else:\n        result: List[SSHKey] = []\n\n        for key in keylist:\n            if isinstance(key, (PurePath, str)):\n                key = read_public_key(key)\n            elif isinstance(key, bytes):\n                key = import_public_key(key)\n\n            result.append(key)\n\n        return result\n\n\ndef load_default_host_public_keys() -> Sequence[Union[SSHKey, SSHCertificate]]:\n    \"\"\"Return a list of default host public keys or certificates\"\"\"\n\n    result: List[Union[SSHKey, SSHCertificate]] = []\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                cert = read_certificate(Path(host_key_dir, file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert)\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                key = read_public_key(Path(host_key_dir, file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key)\n\n    return result\n\n\ndef load_certificates(certlist: CertListArg) -> Sequence[SSHCertificate]:\n    \"\"\"Load certificates\n\n       This function loads a list of OpenSSH or X.509 certificates.\n\n       :param certlist:\n           The list of certificates to load.\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n\n       :returns: A list of :class:`SSHCertificate` objects\n\n    \"\"\"\n\n    if isinstance(certlist, SSHCertificate):\n        return [certlist]\n    elif isinstance(certlist, (PurePath, str, bytes)):\n        certlist = [certlist]\n\n    result: List[SSHCertificate] = []\n\n    for cert in certlist:\n        if isinstance(cert, (PurePath, str)):\n            certs = read_certificate_list(cert)\n        elif isinstance(cert, bytes):\n            certs = _decode_certificate_list(cert)\n        elif isinstance(cert, SSHCertificate):\n            certs = [cert]\n        else:\n            certs = cert\n\n        result.extend(certs)\n\n    return result\n\n\ndef load_identities(keylist: IdentityListArg,\n                    skip_private: bool = False) -> Sequence[bytes]:\n    \"\"\"Load public key and certificate identities\"\"\"\n\n    if isinstance(keylist, (bytes, str, PurePath, SSHKey, SSHCertificate)):\n        identities: Sequence[_IdentityArg] = [keylist]\n    else:\n        identities = keylist\n\n    result = []\n\n    for identity in identities:\n        if isinstance(identity, (PurePath, str)):\n            try:\n                pubdata = read_certificate(identity).public_data\n            except KeyImportError:\n                try:\n                    pubdata = read_public_key(identity).public_data\n                except KeyImportError:\n                    if skip_private:\n                        continue\n\n                    raise\n        elif isinstance(identity, (SSHKey, SSHCertificate)):\n            pubdata = identity.public_data\n        else:\n            pubdata = identity\n\n        result.append(pubdata)\n\n    return result\n\n\ndef load_default_identities() -> Sequence[bytes]:\n    \"\"\"Return a list of default public key and certificate identities\"\"\"\n\n    result: List[bytes] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                cert = read_certificate(Path('~', '.ssh', file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert.public_data)\n\n            try:\n                key = read_public_key(Path('~', '.ssh', file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key.public_data)\n\n    return result\n\n\ndef load_resident_keys(pin: str, *, application: str = 'ssh:',\n                       user: Optional[str] = None,\n                       touch_required: bool = True) -> Sequence[SSHKey]:\n    \"\"\"Load keys resident on attached FIDO2 security keys\n\n       This function loads keys resident on any FIDO2 security keys\n       currently attached to the system. The user name associated\n       with each key is returned in the key's comment field.\n\n       :param pin:\n           The PIN to use to access the security keys, defaulting to `None`.\n       :param application: (optional)\n           The application name associated with the keys to load,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name associated with the keys to load. By default,\n           keys for all users are loaded.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type touch_required: `bool`\n\n    \"\"\"\n\n    application = application.encode('utf-8')\n    flags = SSH_SK_USER_PRESENCE_REQD if touch_required else 0\n    reserved = b''\n\n    try:\n        resident_keys = sk_get_resident(application, user, pin)\n    except ValueError as exc:\n        raise KeyImportError(str(exc)) from None\n\n    result: List[SSHKey] = []\n\n    for sk_alg, name, public_value, key_handle in resident_keys:\n        handler, key_params = _sk_alg_map[sk_alg]\n        key_params += (public_value, application, flags, key_handle, reserved)\n\n        key = handler.make_private(key_params)\n        key.set_comment(name)\n\n        result.append(key)\n\n    return result\n", "prompt": "Please write a python function called 'convert_to_public' base the context. This method converts an SSHKey object that contains a private key into one that contains only the corresponding public key. It first decodes asymmetric encryption. Once decrypted, it proceeds to assign a relevant comment and filename to the associated key. Upon completion of these steps, the method returns the processed data as its final output.:param self: SSHKey. An instance of the SSHKey class.\n:return: SSHKey. The SSHKey object that contains only the corresponding public key..\n        The context you need to refer to is as follows: # Copyright (c) 2013-2023 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH asymmetric encryption handlers\"\"\"\n\nimport binascii\nimport os\nimport re\nimport time\n\nfrom datetime import datetime\nfrom hashlib import md5, sha1, sha256, sha384, sha512\nfrom pathlib import Path, PurePath\nfrom typing import Callable, Dict, List, Mapping, Optional, Sequence, Set\nfrom typing import Tuple, Type, Union, cast\nfrom typing_extensions import Protocol\n\nfrom .crypto import ed25519_available, ed448_available\nfrom .encryption import Encryption\nfrom .sk import sk_available\n\ntry:\n    # pylint: disable=unused-import\n    from .crypto import X509Certificate\n    from .crypto import generate_x509_certificate, import_x509_certificate\n    _x509_available = True\nexcept ImportError: # pragma: no cover\n    _x509_available = False\n\ntry:\n    import bcrypt\n    _bcrypt_available = hasattr(bcrypt, 'kdf')\nexcept ImportError: # pragma: no cover\n    _bcrypt_available = False\n\nfrom .asn1 import ASN1DecodeError, BitString, ObjectIdentifier\nfrom .asn1 import der_encode, der_decode, der_decode_partial\nfrom .crypto import CryptoKey, PyCAKey\nfrom .encryption import get_encryption_params, get_encryption\nfrom .misc import BytesOrStr, DefTuple, FilePath, IPNetwork\nfrom .misc import ip_network, read_file, write_file, parse_time_interval\nfrom .packet import NameList, String, UInt32, UInt64\nfrom .packet import PacketDecodeError, SSHPacket\nfrom .pbe import KeyEncryptionError, pkcs1_encrypt, pkcs8_encrypt\nfrom .pbe import pkcs1_decrypt, pkcs8_decrypt\nfrom .sk import SSH_SK_USER_PRESENCE_REQD, sk_get_resident\n\n\n_Comment = Optional[BytesOrStr]\n_CertPrincipals = Union[str, Sequence[str]]\n_Time = Union[int, float, datetime, str]\n\n_PubKeyAlgMap = Dict[bytes, Type['SSHKey']]\n_CertAlgMap = Dict[bytes, Tuple[Optional[Type['SSHKey']],\n                                Type['SSHCertificate']]]\n_CertSigAlgMap = Dict[bytes, bytes]\n_CertVersionMap = Dict[Tuple[bytes, int],\n                       Tuple[bytes, Type['SSHOpenSSHCertificate']]]\n\n_PEMMap = Dict[bytes, Type['SSHKey']]\n_PKCS8OIDMap = Dict[ObjectIdentifier, Type['SSHKey']]\n_SKAlgMap = Dict[int, Tuple[Type['SSHKey'], Tuple[object, ...]]]\n\n_OpenSSHCertOptions = Dict[str, object]\n_OpenSSHCertParams = Tuple[object, int, int, bytes, bytes,\n                           int, int, bytes, bytes]\n_OpenSSHCertEncoders = Sequence[Tuple[str, Callable[[object], bytes]]]\n_OpenSSHCertDecoders = Dict[bytes, Callable[[SSHPacket], object]]\n\nX509CertPurposes = Union[None, str, Sequence[str]]\n\n_IdentityArg = Union[bytes, FilePath, 'SSHKey', 'SSHCertificate']\nIdentityListArg = Union[_IdentityArg, Sequence[_IdentityArg]]\n_KeyArg = Union[bytes, FilePath, 'SSHKey']\nKeyListArg = Union[FilePath, Sequence[_KeyArg]]\n_CertArg = Union[bytes, FilePath, 'SSHCertificate']\nCertListArg = Union[_CertArg, Sequence[_CertArg]]\n_KeyPairArg = Union['SSHKeyPair', _KeyArg, Tuple[_KeyArg, _CertArg]]\nKeyPairListArg = Union[_KeyPairArg, Sequence[_KeyPairArg]]\n\n\n# Default file names in .ssh directory to read private keys from\n_DEFAULT_KEY_FILES = (\n    ('id_ed25519_sk', ed25519_available and sk_available),\n    ('id_ecdsa_sk', sk_available),\n    ('id_ed448', ed448_available),\n    ('id_ed25519', ed25519_available),\n    ('id_ecdsa', True),\n    ('id_rsa', True),\n    ('id_dsa', True)\n)\n\n# Default directories and file names to read host keys from\n_DEFAULT_HOST_KEY_DIRS = ('/opt/local/etc', '/opt/local/etc/ssh',\n                          '/usr/local/etc', '/usr/local/etc/ssh',\n                          '/etc', '/etc/ssh')\n_DEFAULT_HOST_KEY_FILES = ('ssh_host_ed448_key', 'ssh_host_ed25519_key',\n                           'ssh_host_ecdsa_key', 'ssh_host_rsa_key',\n                           'ssh_host_dsa_key')\n\n_hashes = {'md5': md5, 'sha1': sha1, 'sha256': sha256,\n           'sha384': sha384, 'sha512': sha512}\n\n_public_key_algs: List[bytes] = []\n_default_public_key_algs: List[bytes] = []\n\n_certificate_algs: List[bytes] = []\n_default_certificate_algs: List[bytes] = []\n\n_x509_certificate_algs: List[bytes] = []\n_default_x509_certificate_algs: List[bytes] = []\n\n_public_key_alg_map: _PubKeyAlgMap = {}\n_certificate_alg_map: _CertAlgMap = {}\n_certificate_sig_alg_map: _CertSigAlgMap = {}\n_certificate_version_map: _CertVersionMap = {}\n_pem_map: _PEMMap = {}\n_pkcs8_oid_map: _PKCS8OIDMap = {}\n_sk_alg_map: _SKAlgMap = {}\n\n_abs_date_pattern = re.compile(r'\\d{8}')\n_abs_time_pattern = re.compile(r'\\d{14}')\n\n_subject_pattern = re.compile(r'(?:Distinguished[ -_]?Name|Subject|DN)[=:]?\\s?',\n                              re.IGNORECASE)\n\n# SSH certificate types\nCERT_TYPE_USER = 1\nCERT_TYPE_HOST = 2\n\n# Flag to omit second argument in alg_params\nOMIT = object()\n\n_OPENSSH_KEY_V1 = b'openssh-key-v1\\0'\n_OPENSSH_SALT_LEN = 16\n_OPENSSH_WRAP_LEN = 70\n\n\ndef _parse_time(t: _Time) -> int:\n    \"\"\"Parse a time value\"\"\"\n\n    if isinstance(t, int):\n        return t\n    elif isinstance(t, float):\n        return int(t)\n    elif isinstance(t, datetime):\n        return int(t.timestamp())\n    elif isinstance(t, str):\n        if t == 'now':\n            return int(time.time())\n\n        match = _abs_date_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d').timestamp())\n\n        match = _abs_time_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d%H%M%S').timestamp())\n\n        try:\n            return int(time.time() + parse_time_interval(t))\n        except ValueError:\n            pass\n\n    raise ValueError('Unrecognized time value')\n\n\ndef _wrap_base64(data: bytes, wrap: int = 64) -> bytes:\n    \"\"\"Break a Base64 value into multiple lines.\"\"\"\n\n    data = binascii.b2a_base64(data)[:-1]\n    return b'\\n'.join(data[i:i+wrap]\n                      for i in range(0, len(data), wrap)) + b'\\n'\n\n\nclass KeyGenerationError(ValueError):\n    \"\"\"Key generation error\n\n       This exception is raised by :func:`generate_private_key`,\n       :meth:`generate_user_certificate() <SSHKey.generate_user_certificate>`\n       or :meth:`generate_host_certificate()\n       <SSHKey.generate_host_certificate>` when the requested parameters are\n       unsupported.\n\n    \"\"\"\n\n\nclass KeyImportError(ValueError):\n    \"\"\"Key import error\n\n       This exception is raised by key import functions when the\n       data provided cannot be imported as a valid key.\n\n    \"\"\"\n\n\nclass KeyExportError(ValueError):\n    \"\"\"Key export error\n\n       This exception is raised by key export functions when the\n       requested format is unknown or encryption is requested for a\n       format which doesn't support it.\n\n    \"\"\"\n\n\nclass SigningKey(Protocol):\n    \"\"\"Protocol for signing a block of data\"\"\"\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with a private key\"\"\"\n\n\nclass VerifyingKey(Protocol):\n    \"\"\"Protocol for verifying a signature on a block of data\"\"\"\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify a signature on a block of data with a public key\"\"\"\n\n\nclass SSHKey:\n    \"\"\"Parent class which holds an asymmetric encryption key\"\"\"\n\n    algorithm: bytes = b''\n    sig_algorithms: Sequence[bytes] = ()\n    cert_algorithms: Sequence[bytes] = ()\n    x509_algorithms: Sequence[bytes] = ()\n    all_sig_algorithms: Set[bytes] = set()\n    default_x509_hash: str = ''\n    pem_name: bytes = b''\n    pkcs8_oid: Optional[ObjectIdentifier] = None\n    use_executor: bool = False\n\n    def __init__(self, key: Optional[CryptoKey] = None):\n        self._key = key\n        self._comment: Optional[bytes] = None\n        self._filename: Optional[bytes] = None\n        self._touch_required = False\n\n    @classmethod\n    def generate(cls, algorithm: bytes, **kwargs) -> 'SSHKey':\n        \"\"\"Generate a new SSH private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_private(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_public(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a public key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def decode_pkcs1_private(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs1_public(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_private(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_public(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    @classmethod\n    def decode_ssh_private(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format private key\"\"\"\n\n    @classmethod\n    def decode_ssh_public(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format public key\"\"\"\n\n    @property\n    def private_data(self) -> bytes:\n        \"\"\"Return private key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_private()\n\n    @property\n    def public_data(self) -> bytes:\n        \"\"\"Return public key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_public()\n\n    @property\n    def pyca_key(self) -> PyCAKey:\n        \"\"\"Return PyCA key for use in X.509 module\"\"\"\n\n        assert self._key is not None\n        return self._key.pyca_key\n\n    def _generate_certificate(self, key: 'SSHKey', version: int, serial: int,\n                              cert_type: int, key_id: str,\n                              principals: _CertPrincipals,\n                              valid_after: _Time, valid_before: _Time,\n                              cert_options: _OpenSSHCertOptions,\n                              sig_alg_name: DefTuple[str],\n                              comment: DefTuple[_Comment]) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        if isinstance(principals, str):\n            principals = [p.strip() for p in principals.split(',')]\n        else:\n            principals = list(principals)\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if sig_alg_name == ():\n            sig_alg = self.sig_algorithms[0]\n        else:\n            sig_alg = cast(str, sig_alg_name).encode()\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        comment: _Comment\n\n        try:\n            algorithm, cert_handler = _certificate_version_map[key.algorithm,\n                                                               version]\n        except KeyError:\n            raise KeyGenerationError('Unknown certificate version') from None\n\n        return cert_handler.generate(self, algorithm, key, serial, cert_type,\n                                     key_id, principals, valid_after,\n                                     valid_before, cert_options,\n                                     sig_alg, comment)\n\n    def _generate_x509_certificate(self, key: 'SSHKey', subject: str,\n                                   issuer: Optional[str],\n                                   serial: Optional[int],\n                                   valid_after: _Time, valid_before: _Time,\n                                   ca: bool, ca_path_len: Optional[int],\n                                   purposes: X509CertPurposes,\n                                   user_principals: _CertPrincipals,\n                                   host_principals: _CertPrincipals,\n                                   hash_name: DefTuple[str],\n                                   comment: DefTuple[_Comment]) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if hash_name == ():\n            hash_name = key.default_x509_hash\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        hash_name: str\n        comment: _Comment\n\n        return SSHX509Certificate.generate(self, key, subject, issuer,\n                                           serial, valid_after, valid_before,\n                                           ca, ca_path_len, purposes,\n                                           user_principals, host_principals,\n                                           hash_name, comment)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this key\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key as a byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key as a Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def get_filename(self) -> Optional[bytes]:\n        \"\"\"Return the filename associated with this key\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._filename\n\n    def set_filename(self, filename: Union[None, bytes, FilePath]) -> None:\n        \"\"\"Set the filename associated with this key\n\n           :param filename:\n               The new filename to associate with this key\n           :type filename: `PurePath`, `str`, `bytes`, or `None`\n\n        \"\"\"\n\n        if isinstance(filename, PurePath):\n            filename = str(filename)\n\n        if isinstance(filename, str):\n            filename = filename.encode('utf-8')\n\n        self._filename = filename or None\n\n    def get_fingerprint(self, hash_name: str = 'sha256') -> str:\n        \"\"\"Get the fingerprint of this key\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           :param hash_name: (optional)\n               The hash algorithm to use to construct the fingerprint.\n           :type hash_name: `str`\n\n           :returns: `str`\n\n           :raises: :exc:`ValueError` if the hash name is invalid\n\n        \"\"\"\n\n        try:\n            hash_alg = _hashes[hash_name]\n        except KeyError:\n            raise ValueError('Unknown hash algorithm') from None\n\n        h = hash_alg(self.public_data)\n\n        if hash_name == 'md5':\n            fp = h.hexdigest()\n            fp_text = ':'.join(fp[i:i+2] for i in range(0, len(fp), 2))\n        else:\n            fpb = h.digest()\n            fp_text = binascii.b2a_base64(fpb).decode('ascii')[:-1].strip('=')\n\n        return hash_name.upper() + ':' + fp_text\n\n    def set_touch_required(self, touch_required: bool) -> None:\n        \"\"\"Set whether touch is required when using a security key\"\"\"\n\n        self._touch_required = touch_required\n\n    def sign_raw(self, data: bytes, hash_name: str) -> bytes:\n        \"\"\"Return a raw signature of the specified data\"\"\"\n\n        assert self._key is not None\n        return self._key.sign(data, hash_name)\n\n    def sign_ssh(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Abstract method to compute an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def verify_ssh(self, data: bytes, sig_algorithm: bytes,\n                   packet: SSHPacket) -> bool:\n        \"\"\"Abstract method to verify an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def sign(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Return an SSH-encoded signature of the specified data\"\"\"\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return b''.join((String(sig_algorithm),\n                         self.sign_ssh(data, sig_algorithm)))\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify an SSH signature of the specified data using this key\"\"\"\n\n        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False\n\n    def encode_pkcs1_private(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 private key export not supported')\n\n    def encode_pkcs1_public(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 public key export not supported')\n\n    def encode_pkcs8_private(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 private key export not supported')\n\n    def encode_pkcs8_public(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 public key export not supported')\n\n    def encode_ssh_private(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH private key export not supported')\n\n    def encode_ssh_public(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH public key export not supported')\n\n    def encode_agent_cert_private(self) -> bytes:\n        \"\"\"Encode certificate private key data for agent\"\"\"\n\n        raise NotImplementedError\n\n###The function: convert_to_public###\n    def generate_user_certificate(\n            self, user_key: 'SSHKey', key_id: str, version: int = 1,\n            serial: int = 0, principals: _CertPrincipals = (),\n            valid_after: _Time = 0, valid_before: _Time = 0xffffffffffffffff,\n            force_command: Optional[str] = None,\n            source_address: Optional[Sequence[str]] = None,\n            permit_x11_forwarding: bool = True,\n            permit_agent_forwarding: bool = True,\n            permit_port_forwarding: bool = True, permit_pty: bool = True,\n            permit_user_rc: bool = True, touch_required: bool = True,\n            sig_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH user certificate\n\n           This method returns an SSH user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param force_command: (optional)\n               The command (if any) to force a session to run when this\n               certificate is used.\n           :param source_address: (optional)\n               A list of source addresses and networks for which the\n               certificate is valid, defaulting to all addresses.\n           :param permit_x11_forwarding: (optional)\n               Whether or not to allow this user to use X11 forwarding,\n               defaulting to `True`.\n           :param permit_agent_forwarding: (optional)\n               Whether or not to allow this user to use agent forwarding,\n               defaulting to `True`.\n           :param permit_port_forwarding: (optional)\n               Whether or not to allow this user to use port forwarding,\n               defaulting to `True`.\n           :param permit_pty: (optional)\n               Whether or not to allow this user to allocate a\n               pseudo-terminal, defaulting to `True`.\n           :param permit_user_rc: (optional)\n               Whether or not to run the user rc file when this certificate\n               is used, defaulting to `True`.\n           :param touch_required: (optional)\n               Whether or not to require the user to touch the security key\n               when authenticating with it, defaulting to `True`.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type force_command: `str` or `None`\n           :type source_address: list of ip_address and ip_network values\n           :type permit_x11_forwarding: `bool`\n           :type permit_agent_forwarding: `bool`\n           :type permit_port_forwarding: `bool`\n           :type permit_pty: `bool`\n           :type permit_user_rc: `bool`\n           :type touch_required: `bool`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        cert_options: _OpenSSHCertOptions = {}\n\n        if force_command:\n            cert_options['force-command'] = force_command\n\n        if source_address:\n            cert_options['source-address'] = [ip_network(addr)\n                                              for addr in source_address]\n\n        if permit_x11_forwarding:\n            cert_options['permit-X11-forwarding'] = True\n\n        if permit_agent_forwarding:\n            cert_options['permit-agent-forwarding'] = True\n\n        if permit_port_forwarding:\n            cert_options['permit-port-forwarding'] = True\n\n        if permit_pty:\n            cert_options['permit-pty'] = True\n\n        if permit_user_rc:\n            cert_options['permit-user-rc'] = True\n\n        if not touch_required:\n            cert_options['no-touch-required'] = True\n\n        return self._generate_certificate(user_key, version, serial,\n                                          CERT_TYPE_USER, key_id,\n                                          principals, valid_after,\n                                          valid_before, cert_options,\n                                          sig_alg, comment)\n\n    def generate_host_certificate(self, host_key: 'SSHKey', key_id: str,\n                                  version: int = 1, serial: int = 0,\n                                  principals: _CertPrincipals = (),\n                                  valid_after: _Time = 0,\n                                  valid_before: _Time = 0xffffffffffffffff,\n                                  sig_alg: DefTuple[str] = (),\n                                  comment: DefTuple[_Comment] = ()) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH host certificate\n\n           This method returns an SSH host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        if comment == ():\n            comment = host_key.get_comment_bytes()\n\n        return self._generate_certificate(host_key, version, serial,\n                                          CERT_TYPE_HOST, key_id,\n                                          principals, valid_after,\n                                          valid_before, {}, sig_alg, comment)\n\n    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 user certificate\n\n           This method returns an X.509 user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellClient'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n\n    def generate_x509_host_certificate(\n            self, host_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellServer',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 host certificate\n\n           This method returns an X.509 host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellServer'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(host_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)\n\n    def generate_x509_ca_certificate(self, ca_key: 'SSHKey', subject: str,\n                                     issuer: Optional[str] = None,\n                                     serial: Optional[int] = None,\n                                     valid_after: _Time = 0,\n                                     valid_before: _Time = 0xffffffffffffffff,\n                                     ca_path_len: Optional[int] = None,\n                                     hash_alg: DefTuple[str] = (),\n                                     comment: DefTuple[_Comment] = ()) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 CA certificate\n\n           This method returns an X.509 CA certificate with the requested\n           attributes signed by this private key.\n\n           :param ca_key:\n               The new CA's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param ca_path_len: (optional)\n               The maximum number of levels of intermediate CAs allowed\n               below this new CA or `None` to not enforce a limit,\n               defaulting to no limit.\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               ca_key.\n           :type ca_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type ca_path_len: `int` or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(ca_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, True,\n                                               ca_path_len, None, (), (),\n                                               hash_alg, comment)\n\n    def export_private_key(self, format_name: str = 'openssh',\n                           passphrase: Optional[BytesOrStr] = None,\n                           cipher_name: str = 'aes256-cbc',\n                           hash_name: str = 'sha256',\n                           pbe_version: int = 2, rounds: int = 128,\n                           ignore_few_rounds: bool = False) -> bytes:\n        \"\"\"Export a private key in the requested format\n\n           This method returns this object's private key encoded in the\n           requested format. If a passphrase is specified, the key will\n           be exported in encrypted form.\n\n           Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh\n\n           By default, openssh format will be used.\n\n           Encryption is supported in pkcs1-pem, pkcs8-der, pkcs8-pem,\n           and openssh formats. For pkcs1-pem, only the cipher can be\n           specified. For pkcs8-der and pkcs-8, cipher,  hash and PBE\n           version can be specified. For openssh, cipher and rounds\n           can be specified.\n\n           Available ciphers for pkcs1-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, des-cbc, des3-cbc\n\n           Available ciphers for pkcs8-der and pkcs8-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, blowfish-cbc,\n               cast128-cbc, des-cbc, des2-cbc, des3-cbc, rc4-40, rc4-128\n\n           Available ciphers for openssh format include the following\n           :ref:`encryption algorithms <EncryptionAlgs>`.\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           Available PBE versions include 1 for PBES1 and 2 for PBES2.\n\n           Not all combinations of cipher, hash, and version are supported.\n\n           The default cipher is aes256. In the pkcs8 formats, the default\n           hash is sha256 and default version is PBES2.\n\n           In openssh format, the default number of rounds is 128.\n\n           .. note:: The openssh format uses bcrypt for encryption, but\n                     unlike the traditional bcrypt cost factor used in\n                     password hashing which scales logarithmically, the\n                     encryption strength here scales linearly with the\n                     rounds value. Since the cipher is rekeyed 64 times\n                     per round, the default rounds value of 128 corresponds\n                     to 8192 total iterations, which is the equivalent of\n                     a bcrypt cost factor of 13.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :param passphrase: (optional)\n               A passphrase to encrypt the private key with.\n           :param cipher_name: (optional)\n               The cipher to use for private key encryption.\n           :param hash_name: (optional)\n               The hash to use for private key encryption.\n           :param pbe_version: (optional)\n               The PBE version to use for private key encryption.\n           :param rounds: (optional)\n               The number of KDF rounds to apply to the passphrase.\n           :type format_name: `str`\n           :type passphrase: `str` or `bytes`\n           :type cipher_name: `str`\n           :type hash_name: `str`\n           :type pbe_version: `int`\n           :type rounds: `int`\n\n           :returns: `bytes` representing the exported private key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_private())\n\n            if passphrase is not None:\n                if format_name == 'pkcs1-der':\n                    raise KeyExportError('PKCS#1 DER format does not support '\n                                         'private key encryption')\n\n                alg, iv, data = pkcs1_encrypt(data, cipher_name, passphrase)\n                headers = (b'Proc-Type: 4,ENCRYPTED\\n' +\n                           b'DEK-Info: ' + alg + b',' +\n                           binascii.b2a_hex(iv).upper() + b'\\n\\n')\n            else:\n                headers = b''\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PRIVATE KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        headers + _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_private()\n\n            if alg_params is OMIT:\n                data = der_encode((0, (self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode((0, (self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if passphrase is not None:\n                data = pkcs8_encrypt(data, cipher_name, hash_name,\n                                     pbe_version, passphrase)\n\n            if format_name == 'pkcs8-pem':\n                if passphrase is not None:\n                    keytype = b'ENCRYPTED PRIVATE KEY'\n                else:\n                    keytype = b'PRIVATE KEY'\n\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            check = os.urandom(4)\n            nkeys = 1\n\n            data = b''.join((check, check, self.private_data,\n                             String(self._comment or b'')))\n\n            cipher: Optional[Encryption]\n\n            if passphrase is not None:\n                try:\n                    alg = cipher_name.encode('ascii')\n                    key_size, iv_size, block_size, _, _, _ = \\\n                        get_encryption_params(alg)\n                except (KeyError, UnicodeEncodeError):\n                    raise KeyEncryptionError('Unknown cipher: %s' %\n                                             cipher_name) from None\n\n                if not _bcrypt_available: # pragma: no cover\n                    raise KeyExportError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n                kdf = b'bcrypt'\n                salt = os.urandom(_OPENSSH_SALT_LEN)\n                kdf_data = b''.join((String(salt), UInt32(rounds)))\n\n                if isinstance(passphrase, str):\n                    passphrase = passphrase.encode('utf-8')\n\n                key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                 rounds, ignore_few_rounds)\n\n                cipher = get_encryption(alg, key[:key_size], key[key_size:])\n                block_size = max(block_size, 8)\n            else:\n                cipher = None\n                alg = b'none'\n                kdf = b'none'\n                kdf_data = b''\n                block_size = 8\n                mac = b''\n\n            pad = len(data) % block_size\n            if pad: # pragma: no branch\n                data = data + bytes(range(1, block_size + 1 - pad))\n\n            if cipher:\n                data, mac = cipher.encrypt_packet(0, b'', data)\n            else:\n                mac = b''\n\n            data = b''.join((_OPENSSH_KEY_V1, String(alg), String(kdf),\n                             String(kdf_data), UInt32(nkeys),\n                             String(self.public_data), String(data), mac))\n\n            return (b'-----BEGIN OPENSSH PRIVATE KEY-----\\n' +\n                    _wrap_base64(data, _OPENSSH_WRAP_LEN) +\n                    b'-----END OPENSSH PRIVATE KEY-----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def export_public_key(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a public key in the requested format\n\n           This method returns this object's public key encoded in the\n           requested format. Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh, rfc4716\n\n           By default, openssh format will be used.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported public key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_public())\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PUBLIC KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_public()\n            pkcs8_data = BitString(pkcs8_data)\n\n            if alg_params is OMIT:\n                data = der_encode(((self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode(((self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if format_name == 'pkcs8-pem':\n                data = (b'-----BEGIN PUBLIC KEY-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END PUBLIC KEY-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs))\n\n    def write_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs))\n\n    def append_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs), 'ab')\n\n    def append_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs), 'ab')\n\n\nclass SSHCertificate:\n    \"\"\"Parent class which holds an SSH certificate\"\"\"\n\n    is_x509 = False\n    is_x509_chain = False\n\n    def __init__(self, algorithm: bytes, sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes], key: SSHKey,\n                 public_data: bytes, comment: _Comment):\n        self.algorithm = algorithm\n        self.sig_algorithms = sig_algorithms\n        self.host_key_algorithms = host_key_algorithms\n        self.key = key\n        self.public_data = public_data\n\n        self.set_comment(comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        raise NotImplementedError\n\n    def __eq__(self, other: object) -> bool:\n        return (isinstance(other, type(self)) and\n                self.public_data == other.public_data)\n\n    def __hash__(self) -> int:\n        return hash(self.public_data)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this certificate\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this certificate\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this certificate as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this certificate as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        return self._comment.decode(encoding, errors) if self._comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this certificate\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def export_certificate(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a certificate in the requested format\n\n           This function returns this certificate encoded in the requested\n           format. Available formats include:\n\n               der, pem, openssh, rfc4716\n\n           By default, OpenSSH format will be used.\n\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported certificate\n\n        \"\"\"\n\n        if self.is_x509:\n            if format_name == 'rfc4716':\n                raise KeyExportError('RFC4716 format is not supported for '\n                                     'X.509 certificates')\n        else:\n            if format_name in ('der', 'pem'):\n                raise KeyExportError('DER and PEM formats are not supported '\n                                     'for OpenSSH certificates')\n\n        if format_name == 'der':\n            return self.public_data\n        elif format_name == 'pem':\n            return (b'-----BEGIN CERTIFICATE-----\\n' +\n                    _wrap_base64(self.public_data) +\n                    b'-----END CERTIFICATE-----\\n')\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_certificate(self, filename: FilePath,\n                          format_name: str = 'openssh') -> None:\n        \"\"\"Write a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which writes the exported certificate to a file.\n\n           :param filename:\n               The filename to write the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name))\n\n    def append_certificate(self, filename: FilePath,\n                           format_name: str = 'openssh') -> None:\n        \"\"\"Append a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which appends the exported certificate to an existing file.\n\n           :param filename:\n               The filename to append the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name), 'ab')\n\n\nclass SSHOpenSSHCertificate(SSHCertificate):\n    \"\"\"Class which holds an OpenSSH certificate\"\"\"\n\n    _user_option_encoders: _OpenSSHCertEncoders = ()\n    _user_extension_encoders: _OpenSSHCertEncoders = ()\n    _host_option_encoders: _OpenSSHCertEncoders = ()\n    _host_extension_encoders: _OpenSSHCertEncoders = ()\n\n    _user_option_decoders: _OpenSSHCertDecoders = {}\n    _user_extension_decoders: _OpenSSHCertDecoders = {}\n    _host_option_decoders: _OpenSSHCertDecoders = {}\n    _host_extension_decoders: _OpenSSHCertDecoders = {}\n\n    def __init__(self, algorithm: bytes, key: SSHKey, data: bytes,\n                 principals: Sequence[str], options: _OpenSSHCertOptions,\n                 signing_key: SSHKey, serial: int, cert_type: int,\n                 key_id: str, valid_after: int, valid_before: int,\n                 comment: _Comment):\n        super().__init__(algorithm, key.sig_algorithms,\n                         key.cert_algorithms or (algorithm,),\n                         key, data, comment)\n\n        self.principals = principals\n        self.options = options\n        self.signing_key = signing_key\n\n        self._serial = serial\n        self._cert_type = cert_type\n        self._key_id = key_id\n        self._valid_after = valid_after\n        self._valid_before = valid_before\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', algorithm: bytes, key: 'SSHKey',\n                 serial: int, cert_type: int, key_id: str,\n                 principals: Sequence[str], valid_after: int,\n                 valid_before: int, options: _OpenSSHCertOptions,\n                 sig_alg: bytes, comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        principal_bytes = b''.join(String(p) for p in principals)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._encode_options(options,\n                                               cls._user_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._user_extension_encoders)\n        else:\n            cert_options = cls._encode_options(options,\n                                               cls._host_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._host_extension_encoders)\n\n        key = key.convert_to_public()\n\n        data = b''.join((String(algorithm),\n                         cls._encode(key, serial, cert_type, key_id,\n                                     principal_bytes, valid_after,\n                                     valid_before, cert_options,\n                                     cert_extensions),\n                         String(signing_key.public_data)))\n\n        data += String(signing_key.sign(data, sig_alg))\n\n        signing_key = signing_key.convert_to_public()\n\n        return cls(algorithm, key, data, principals, options, signing_key,\n                   serial, cert_type, key_id, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        assert key_handler is not None\n\n        key_params, serial, cert_type, key_id, \\\n            principals, valid_after, valid_before, \\\n            options, extensions = cls._decode(packet, key_handler)\n\n        signing_key = decode_ssh_public_key(packet.get_string())\n        data = packet.get_consumed_payload()\n        signature = packet.get_string()\n        packet.check_end()\n\n        if not signing_key.verify(data, signature):\n            raise KeyImportError('Invalid certificate signature')\n\n        key = key_handler.make_public(key_params)\n        data = packet.get_consumed_payload()\n\n        try:\n            key_id_bytes = key_id.decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in key ID') from None\n\n        packet = SSHPacket(principals)\n        principals: List[str] = []\n\n        while packet:\n            try:\n                principal = packet.get_string().decode('utf-8')\n            except UnicodeDecodeError:\n                raise KeyImportError('Invalid characters in principal '\n                                     'name') from None\n\n            principals.append(principal)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._decode_options(\n                options, cls._user_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._user_extension_decoders, False))\n        elif cert_type == CERT_TYPE_HOST:\n            cert_options = cls._decode_options(\n                options, cls._host_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._host_extension_decoders, False))\n        else:\n            raise KeyImportError('Unknown certificate type')\n\n        return cls(algorithm, key, data, principals, cert_options, signing_key,\n                   serial, cert_type, key_id_bytes, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n\n        \"\"\"Encode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def _encode_options(options: _OpenSSHCertOptions,\n                        encoders: _OpenSSHCertEncoders) -> bytes:\n        \"\"\"Encode options found in this certificate\"\"\"\n\n        result = []\n\n        for name, encoder in encoders:\n            value = options.get(name)\n            if value:\n                result.append(String(name) + String(encoder(value)))\n\n        return b''.join(result)\n\n    @staticmethod\n    def _encode_bool(_value: object) -> bytes:\n        \"\"\"Encode a boolean option value\"\"\"\n\n        return b''\n\n    @staticmethod\n    def _encode_force_cmd(force_command: object) -> bytes:\n        \"\"\"Encode a force-command option\"\"\"\n\n        return String(cast(BytesOrStr, force_command))\n\n    @staticmethod\n    def _encode_source_addr(source_address: object) -> bytes:\n        \"\"\"Encode a source-address option\"\"\"\n\n        return NameList(str(addr).encode('ascii')\n                        for addr in cast(Sequence[IPNetwork], source_address))\n\n    @staticmethod\n    def _decode_bool(_packet: SSHPacket) -> bool:\n        \"\"\"Decode a boolean option value\"\"\"\n\n        return True\n\n    @staticmethod\n    def _decode_force_cmd(packet: SSHPacket) -> str:\n        \"\"\"Decode a force-command option\"\"\"\n\n        try:\n            return packet.get_string().decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in command') from None\n\n    @staticmethod\n    def _decode_source_addr(packet: SSHPacket) -> Sequence[IPNetwork]:\n        \"\"\"Decode a source-address option\"\"\"\n\n        try:\n            return [ip_network(addr.decode('ascii'))\n                    for addr in packet.get_namelist()]\n        except (UnicodeDecodeError, ValueError):\n            raise KeyImportError('Invalid source address') from None\n\n    @staticmethod\n    def _decode_options(options: bytes, decoders: _OpenSSHCertDecoders,\n                        critical: bool = True) -> _OpenSSHCertOptions:\n        \"\"\"Decode options found in this certificate\"\"\"\n\n        packet = SSHPacket(options)\n        result: _OpenSSHCertOptions = {}\n\n        while packet:\n            name = packet.get_string()\n\n            decoder = decoders.get(name)\n            if decoder:\n                data_packet = SSHPacket(packet.get_string())\n                result[name.decode('ascii')] = decoder(data_packet)\n                data_packet.check_end()\n            elif critical:\n                raise KeyImportError('Unrecognized critical option: %s' %\n                                     name.decode('ascii', errors='replace'))\n\n        return result\n\n    def validate(self, cert_type: int, principal: str) -> None:\n        \"\"\"Validate an OpenSSH certificate\"\"\"\n\n        if self._cert_type != cert_type:\n            raise ValueError('Invalid certificate type')\n\n        now = time.time()\n\n        if now < self._valid_after:\n            raise ValueError('Certificate not yet valid')\n\n        if now >= self._valid_before:\n            raise ValueError('Certificate expired')\n\n        if principal and self.principals and principal not in self.principals:\n            raise ValueError('Certificate principal mismatch')\n\n\nclass SSHOpenSSHCertificateV01(SSHOpenSSHCertificate):\n    \"\"\"Encoder/decoder class for version 01 OpenSSH certificates\"\"\"\n\n    _user_option_encoders = (\n        ('force-command',           SSHOpenSSHCertificate._encode_force_cmd),\n        ('source-address',          SSHOpenSSHCertificate._encode_source_addr)\n    )\n\n    _user_extension_encoders = (\n        ('permit-X11-forwarding',   SSHOpenSSHCertificate._encode_bool),\n        ('permit-agent-forwarding', SSHOpenSSHCertificate._encode_bool),\n        ('permit-port-forwarding',  SSHOpenSSHCertificate._encode_bool),\n        ('permit-pty',              SSHOpenSSHCertificate._encode_bool),\n        ('permit-user-rc',          SSHOpenSSHCertificate._encode_bool),\n        ('no-touch-required',       SSHOpenSSHCertificate._encode_bool)\n    )\n\n    _user_option_decoders = {\n        b'force-command':           SSHOpenSSHCertificate._decode_force_cmd,\n        b'source-address':          SSHOpenSSHCertificate._decode_source_addr\n    }\n\n    _user_extension_decoders = {\n        b'permit-X11-forwarding':   SSHOpenSSHCertificate._decode_bool,\n        b'permit-agent-forwarding': SSHOpenSSHCertificate._decode_bool,\n        b'permit-port-forwarding':  SSHOpenSSHCertificate._decode_bool,\n        b'permit-pty':              SSHOpenSSHCertificate._decode_bool,\n        b'permit-user-rc':          SSHOpenSSHCertificate._decode_bool,\n        b'no-touch-required':       SSHOpenSSHCertificate._decode_bool\n    }\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n        \"\"\"Encode a version 01 SSH certificate\"\"\"\n\n        return b''.join((String(os.urandom(32)), key.encode_ssh_public(),\n                         UInt64(serial), UInt32(cert_type), String(key_id),\n                         String(principals), UInt64(valid_after),\n                         UInt64(valid_before), String(options),\n                         String(extensions), String('')))\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode a version 01 SSH certificate\"\"\"\n\n        _ = packet.get_string()                             # nonce\n        key_params = key_handler.decode_ssh_public(packet)\n        serial = packet.get_uint64()\n        cert_type = packet.get_uint32()\n        key_id = packet.get_string()\n        principals = packet.get_string()\n        valid_after = packet.get_uint64()\n        valid_before = packet.get_uint64()\n        options = packet.get_string()\n        extensions = packet.get_string()\n        _ = packet.get_string()                             # reserved\n\n        return (key_params, serial, cert_type, key_id, principals,\n                valid_after, valid_before, options, extensions)\n\n\nclass SSHX509Certificate(SSHCertificate):\n    \"\"\"Encoder/decoder class for SSH X.509 certificates\"\"\"\n\n    is_x509 = True\n\n    def __init__(self, key: SSHKey, x509_cert: 'X509Certificate',\n                 comment: _Comment = None):\n        super().__init__(b'x509v3-' + key.algorithm, key.x509_algorithms,\n                         key.x509_algorithms, key, x509_cert.data,\n                         x509_cert.comment or comment)\n\n        self.subject = x509_cert.subject\n        self.issuer = x509_cert.issuer\n        self.issuer_hash = x509_cert.issuer_hash\n        self.user_principals = x509_cert.user_principals\n        self.x509_cert = x509_cert\n\n    def _expand_trust_store(self, cert: 'SSHX509Certificate',\n                            trusted_cert_paths: Sequence[FilePath],\n                            trust_store: Set['SSHX509Certificate']) -> None:\n        \"\"\"Look up certificates by issuer hash to build a trust store\"\"\"\n\n        issuer_hash = cert.issuer_hash\n\n        for path in trusted_cert_paths:\n            idx = 0\n\n            try:\n                while True:\n                    cert_path = Path(path, issuer_hash + '.' + str(idx))\n                    idx += 1\n\n                    c = cast('SSHX509Certificate', read_certificate(cert_path))\n\n                    if c.subject != cert.issuer or c in trust_store:\n                        continue\n\n                    trust_store.add(c)\n                    self._expand_trust_store(c, trusted_cert_paths, trust_store)\n            except (OSError, KeyImportError):\n                pass\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        raise RuntimeError # pragma: no cover\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', key: 'SSHKey', subject: str,\n                 issuer: Optional[str], serial: Optional[int],\n                 valid_after: int, valid_before: int, ca: bool,\n                 ca_path_len: Optional[int], purposes: X509CertPurposes,\n                 user_principals: _CertPrincipals,\n                 host_principals: _CertPrincipals, hash_name: str,\n                 comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        key = key.convert_to_public()\n\n        x509_cert = generate_x509_certificate(signing_key.pyca_key,\n                                              key.pyca_key, subject, issuer,\n                                              serial, valid_after, valid_before,\n                                              ca, ca_path_len, purposes,\n                                              user_principals, host_principals,\n                                              hash_name, comment)\n\n        return cls(key, x509_cert)\n\n    @classmethod\n    def construct_from_der(cls, data: bytes,\n                           comment: _Comment = None) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from DER data\"\"\"\n\n        try:\n            x509_cert = import_x509_certificate(data)\n            key = import_public_key(x509_cert.key_data)\n        except ValueError as exc:\n            raise KeyImportError(str(exc)) from None\n\n        return cls(key, x509_cert, comment)\n\n    def validate_chain(self, trust_chain: Sequence['SSHX509Certificate'],\n                       trusted_certs: Sequence['SSHX509Certificate'],\n                       trusted_cert_paths: Sequence[FilePath],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        trust_store = set(c for c in trust_chain if c.subject != c.issuer) | \\\n            set(c for c in trusted_certs)\n\n        if trusted_cert_paths:\n            self._expand_trust_store(self, trusted_cert_paths, trust_store)\n\n            for c in trust_chain:\n                self._expand_trust_store(c, trusted_cert_paths, trust_store)\n\n        self.x509_cert.validate([c.x509_cert for c in trust_store],\n                                purposes, user_principal, host_principal)\n\n\nclass SSHX509CertificateChain(SSHCertificate):\n    \"\"\"Encoder/decoder class for an SSH X.509 certificate chain\"\"\"\n\n    is_x509_chain = True\n\n    def __init__(self, algorithm: bytes, certs: Sequence[SSHCertificate],\n                 ocsp_responses: Sequence[bytes], comment: _Comment):\n        key = certs[0].key\n        data = self._public_data(algorithm, certs, ocsp_responses)\n\n        super().__init__(algorithm, key.x509_algorithms, key.x509_algorithms,\n                         key, data, comment)\n\n        x509_certs = cast(Sequence[SSHX509Certificate], certs)\n        first_cert = x509_certs[0]\n        last_cert = x509_certs[-1]\n\n        self.subject = first_cert.subject\n        self.issuer = last_cert.issuer\n        self.user_principals = first_cert.user_principals\n\n        self._certs = x509_certs\n        self._ocsp_responses = ocsp_responses\n\n    @staticmethod\n    def _public_data(algorithm: bytes, certs: Sequence[SSHCertificate],\n                     ocsp_responses: Sequence[bytes]) -> bytes:\n        \"\"\"Return the X509 chain public data\"\"\"\n\n        return (String(algorithm) + UInt32(len(certs)) +\n                b''.join(String(c.public_data) for c in certs) +\n                UInt32(len(ocsp_responses)) +\n                b''.join(String(resp) for resp in ocsp_responses))\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        cert_count = packet.get_uint32()\n        certs = [import_certificate(packet.get_string())\n                 for _ in range(cert_count)]\n\n        ocsp_resp_count = packet.get_uint32()\n        ocsp_responses = [packet.get_string() for _ in range(ocsp_resp_count)]\n\n        packet.check_end()\n\n        if not certs:\n            raise KeyImportError('No certificates present')\n\n        return cls(algorithm, certs, ocsp_responses, comment)\n\n    @classmethod\n    def construct_from_certs(cls, certs: Sequence['SSHCertificate']) -> \\\n            'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate chain from certificates\"\"\"\n\n        cert = certs[0]\n\n        return cls(cert.algorithm, certs, (), cert.get_comment_bytes())\n\n    def adjust_public_data(self, algorithm: bytes) -> bytes:\n        \"\"\"Adjust public data to reflect chosen signature algorithm\"\"\"\n\n        return self._public_data(algorithm, self._certs, self._ocsp_responses)\n\n    def validate_chain(self, trusted_certs: Sequence[SSHX509Certificate],\n                       trusted_cert_paths: Sequence[FilePath],\n                       revoked_certs: Set[SSHX509Certificate],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        if revoked_certs:\n            for cert in self._certs:\n                if cert in revoked_certs:\n                    raise ValueError('Revoked X.509 certificate in '\n                                     'certificate chain')\n\n        self._certs[0].validate_chain(self._certs[1:], trusted_certs,\n                                      trusted_cert_paths, purposes,\n                                      user_principal, host_principal)\n\n\nclass SSHKeyPair:\n    \"\"\"Parent class which represents an asymmetric key pair\n\n       This is an abstract class which provides a method to sign data\n       with a private key and members to access the corresponding\n       algorithm and public key or certificate information needed to\n       identify what key was used for signing.\n\n    \"\"\"\n\n    _key_type = 'unknown'\n\n    def __init__(self, algorithm: bytes, sig_algorithm: bytes,\n                 sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes],\n                 public_data: bytes, comment: _Comment,\n                 cert: Optional[SSHCertificate] = None,\n                 filename: Optional[bytes] = None,\n                 use_executor: bool = False):\n        self.key_algorithm = algorithm\n        self.key_public_data = public_data\n\n        self.set_comment(comment)\n        self._cert = cert\n        self._filename = filename\n\n        self.use_executor = use_executor\n\n        if cert:\n            if cert.key.public_data != self.key_public_data:\n                raise ValueError('Certificate key mismatch')\n\n            self.algorithm = cert.algorithm\n\n            if cert.is_x509_chain:\n                self.sig_algorithm = cert.algorithm\n            else:\n                self.sig_algorithm = sig_algorithm\n\n            self.sig_algorithms = cert.sig_algorithms\n            self.host_key_algorithms = cert.host_key_algorithms\n            self.public_data = cert.public_data\n        else:\n            self.algorithm = algorithm\n            self.sig_algorithm = algorithm\n            self.sig_algorithms = sig_algorithms\n            self.host_key_algorithms = host_key_algorithms\n            self.public_data = public_data\n\n    def get_key_type(self) -> str:\n        \"\"\"Return what type of key pair this is\n\n           This method returns 'local' for locally loaded keys, and\n           'agent' for keys managed by an SSH agent.\n\n        \"\"\"\n\n        return self._key_type\n\n    @property\n    def has_cert(self) -> bool:\n        \"\"\" Return if this key pair has an associated cert\"\"\"\n\n        return bool(self._cert)\n\n    @property\n    def has_x509_chain(self) -> bool:\n        \"\"\" Return if this key pair has an associated X.509 cert chain\"\"\"\n\n        return self._cert.is_x509_chain if self._cert else False\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key pair\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyImportError('Private key export to agent not supported')\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key pair as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key pair as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key pair\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def set_certificate(self, cert: SSHCertificate) -> None:\n        \"\"\"Set certificate to use with this key\"\"\"\n\n        if cert.key.public_data != self.key_public_data:\n            raise ValueError('Certificate key mismatch')\n\n        self._cert = cert\n        self.algorithm = cert.algorithm\n\n        if cert.is_x509_chain:\n            self.sig_algorithm = cert.algorithm\n        else:\n            self.sig_algorithm = self.key_algorithm\n\n        self.sig_algorithms = cert.sig_algorithms\n        self.host_key_algorithms = cert.host_key_algorithms\n        self.public_data = cert.public_data\n\n    def set_sig_algorithm(self, sig_algorithm: bytes) -> None:\n        \"\"\"Set the signature algorithm to use when signing data\"\"\"\n\n        try:\n            sig_algorithm = _certificate_sig_alg_map[sig_algorithm]\n        except KeyError:\n            pass\n\n        self.sig_algorithm = sig_algorithm\n\n        if not self.has_cert:\n            self.algorithm = sig_algorithm\n        elif self.has_x509_chain:\n            self.algorithm = sig_algorithm\n\n            cert = cast('SSHX509CertificateChain', self._cert)\n            self.public_data = cert.adjust_public_data(sig_algorithm)\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise RuntimeError # pragma: no cover\n\n\nclass SSHLocalKeyPair(SSHKeyPair):\n    \"\"\"Class which holds a local asymmetric key pair\n\n       This class holds a private key and associated public data\n       which can either be the matching public key or a certificate\n       which has signed that public key.\n\n    \"\"\"\n\n    _key_type = 'local'\n\n    def __init__(self, key: SSHKey, pubkey: Optional[SSHKey] = None,\n                 cert: Optional[SSHCertificate] = None):\n        if pubkey and pubkey.public_data != key.public_data:\n            raise ValueError('Public key mismatch')\n\n        if key.has_comment():\n            comment = key.get_comment_bytes()\n        elif cert and cert.has_comment():\n            comment = cert.get_comment_bytes()\n        elif pubkey and pubkey.has_comment():\n            comment = pubkey.get_comment_bytes()\n        else:\n            comment = None\n\n        super().__init__(key.algorithm, key.algorithm, key.sig_algorithms,\n                         key.sig_algorithms, key.public_data, comment, cert,\n                         key.get_filename(), key.use_executor)\n\n        self._key = key\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        if self._cert:\n            data = String(self.public_data) + \\\n                       self._key.encode_agent_cert_private()\n        else:\n            data = self._key.encode_ssh_private()\n\n        return String(self.algorithm) + data\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        return self._key.sign(data, self.sig_algorithm)\n\n\ndef _parse_openssh(data: bytes) -> Tuple[bytes, Optional[bytes], bytes]:\n    \"\"\"Parse an OpenSSH format public key or certificate\"\"\"\n\n    line = data.split(None, 2)\n\n    if len(line) < 2:\n        raise KeyImportError('Invalid OpenSSH public key or certificate')\n    elif len(line) == 2:\n        comment = None\n    else:\n        comment = line[2]\n\n    if (line[0] not in _public_key_alg_map and\n            line[0] not in _certificate_alg_map):\n        raise KeyImportError('Unknown OpenSSH public key algorithm')\n\n    try:\n        return line[0], comment, binascii.a2b_base64(line[1])\n    except binascii.Error:\n        raise KeyImportError('Invalid OpenSSH public key '\n                             'or certificate') from None\n\n\ndef _parse_pem(data: bytes) -> Tuple[Mapping[bytes, bytes], bytes]:\n    \"\"\"Parse a PEM data block\"\"\"\n\n    start = 0\n    end: Optional[int] = None\n    headers: Dict[bytes, bytes] = {}\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if b':' in line:\n            hdr, value = line.split(b':', 1)\n            headers[hdr.strip()] = value.strip()\n        else:\n            break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return headers, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid PEM data') from None\n\n\ndef _parse_rfc4716(data: bytes) -> Tuple[Optional[bytes], bytes]:\n    \"\"\"Parse an RFC 4716 data block\"\"\"\n\n    start = 0\n    end = None\n    hdr = b''\n    comment = None\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if line[-1:] == b'\\\\':\n            hdr += line[:-1]\n        else:\n            hdr += line\n            if b':' in hdr:\n                hdr, value = hdr.split(b':', 1)\n\n                if hdr.strip() == b'Comment':\n                    comment = value.strip()\n                    if comment[:1] == b'\"' and comment[-1:] == b'\"':\n                        comment = comment[1:-1]\n\n                hdr = b''\n            else:\n                break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return comment, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid RFC 4716 data') from None\n\n\ndef _match_block(data: bytes, start: int, header: bytes,\n                 fmt: str) -> Tuple[bytes, int]:\n    \"\"\"Match a block of data wrapped in a header/footer\"\"\"\n\n    match = re.compile(b'^' + header[:5] + b'END' + header[10:] +\n                       rb'[ \\t\\r\\f\\v]*$', re.M).search(data, start)\n\n    if not match:\n        raise KeyImportError('Missing %s footer' % fmt)\n\n    return data[start:match.start()], match.end()\n\n\ndef _match_next(data: bytes, keytype: bytes, public: bool = False) -> \\\n        Tuple[Optional[str], Tuple, Optional[int]]:\n    \"\"\"Find the next key/certificate and call the appropriate decode\"\"\"\n\n    end: Optional[int]\n\n    if data.startswith(b'\\x30'):\n        try:\n            key_data, end = der_decode_partial(data)\n            return 'der', (key_data,), end\n        except ASN1DecodeError:\n            pass\n\n    start = 0\n    end = None\n\n    while end != 0:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if (line.startswith(b'-----BEGIN ') and\n                line.endswith(b' ' + keytype + b'-----')):\n            pem_name = line[11:-(6+len(keytype))].strip()\n            data, end = _match_block(data, end, line, 'PEM')\n            headers, data = _parse_pem(data)\n            return 'pem', (pem_name, headers, data), end\n        elif public:\n            if line == b'---- BEGIN SSH2 PUBLIC KEY ----':\n                data, end = _match_block(data, end, line, 'RFC 4716')\n                return 'rfc4716', _parse_rfc4716(data), end\n            else:\n                try:\n                    cert = _parse_openssh(line)\n                except KeyImportError:\n                    pass\n                else:\n                    return 'openssh', cert, (end if end else len(data))\n\n        start = end\n\n    return None, (), len(data)\n\n\ndef _decode_pkcs1_private(\n        pem_name: bytes, key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_private(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s private key' %\n                             pem_name.decode('ascii'))\n\n    if pem_name == b'RSA':\n        key_params = cast(Tuple, key_params) + \\\n            (unsafe_skip_rsa_key_validation,)\n\n    return handler.make_private(key_params)\n\n\ndef _decode_pkcs1_public(pem_name: bytes, key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_public(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s public key' %\n                             pem_name.decode('ascii'))\n\n    return handler.make_public(key_params)\n\n\ndef _decode_pkcs8_private(\n        key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) >= 3 and\n            key_data[0] in (0, 1) and isinstance(key_data[1], tuple) and\n            1 <= len(key_data[1]) <= 2 and isinstance(key_data[2], bytes)):\n        if len(key_data[1]) == 2:\n            alg, alg_params = key_data[1]\n        else:\n            alg, alg_params = key_data[1][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_private(alg_params, key_data[2])\n        if key_params is None:\n            raise KeyImportError('Invalid %s private key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        if alg == ObjectIdentifier('1.2.840.113549.1.1.1'):\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        return handler.make_private(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 private key')\n\n\ndef _decode_pkcs8_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) == 2 and\n            isinstance(key_data[0], tuple) and 1 <= len(key_data[0]) <= 2 and\n            isinstance(key_data[1], BitString) and key_data[1].unused == 0):\n        if len(key_data[0]) == 2:\n            alg, alg_params = key_data[0]\n        else:\n            alg, alg_params = key_data[0][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_public(alg_params, key_data[1].value)\n        if key_params is None:\n            raise KeyImportError('Invalid %s public key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        return handler.make_public(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 public key')\n\n\ndef _decode_openssh_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode an OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        cipher_name = packet.get_string()\n        kdf = packet.get_string()\n        kdf_data = packet.get_string()\n        nkeys = packet.get_uint32()\n        _ = packet.get_string()                 # public_key\n        key_data = packet.get_string()\n        mac = packet.get_remaining_payload()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if cipher_name != b'none':\n            if passphrase is None:\n                raise KeyImportError('Passphrase must be specified to import '\n                                     'encrypted private keys')\n\n            try:\n                key_size, iv_size, block_size, _, _, _ = \\\n                    get_encryption_params(cipher_name)\n            except KeyError:\n                raise KeyEncryptionError('Unknown cipher: %s' %\n                                         cipher_name.decode('ascii')) from None\n\n            if kdf != b'bcrypt':\n                raise KeyEncryptionError('Unknown kdf: %s' %\n                                         kdf.decode('ascii'))\n\n            if not _bcrypt_available: # pragma: no cover\n                raise KeyEncryptionError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n            packet = SSHPacket(kdf_data)\n            salt = packet.get_string()\n            rounds = packet.get_uint32()\n            packet.check_end()\n\n            if isinstance(passphrase, str):\n                passphrase = passphrase.encode('utf-8')\n\n            try:\n                bcrypt_key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                        rounds, ignore_few_rounds=True)\n            except ValueError:\n                raise KeyEncryptionError('Invalid OpenSSH '\n                                         'private key') from None\n\n            cipher = get_encryption(cipher_name, bcrypt_key[:key_size],\n                                    bcrypt_key[key_size:])\n\n            decrypted_key = cipher.decrypt_packet(0, b'', key_data, 0, mac)\n\n            if decrypted_key is None:\n                raise KeyEncryptionError('Incorrect passphrase')\n\n            key_data = decrypted_key\n            block_size = max(block_size, 8)\n        else:\n            block_size = 8\n\n        packet = SSHPacket(key_data)\n\n        check1 = packet.get_uint32()\n        check2 = packet.get_uint32()\n        if check1 != check2:\n            if cipher_name != b'none':\n                raise KeyEncryptionError('Incorrect passphrase') from None\n            else:\n                raise KeyImportError('Invalid OpenSSH private key')\n\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n        if not handler:\n            raise KeyImportError('Unknown OpenSSH private key algorithm')\n\n        key_params = handler.decode_ssh_private(packet)\n        comment = packet.get_string()\n        pad = packet.get_remaining_payload()\n\n        if len(pad) >= block_size or pad != bytes(range(1, len(pad) + 1)):\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if alg == b'ssh-rsa':\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        key = handler.make_private(key_params)\n        key.set_comment(comment)\n        return key\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_openssh_public(data: bytes) -> SSHKey:\n    \"\"\"Decode public key within OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        _ = packet.get_string()                 # cipher_name\n        _ = packet.get_string()                 # kdf\n        _ = packet.get_string()                 # kdf_data\n        nkeys = packet.get_uint32()\n        pubkey = packet.get_string()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        return decode_ssh_public_key(pubkey)\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_der_private(\n        key_data: object, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a DER format private key\"\"\"\n\n    # First, if there's a passphrase, try to decrypt PKCS#8\n    if passphrase is not None:\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            # Decryption failed - try decoding it as unencrypted\n            pass\n\n    # Then, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_private(pem_name, key_data,\n                                         unsafe_skip_rsa_key_validation)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER private key')\n\n\ndef _decode_der_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a DER format public key\"\"\"\n\n    # First, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_public(key_data)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_public(pem_name, key_data)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER public key')\n\n\ndef _decode_der_certificate(data: bytes,\n                            comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a DER format X.509 certificate\"\"\"\n\n    return SSHX509Certificate.construct_from_der(data, comment)\n\n\ndef _decode_pem_private(\n        pem_name: bytes, headers: Mapping[bytes, bytes],\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PEM format private key\"\"\"\n\n    if pem_name == b'OPENSSH':\n        return _decode_openssh_private(data, passphrase,\n                                       unsafe_skip_rsa_key_validation)\n\n    if headers.get(b'Proc-Type') == b'4,ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        dek_info = headers.get(b'DEK-Info', b'').split(b',')\n        if len(dek_info) != 2:\n            raise KeyImportError('Invalid PEM encryption params')\n\n        alg, iv = dek_info\n        try:\n            iv = binascii.a2b_hex(iv)\n        except binascii.Error:\n            raise KeyImportError('Invalid PEM encryption params') from None\n\n        try:\n            data = pkcs1_decrypt(data, alg, iv, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#1 '\n                                 'private key') from None\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM private key') from None\n\n    if pem_name == b'ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        pem_name = b''\n\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#8 '\n                                 'private key') from None\n\n    if pem_name:\n        return _decode_pkcs1_private(pem_name, key_data,\n                                     unsafe_skip_rsa_key_validation)\n    else:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n\n\ndef _decode_pem_public(pem_name: bytes, data: bytes) -> SSHKey:\n    \"\"\"Decode a PEM format public key\"\"\"\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM public key') from None\n\n    if pem_name:\n        return _decode_pkcs1_public(pem_name, key_data)\n    else:\n        return _decode_pkcs8_public(key_data)\n\n\ndef _decode_pem_certificate(pem_name: bytes, data: bytes) -> SSHCertificate:\n    \"\"\"Decode a PEM format X.509 certificate\"\"\"\n\n    if pem_name == b'TRUSTED':\n        # Strip off OpenSSL trust information\n        try:\n            _, end = der_decode_partial(data)\n            data = data[:end]\n        except ASN1DecodeError:\n            raise KeyImportError('Invalid PEM trusted certificate') from None\n    elif pem_name:\n        raise KeyImportError('Invalid PEM certificate')\n\n    return SSHX509Certificate.construct_from_der(data)\n\n\ndef _decode_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> \\\n            Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a private key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_private(key_info[0], passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    elif fmt == 'pem':\n        pem_name, headers, data = key_info\n        key = _decode_pem_private(pem_name, headers, data, passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    else:\n        key = None\n\n    return key, end\n\n\ndef _decode_public(data: bytes) -> Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a public key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PUBLIC KEY', public=True)\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_public(key_info[0])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        key = _decode_pem_public(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n        key = decode_ssh_public_key(data)\n\n        if algorithm != key.algorithm:\n            raise KeyImportError('Public key algorithm mismatch')\n\n        key.set_comment(comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        key = decode_ssh_public_key(data)\n        key.set_comment(comment)\n    else:\n        fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n        if fmt == 'pem' and key_info[0] == b'OPENSSH':\n            key = _decode_openssh_public(key_info[2])\n        else:\n            key, _ = _decode_private(data, None, False)\n\n            if key:\n                key = key.convert_to_public()\n\n    return key, end\n\n\ndef _decode_certificate(data: bytes) -> \\\n        Tuple[Optional[SSHCertificate], Optional[int]]:\n    \"\"\"Decode a certificate\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'CERTIFICATE', public=True)\n\n    cert: Optional[SSHCertificate]\n\n    if fmt == 'der':\n        cert = _decode_der_certificate(data[:end])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        cert = _decode_pem_certificate(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n\n        if algorithm.startswith(b'x509v3-'):\n            cert = _decode_der_certificate(data, comment)\n        else:\n            cert = decode_ssh_certificate(data, comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        cert = decode_ssh_certificate(data, comment)\n    else:\n        cert = None\n\n    return cert, end\n\n\ndef _decode_private_list(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> Sequence[SSHKey]:\n    \"\"\"Decode a private key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_private(data, passphrase,\n                                   unsafe_skip_rsa_key_validation)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_public_list(data: bytes) -> Sequence[SSHKey]:\n    \"\"\"Decode a public key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_public(data)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_certificate_list(data: bytes) -> Sequence[SSHCertificate]:\n    \"\"\"Decode a certificate list\"\"\"\n\n    certs: List[SSHCertificate] = []\n\n    while data:\n        cert, end = _decode_certificate(data)\n\n        if cert:\n            certs.append(cert)\n\n        data = data[end:]\n\n    return certs\n\n\ndef register_sk_alg(sk_alg: int, handler: Type[SSHKey], *args: object) -> None:\n    \"\"\"Register a new security key algorithm\"\"\"\n\n    _sk_alg_map[sk_alg] = handler, args\n\n\ndef register_public_key_alg(algorithm: bytes, handler: Type[SSHKey],\n                            default: bool,\n                            sig_algorithms: Optional[Sequence[bytes]] = \\\n                                None) -> None:\n    \"\"\"Register a new public key algorithm\"\"\"\n\n    if not sig_algorithms:\n        sig_algorithms = handler.sig_algorithms\n\n    _public_key_algs.extend(sig_algorithms)\n\n    if default:\n        _default_public_key_algs.extend(sig_algorithms)\n\n    _public_key_alg_map[algorithm] = handler\n\n    if handler.pem_name:\n        _pem_map[handler.pem_name] = handler\n\n    if handler.pkcs8_oid: # pragma: no branch\n        _pkcs8_oid_map[handler.pkcs8_oid] = handler\n\n\ndef register_certificate_alg(version: int, algorithm: bytes,\n                             cert_algorithm: bytes,\n                             key_handler: Type[SSHKey],\n                             cert_handler: Type[SSHOpenSSHCertificate],\n                             default: bool) -> None:\n    \"\"\"Register a new certificate algorithm\"\"\"\n\n    _certificate_algs.append(cert_algorithm)\n\n    if default:\n        _default_certificate_algs.append(cert_algorithm)\n\n    _certificate_alg_map[cert_algorithm] = (key_handler, cert_handler)\n\n    _certificate_sig_alg_map[cert_algorithm] = algorithm\n\n    _certificate_version_map[algorithm, version] = \\\n        (cert_algorithm, cert_handler)\n\n\ndef register_x509_certificate_alg(cert_algorithm: bytes, default: bool) -> None:\n    \"\"\"Register a new X.509 certificate algorithm\"\"\"\n\n    if _x509_available: # pragma: no branch\n        _x509_certificate_algs.append(cert_algorithm)\n\n        if default:\n            _default_x509_certificate_algs.append(cert_algorithm)\n\n        _certificate_alg_map[cert_algorithm] = (None, SSHX509CertificateChain)\n\n\ndef get_public_key_algs() -> List[bytes]:\n    \"\"\"Return supported public key algorithms\"\"\"\n\n    return _public_key_algs\n\n\ndef get_default_public_key_algs() -> List[bytes]:\n    \"\"\"Return default public key algorithms\"\"\"\n\n    return _default_public_key_algs\n\n\ndef get_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported certificate-based public key algorithms\"\"\"\n\n    return _certificate_algs\n\n\ndef get_default_certificate_algs() -> List[bytes]:\n    \"\"\"Return default certificate-based public key algorithms\"\"\"\n\n    return _default_certificate_algs\n\n\ndef get_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported X.509 certificate-based public key algorithms\"\"\"\n\n    return _x509_certificate_algs\n\n\ndef get_default_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return default X.509 certificate-based public key algorithms\"\"\"\n\n    return _default_x509_certificate_algs\n\n\ndef decode_ssh_public_key(data: bytes) -> SSHKey:\n    \"\"\"Decode a packetized SSH public key\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n\n        if handler:\n            key_params = handler.decode_ssh_public(packet)\n            packet.check_end()\n\n            key = handler.make_public(key_params)\n            key.algorithm = alg\n            return key\n        else:\n            raise KeyImportError('Unknown key algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except PacketDecodeError:\n        raise KeyImportError('Invalid public key') from None\n\n\ndef decode_ssh_certificate(data: bytes,\n                           comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a packetized SSH certificate\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        key_handler, cert_handler = _certificate_alg_map.get(alg, (None, None))\n\n        if cert_handler:\n            return cert_handler.construct(packet, alg, key_handler, comment)\n        else:\n            raise KeyImportError('Unknown certificate algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except (PacketDecodeError, ValueError):\n        raise KeyImportError('Invalid OpenSSH certificate') from None\n\n\ndef generate_private_key(alg_name: str, comment: _Comment = None,\n                         **kwargs) -> SSHKey:\n    \"\"\"Generate a new private key\n\n       This function generates a new private key of a type matching\n       the requested SSH algorithm. Depending on the algorithm, additional\n       parameters can be passed which affect the generated key.\n\n       Available algorithms include:\n\n           ssh-dss, ssh-rsa, ecdsa-sha2-nistp256, ecdsa-sha2-nistp384,\n           ecdsa-sha2-nistp521, ecdsa-sha2-1.3.132.0.10, ssh-ed25519,\n           ssh-ed448, sk-ecdsa-sha2-nistp256\\\\@openssh.com,\n           sk-ssh-ed25519\\\\@openssh.com\n\n       For dss keys, no parameters are supported. The key size is fixed at\n       1024 bits due to the use of SHA1 signatures.\n\n       For rsa keys, the key size can be specified using the `key_size`\n       parameter, and the RSA public exponent can be changed using the\n       `exponent` parameter. By default, generated keys are 2048 bits\n       with a public exponent of 65537.\n\n       For ecdsa keys, the curve to use is part of the SSH algorithm name\n       and that determines the key size. No other parameters are supported.\n\n       For ed25519 and ed448 keys, no parameters are supported. The key size\n       is fixed by the algorithms at 256 bits and 448 bits, respectively.\n\n       For sk keys, the application name to associate with the generated\n       key can be specified using the `application` parameter. It defaults\n       to `'ssh:'`. The user name to associate with the generated key can\n       be specified using the `user` parameter. It defaults to `'AsyncSSH'`.\n\n       When generating an sk key, a PIN can be provided via the `pin`\n       parameter if the security key requires it.\n\n       The `resident` parameter can be set to `True` to request that a\n       resident key be created on the security key. This allows the key\n       handle and public key information to later be retrieved so that\n       the generated key can be used without having to store any\n       information on the client system. It defaults to `False`.\n\n       You can enable or disable the security key touch requirement by\n       setting the `touch_required` parameter. It defaults to `True`,\n       requiring that the user confirm their presence by touching the\n       security key each time they use it to authenticate.\n\n       :param alg_name:\n           The SSH algorithm name corresponding to the desired type of key.\n       :param comment: (optional)\n           A comment to associate with this key.\n       :param key_size: (optional)\n           The key size in bits for RSA keys.\n       :param exponent: (optional)\n           The public exponent for RSA keys.\n       :param application: (optional)\n           The application name to associate with the generated SK key,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name to associate with the generated SK key, defaulting\n           to `'AsyncSSH'`.\n       :param pin: (optional)\n           The PIN to use to access the security key, defaulting to `None`.\n       :param resident: (optional)\n           Whether or not to create a resident key on the security key,\n           defaulting to `False`.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type alg_name: `str`\n       :type comment: `str`, `bytes`, or `None`\n       :type key_size: `int`\n       :type exponent: `int`\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type resident: `bool`\n       :type touch_required: `bool`\n\n       :returns: An :class:`SSHKey` private key\n\n       :raises: :exc:`KeyGenerationError` if the requested key parameters\n                are unsupported\n    \"\"\"\n\n    algorithm = alg_name.encode('utf-8')\n    handler = _public_key_alg_map.get(algorithm)\n\n    if handler:\n        try:\n            key = handler.generate(algorithm, **kwargs)\n        except (TypeError, ValueError) as exc:\n            raise KeyGenerationError(str(exc)) from None\n    else:\n        raise KeyGenerationError('Unknown algorithm: %s' % alg_name)\n\n    key.set_comment(comment)\n    return key\n\ndef import_private_key(\n        data: BytesOrStr, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Import a private key\n\n       This function imports a private key encoded in PKCS#1 or PKCS#8 DER\n       or PEM format or OpenSSH format. Encrypted private keys can be\n       imported by specifying the passphrase needed to decrypt them.\n\n       :param data:\n           The data to import.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type data: `bytes` or ASCII `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_private(data, passphrase, unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_private_key_and_certs(\n        data: bytes, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Import a private key and optional certificate chain\"\"\"\n\n    key, end = _decode_private(data, passphrase,\n                               unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key, import_certificate_chain(data[end:])\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_public_key(data: BytesOrStr) -> SSHKey:\n    \"\"\"Import a public key\n\n       This function imports a public key encoded in OpenSSH, RFC4716, or\n       PKCS#1 or PKCS#8 DER or PEM format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_public(data)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid public key')\n\n\ndef import_certificate(data: BytesOrStr) -> SSHCertificate:\n    \"\"\"Import a certificate\n\n       This function imports an SSH certificate in DER, PEM, OpenSSH, or\n       RFC4716 format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    cert, _ = _decode_certificate(data)\n\n    if cert:\n        return cert\n    else:\n        raise KeyImportError('Invalid certificate')\n\n\ndef import_certificate_chain(data: bytes) -> Optional[SSHX509CertificateChain]:\n    \"\"\"Import an X.509 certificate chain\"\"\"\n\n    certs = _decode_certificate_list(data)\n\n    chain: Optional[SSHX509CertificateChain]\n\n    if certs:\n        chain = SSHX509CertificateChain.construct_from_certs(certs)\n    else:\n        chain = None\n\n    return chain\n\n\ndef import_certificate_subject(data: str) -> str:\n    \"\"\"Import an X.509 certificate subject name\"\"\"\n\n    try:\n        algorithm, data = data.strip().split(None, 1)\n    except ValueError:\n        raise KeyImportError('Missing certificate subject algorithm') from None\n\n    if algorithm.startswith('x509v3-'):\n        match = _subject_pattern.match(data)\n\n        if match:\n            return data[match.end():]\n\n    raise KeyImportError('Invalid certificate subject')\n\n\ndef read_private_key(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Read a private key from a file\n\n       This function reads a private key from a file. See the function\n       :func:`import_private_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    key = import_private_key(read_file(filename), passphrase,\n                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_private_key_and_certs(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Read a private key and optional certificate chain from a file\"\"\"\n\n    key, cert = import_private_key_and_certs(read_file(filename), passphrase,\n                                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key, cert\n\n\ndef read_public_key(filename: FilePath) -> SSHKey:\n    \"\"\"Read a public key from a file\n\n       This function reads a public key from a file. See the function\n       :func:`import_public_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    key = import_public_key(read_file(filename))\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_certificate(filename: FilePath) -> SSHCertificate:\n    \"\"\"Read a certificate from a file\n\n       This function reads an SSH certificate from a file. See the\n       function :func:`import_certificate` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the certificate from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    return import_certificate(read_file(filename))\n\n\ndef read_private_key_list(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKey]:\n    \"\"\"Read a list of private keys from a file\n\n       This function reads a list of private keys from a file. See the\n       function :func:`import_private_key` for information about the\n       formats supported. If any of the keys are encrypted, they must\n       all be encrypted with the same passphrase.\n\n       :param filename:\n           The file to read the keys from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKey` private keys\n\n    \"\"\"\n\n    keys = _decode_private_list(read_file(filename), passphrase,\n                                unsafe_skip_rsa_key_validation)\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_public_key_list(filename: FilePath) -> Sequence[SSHKey]:\n    \"\"\"Read a list of public keys from a file\n\n       This function reads a list of public keys from a file. See the\n       function :func:`import_public_key` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the keys from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHKey` public keys\n\n    \"\"\"\n\n    keys = _decode_public_list(read_file(filename))\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_certificate_list(filename: FilePath) -> Sequence[SSHCertificate]:\n    \"\"\"Read a list of certificates from a file\n\n       This function reads a list of SSH certificates from a file. See\n       the function :func:`import_certificate` for information about\n       the formats supported.\n\n       :param filename:\n           The file to read the certificates from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHCertificate` certificates\n\n    \"\"\"\n\n    return _decode_certificate_list(read_file(filename))\n\n\ndef load_keypairs(\n        keylist: KeyPairListArg, passphrase: Optional[BytesOrStr] = None,\n        certlist: CertListArg = (), skip_public: bool = False,\n        ignore_encrypted: bool = False,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKeyPair]:\n    \"\"\"Load SSH private keys and optional matching certificates\n\n       This function loads a list of SSH keys and optional matching\n       certificates.\n\n       When certificates are specified, the private key is added to\n       the list both with and without the certificate.\n\n       :param keylist:\n           The list of private keys and certificates to load.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt private keys.\n       :param certlist: (optional)\n           A list of certificates to attempt to pair with the provided\n           list of private keys.\n       :param skip_public: (optional)\n           An internal parameter used to skip public keys and certificates\n           when IdentitiesOnly and IdentityFile are used to specify a\n           mixture of private and public keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type keylist: *see* :ref:`SpecifyingPrivateKeys`\n       :type passphrase: `str` or `bytes`\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n       :type skip_public: `bool`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKeyPair` objects\n\n    \"\"\"\n\n    keys_to_load: Sequence[_KeyPairArg]\n    result: List[SSHKeyPair] = []\n\n    certlist = load_certificates(certlist)\n    certdict = {cert.key.public_data: cert for cert in certlist}\n\n    if isinstance(keylist, (PurePath, str)):\n        try:\n            priv_keys = read_private_key_list(keylist, passphrase,\n                                              unsafe_skip_rsa_key_validation)\n            keys_to_load = [keylist] if len(priv_keys) <= 1 else priv_keys\n        except KeyImportError:\n            keys_to_load = [keylist]\n    elif isinstance(keylist, (tuple, bytes, SSHKey, SSHKeyPair)):\n        keys_to_load = [cast(_KeyPairArg, keylist)]\n    else:\n        keys_to_load = keylist if keylist else []\n\n    for key_to_load in keys_to_load:\n        allow_certs = False\n        key_prefix = None\n        saved_exc = None\n        pubkey_or_certs = None\n        pubkey_to_load: Optional[_KeyArg] = None\n        certs_to_load: Optional[_CertArg] = None\n        key: Union['SSHKey', 'SSHKeyPair']\n\n        if isinstance(key_to_load, (PurePath, str, bytes)):\n            allow_certs = True\n        elif isinstance(key_to_load, tuple):\n            key_to_load, pubkey_or_certs = key_to_load\n\n        try:\n            if isinstance(key_to_load, (PurePath, str)):\n                key_prefix = str(key_to_load)\n\n                if allow_certs:\n                    key, certs_to_load = read_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n\n                    if not certs_to_load:\n                        certs_to_load = key_prefix + '-cert.pub'\n                else:\n                    key = read_private_key(key_to_load, passphrase,\n                                           unsafe_skip_rsa_key_validation)\n\n                pubkey_to_load = key_prefix + '.pub'\n            elif isinstance(key_to_load, bytes):\n                if allow_certs:\n                    key, certs_to_load = import_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n                else:\n                    key = import_private_key(key_to_load, passphrase,\n                                             unsafe_skip_rsa_key_validation)\n            else:\n                key = key_to_load\n        except KeyImportError as exc:\n            if skip_public or \\\n                    (ignore_encrypted and str(exc).startswith('Passphrase')):\n                continue\n\n            raise\n\n        certs: Optional[Sequence[SSHCertificate]]\n\n        if pubkey_or_certs:\n            try:\n                certs = load_certificates(pubkey_or_certs)\n            except (TypeError, OSError, KeyImportError) as exc:\n                saved_exc = exc\n                certs = None\n\n            if not certs:\n                pubkey_to_load = cast(_KeyArg, pubkey_or_certs)\n        elif certs_to_load:\n            try:\n                certs = load_certificates(certs_to_load)\n            except (OSError, KeyImportError):\n                certs = None\n        else:\n            certs = None\n\n        pubkey: Optional[SSHKey]\n\n        if pubkey_to_load:\n            try:\n                if isinstance(pubkey_to_load, (PurePath, str)):\n                    pubkey = read_public_key(pubkey_to_load)\n                elif isinstance(pubkey_to_load, bytes):\n                    pubkey = import_public_key(pubkey_to_load)\n                else:\n                    pubkey = pubkey_to_load\n            except (OSError, KeyImportError):\n                pubkey = None\n            else:\n                saved_exc = None\n        else:\n            pubkey = None\n\n        if saved_exc:\n            raise saved_exc # pylint: disable=raising-bad-type\n\n        if not certs:\n            if isinstance(key, SSHKeyPair):\n                pubdata = key.key_public_data\n            else:\n                pubdata = key.public_data\n\n            cert = certdict.get(pubdata)\n\n            if cert and cert.is_x509:\n                cert = SSHX509CertificateChain.construct_from_certs(certlist)\n        elif len(certs) == 1 and not certs[0].is_x509:\n            cert = certs[0]\n        else:\n            cert = SSHX509CertificateChain.construct_from_certs(certs)\n\n        if isinstance(key, SSHKeyPair):\n            if cert:\n                key.set_certificate(cert)\n\n            result.append(key)\n        else:\n            if cert:\n                result.append(SSHLocalKeyPair(key, pubkey, cert))\n\n            result.append(SSHLocalKeyPair(key, pubkey))\n\n    return result\n\n\ndef load_default_keypairs(passphrase: Optional[BytesOrStr] = None,\n                          certlist: CertListArg = ()) -> \\\n        Sequence[SSHKeyPair]:\n    \"\"\"Return a list of default keys from the user's home directory\"\"\"\n\n    result: List[SSHKeyPair] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                path = Path('~', '.ssh', file).expanduser()\n                result.extend(load_keypairs(path, passphrase, certlist,\n                                            ignore_encrypted=True))\n            except OSError:\n                pass\n\n    return result\n\n\ndef load_public_keys(keylist: KeyListArg) -> Sequence[SSHKey]:\n    \"\"\"Load public keys\n\n       This function loads a list of SSH public keys.\n\n       :param keylist:\n           The list of public keys to load.\n       :type keylist: *see* :ref:`SpecifyingPublicKeys`\n\n       :returns: A list of :class:`SSHKey` objects\n\n    \"\"\"\n\n    if isinstance(keylist, (PurePath, str)):\n        return read_public_key_list(keylist)\n    else:\n        result: List[SSHKey] = []\n\n        for key in keylist:\n            if isinstance(key, (PurePath, str)):\n                key = read_public_key(key)\n            elif isinstance(key, bytes):\n                key = import_public_key(key)\n\n            result.append(key)\n\n        return result\n\n\ndef load_default_host_public_keys() -> Sequence[Union[SSHKey, SSHCertificate]]:\n    \"\"\"Return a list of default host public keys or certificates\"\"\"\n\n    result: List[Union[SSHKey, SSHCertificate]] = []\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                cert = read_certificate(Path(host_key_dir, file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert)\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                key = read_public_key(Path(host_key_dir, file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key)\n\n    return result\n\n\ndef load_certificates(certlist: CertListArg) -> Sequence[SSHCertificate]:\n    \"\"\"Load certificates\n\n       This function loads a list of OpenSSH or X.509 certificates.\n\n       :param certlist:\n           The list of certificates to load.\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n\n       :returns: A list of :class:`SSHCertificate` objects\n\n    \"\"\"\n\n    if isinstance(certlist, SSHCertificate):\n        return [certlist]\n    elif isinstance(certlist, (PurePath, str, bytes)):\n        certlist = [certlist]\n\n    result: List[SSHCertificate] = []\n\n    for cert in certlist:\n        if isinstance(cert, (PurePath, str)):\n            certs = read_certificate_list(cert)\n        elif isinstance(cert, bytes):\n            certs = _decode_certificate_list(cert)\n        elif isinstance(cert, SSHCertificate):\n            certs = [cert]\n        else:\n            certs = cert\n\n        result.extend(certs)\n\n    return result\n\n\ndef load_identities(keylist: IdentityListArg,\n                    skip_private: bool = False) -> Sequence[bytes]:\n    \"\"\"Load public key and certificate identities\"\"\"\n\n    if isinstance(keylist, (bytes, str, PurePath, SSHKey, SSHCertificate)):\n        identities: Sequence[_IdentityArg] = [keylist]\n    else:\n        identities = keylist\n\n    result = []\n\n    for identity in identities:\n        if isinstance(identity, (PurePath, str)):\n            try:\n                pubdata = read_certificate(identity).public_data\n            except KeyImportError:\n                try:\n                    pubdata = read_public_key(identity).public_data\n                except KeyImportError:\n                    if skip_private:\n                        continue\n\n                    raise\n        elif isinstance(identity, (SSHKey, SSHCertificate)):\n            pubdata = identity.public_data\n        else:\n            pubdata = identity\n\n        result.append(pubdata)\n\n    return result\n\n\ndef load_default_identities() -> Sequence[bytes]:\n    \"\"\"Return a list of default public key and certificate identities\"\"\"\n\n    result: List[bytes] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                cert = read_certificate(Path('~', '.ssh', file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert.public_data)\n\n            try:\n                key = read_public_key(Path('~', '.ssh', file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key.public_data)\n\n    return result\n\n\ndef load_resident_keys(pin: str, *, application: str = 'ssh:',\n                       user: Optional[str] = None,\n                       touch_required: bool = True) -> Sequence[SSHKey]:\n    \"\"\"Load keys resident on attached FIDO2 security keys\n\n       This function loads keys resident on any FIDO2 security keys\n       currently attached to the system. The user name associated\n       with each key is returned in the key's comment field.\n\n       :param pin:\n           The PIN to use to access the security keys, defaulting to `None`.\n       :param application: (optional)\n           The application name associated with the keys to load,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name associated with the keys to load. By default,\n           keys for all users are loaded.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type touch_required: `bool`\n\n    \"\"\"\n\n    application = application.encode('utf-8')\n    flags = SSH_SK_USER_PRESENCE_REQD if touch_required else 0\n    reserved = b''\n\n    try:\n        resident_keys = sk_get_resident(application, user, pin)\n    except ValueError as exc:\n        raise KeyImportError(str(exc)) from None\n\n    result: List[SSHKey] = []\n\n    for sk_alg, name, public_value, key_handle in resident_keys:\n        handler, key_params = _sk_alg_map[sk_alg]\n        key_params += (public_value, application, flags, key_handle, reserved)\n\n        key = handler.make_private(key_params)\n        key.set_comment(name)\n\n        result.append(key)\n\n    return result\n", "test_list": ["@agent_test\nasync def test_lock(self, agent):\n    \"\"\"Test lock and unlock\"\"\"\n    key = get_test_key('ecdsa-sha2-nistp256')\n    pubkey = key.convert_to_public()\n    await agent.add_keys([key])\n    agent_keys = await agent.get_keys()\n    await agent.lock('passphrase')\n    for agent_key in agent_keys:\n        with self.assertRaises(ValueError):\n            await agent_key.sign_async(b'test')\n    await agent.unlock('passphrase')\n    for agent_key in agent_keys:\n        sig = await agent_key.sign_async(b'test')\n        self.assertTrue(pubkey.verify(b'test', sig))", "def test_rsa_encrypt_error(self):\n    \"\"\"Test RSA encryption error\"\"\"\n    privkey = get_test_key('ssh-rsa', 2048)\n    pubkey = privkey.convert_to_public()\n    self.assertIsNone(pubkey.encrypt(os.urandom(256), pubkey.algorithm))", "def test_generate_errors(self):\n    \"\"\"Test errors in private key and certificate generation\"\"\"\n    for alg_name, kwargs in (('xxx', {}), ('ssh-dss', {'xxx': 0}), ('ssh-rsa', {'xxx': 0}), ('ecdsa-sha2-nistp256', {'xxx': 0}), ('ssh-ed25519', {'xxx': 0}), ('ssh-ed448', {'xxx': 0})):\n        with self.subTest(alg_name=alg_name, **kwargs):\n            with self.assertRaises(asyncssh.KeyGenerationError):\n                asyncssh.generate_private_key(alg_name, **kwargs)\n    privkey = get_test_key('ssh-rsa')\n    pubkey = privkey.convert_to_public()\n    privca = get_test_key('ssh-rsa', 1)\n    with self.assertRaises(asyncssh.KeyGenerationError):\n        privca.generate_user_certificate(pubkey, 'name', version=0)\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after=())\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after='xxx')\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after='now', valid_before='-1m')\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after=())\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after='xxx')\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after='now', valid_before='-1m')\n    privca.x509_algorithms = None\n    with self.assertRaises(asyncssh.KeyGenerationError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user')", "@agent_test\nasync def test_reconnect(self, agent):\n    \"\"\"Test reconnecting to the agent after closing it\"\"\"\n    key = get_test_key('ecdsa-sha2-nistp256')\n    pubkey = key.convert_to_public()\n    async with agent:\n        await agent.add_keys([key])\n        agent_keys = await agent.get_keys()\n    for agent_key in agent_keys:\n        sig = await agent_key.sign_async(b'test')\n        self.assertTrue(pubkey.verify(b'test', sig))", "@agent_test\nasync def test_sign(self, agent):\n    \"\"\"Test signing a block of data using the agent\"\"\"\n    algs = ['ssh-rsa', 'ecdsa-sha2-nistp256']\n    if ed25519_available:\n        algs.append('ssh-ed25519')\n    for alg_name in algs:\n        key = get_test_key(alg_name)\n        pubkey = key.convert_to_public()\n        cert = key.generate_user_certificate(key, 'name')\n        await agent.add_keys([(key, cert)])\n        agent_keys = await agent.get_keys()\n        for agent_key in agent_keys:\n            agent_key.set_sig_algorithm(agent_key.sig_algorithms[0])\n            sig = await agent_key.sign_async(b'test')\n            self.assertTrue(pubkey.verify(b'test', sig))\n        await agent.remove_keys(agent_keys)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'convert_to_public' function should accept an instance of SSHKey and return an SSHKey object containing only the public key data. The output should be of type SSHKey and should not contain any private key data.", "unit_test": "def test_convert_to_public_output_type(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public()\n    self.assertIsInstance(public_key, SSHKey)\n    self.assertNotIn('private_data', dir(public_key))", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_output_type"}, "Exception Handling": {"requirement": "The 'convert_to_public' function should raise a ValueError if the input SSHKey object does not contain a valid private key.", "unit_test": "def test_convert_to_public_invalid_key(self):\n    key = SSHKey()\n    with self.assertRaises(ValueError):\n        key.convert_to_public()", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_invalid_key"}, "Edge Case Handling": {"requirement": "The 'convert_to_public' function should handle cases where the SSHKey object has no comment or filename set, ensuring the output public key still functions correctly.", "unit_test": "def test_convert_to_public_no_comment_filename(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    key.set_comment(None)\n    key.set_filename(None)\n    public_key = key.convert_to_public()\n    self.assertIsNone(public_key.get_comment_bytes())\n    self.assertIsNone(public_key.get_filename())", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_no_comment_filename"}, "Functionality Extension": {"requirement": "Extend the 'convert_to_public' function to optionally accept a new comment and filename for the public key, overriding any existing values.", "unit_test": "def test_convert_to_public_with_new_comment_filename(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public(comment='New Comment', filename='new_filename.pub')\n    self.assertEqual(public_key.get_comment(), 'New Comment')\n    self.assertEqual(public_key.get_filename(), b'new_filename.pub')", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_with_new_comment_filename"}, "Annotation Coverage": {"requirement": "Ensure that the 'convert_to_public' function has complete type annotations for its parameters and return type.", "unit_test": "def test_convert_to_public_annotations(self):\n    annotations = SSHKey.convert_to_public.__annotations__\n    self.assertIn('self', annotations)\n    self.assertEqual(annotations['return'], SSHKey)", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_annotations"}, "Code Complexity": {"requirement": "The 'convert_to_public' function should maintain a cyclomatic complexity of 5 or less to ensure readability and maintainability.", "unit_test": "def test_convert_to_public_complexity(self):\n    complexity = get_cyclomatic_complexity(SSHKey.convert_to_public)\n    self.assertLessEqual(complexity, 5)", "test": "tests/test_agent.py::_TestAgent::test_code_complexity"}, "Code Standard": {"requirement": "The 'convert_to_public' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": "def test_code_style(self):\n    import pep8\n    style = pep8.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/ssh_key_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/test_agent.py::_TestAgent::test_code_style"}, "Context Usage Verification": {"requirement": "The 'convert_to_public' function should utilize the context attributes '_comment', '_filename', and 'public_data' of the SSHKey class.", "unit_test": "def test_convert_to_public_context_usage(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public()\n    self.assertEqual(public_key._comment, key._comment)\n    self.assertEqual(public_key._filename, key._filename)\n    self.assertEqual(public_key.public_data, key.public_data)", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'convert_to_public' function should correctly use the '_comment' and '_filename' attributes to set the corresponding values in the public key.", "unit_test": "def test_convert_to_public_context_correctness(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    key.set_comment('Test Comment')\n    key.set_filename('test_filename.pub')\n    public_key = key.convert_to_public()\n    self.assertEqual(public_key.get_comment(), 'Test Comment')\n    self.assertEqual(public_key.get_filename(), b'test_filename.pub')", "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_context_correctness"}}}
{"namespace": "rows.fields.DateField.deserialize", "type": "method", "project_path": "Text-Processing/rows", "completion_path": "Text-Processing/rows/rows/fields.py", "signature_position": [367, 367], "body_position": [368, 375], "dependency": {"intra_class": ["rows.fields.DateField.INPUT_FORMAT", "rows.fields.DateField.TYPE"], "intra_file": ["rows.fields.Field", "rows.fields.Field.deserialize", "rows.fields.as_string"], "cross_file": []}, "requirement": {"Functionality": "Deserialize a value into a date instance. It first calls the parent class's deserialize method to convert the value into a date object. Then, it checks if the value is already None or an instance of allowed type in DateField class. If so, it returns the value as is. Otherwise, it converts the value into a string, and parse the string value into a datetime object and creates a new date object using the year, month, and day attributes of the datetime object.", "Arguments": ":param cls: Class. The DateField class.\n:param value: Object. The value to be deserialized into a DateField instance.\n:param args: Object. Additional positional arguments.\n:param kwargs: Object. Additional keyword arguments.\n:return: date. The deserialized date instance."}, "tests": ["tests/tests_fields.py::FieldsTestCase::test_DateField"], "indent": 4, "domain": "Text-Processing", "code": "    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n", "context": "# coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n###The function: deserialize###\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "prompt": "Please write a python function called 'deserialize' base the context. Deserialize a value into a date instance. It first calls the parent class's deserialize method to convert the value into a date object. Then, it checks if the value is already None or an instance of allowed type in DateField class. If so, it returns the value as is. Otherwise, it converts the value into a string, and parse the string value into a datetime object and creates a new date object using the year, month, and day attributes of the datetime object.:param cls: Class. The DateField class.\n:param value: Object. The value to be deserialized into a DateField instance.\n:param args: Object. Additional positional arguments.\n:param kwargs: Object. Additional keyword arguments.\n:return: date. The deserialized date instance..\n        The context you need to refer to is as follows: # coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n###The function: deserialize###\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "test_list": ["def test_DateField(self):\n    serialized = '2015-05-27'\n    deserialized = datetime.date(2015, 5, 27)\n    self.assertEqual(fields.DateField.TYPE, (datetime.date,))\n    self.assertEqual(fields.DateField.serialize(None), '')\n    self.assertIs(type(fields.DateField.serialize(None)), six.text_type)\n    self.assertIn(type(fields.DateField.deserialize(serialized)), fields.DateField.TYPE)\n    self.assertEqual(fields.DateField.deserialize(serialized), deserialized)\n    self.assertEqual(fields.DateField.deserialize(deserialized), deserialized)\n    self.assertEqual(fields.DateField.deserialize(None), None)\n    self.assertEqual(fields.DateField.deserialize(''), None)\n    self.assertEqual(fields.DateField.serialize(deserialized), serialized)\n    self.assertIs(type(fields.DateField.serialize(deserialized)), six.text_type)\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(42)\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(serialized + 'T00:00:00')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize('\u00c1lvaro')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(serialized.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'deserialize' function should correctly convert valid string representations of dates into date objects and return None for null or empty string inputs.", "unit_test": "def test_deserialize_input_output_conditions(self):\n    self.assertEqual(fields.DateField.deserialize('2023-10-01'), datetime.date(2023, 10, 1))\n    self.assertEqual(fields.DateField.deserialize(''), None)\n    self.assertEqual(fields.DateField.deserialize(None), None)", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_input_output_conditions"}, "Exception Handling": {"requirement": "The 'deserialize' function should raise a ValueError when the input is not a valid date string or date object.", "unit_test": "def test_deserialize_exception_handling(self):\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize('invalid-date')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(12345)", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_exception_handling"}, "Edge Case Handling": {"requirement": "The 'deserialize' function should handle edge cases such as leap years and the minimum and maximum representable dates.", "unit_test": "def test_deserialize_edge_case_handling(self):\n    self.assertEqual(fields.DateField.deserialize('2020-02-29'), datetime.date(2020, 2, 29))\n    self.assertEqual(fields.DateField.deserialize('0001-01-01'), datetime.date(1, 1, 1))\n    self.assertEqual(fields.DateField.deserialize('9999-12-31'), datetime.date(9999, 12, 31))", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'deserialize' function to support additional date formats specified in the 'DateField.INPUT_FORMAT'.", "unit_test": "def test_deserialize_functionality_extension(self):\n    fields.DateField.INPUT_FORMAT = '%d-%m-%Y'\n    self.assertEqual(fields.DateField.deserialize('01-10-2023'), datetime.date(2023, 10, 1))", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'deserialize' function has complete annotation coverage for parameters and return types.", "unit_test": "def test_deserialize_annotation_coverage(self):\n    annotations = fields.DateField.deserialize.__annotations__\n    self.assertEqual(annotations['value'], 'Object')\n    self.assertEqual(annotations['return'], 'date')", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_annotation_coverage"}, "Code Complexity": {"requirement": "The 'deserialize' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": "def test_deserialize_code_complexity(self):\n    # This is a placeholder for a complexity check tool\n    complexity = calculate_cyclomatic_complexity(fields.DateField.deserialize)\n    self.assertLessEqual(complexity, 5)", "test": "tests/tests_fields.py::FieldsTestCase::test_code_complexity"}, "Code Standard": {"requirement": "The 'deserialize' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_deserialize_code_standard(self):\n    # This is a placeholder for a PEP 8 compliance check tool\n    pep8_violations = check_pep8_compliance(fields.DateField.deserialize)\n    self.assertEqual(pep8_violations, 0)", "test": "tests/tests_fields.py::FieldsTestCase::test_code_style"}, "Context Usage Verification": {"requirement": "The 'deserialize' function should utilize the 'rows.fields.DateField.INPUT_FORMAT' context to parse date strings.", "unit_test": "def test_deserialize_context_usage_verification(self):\n    self.assertIn('rows.fields.DateField.INPUT_FORMAT', fields.DateField.deserialize.__code__.co_names)", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'deserialize' function should correctly use 'rows.fields.DateField.TYPE' to verify the type of deserialized objects.", "unit_test": "def test_deserialize_context_usage_correctness_verification(self):\n    self.assertIn('rows.fields.DateField.TYPE', fields.DateField.deserialize.__code__.co_names)", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_type_verification"}}}
{"namespace": "rows.fields.EmailField.deserialize", "type": "method", "project_path": "Text-Processing/rows", "completion_path": "Text-Processing/rows/rows/fields.py", "signature_position": [445, 445], "body_position": [446, 454], "dependency": {"intra_class": ["rows.fields.EmailField.EMAIL_REGEXP"], "intra_file": ["rows.fields.TextField", "rows.fields.TextField.deserialize", "rows.fields.value_error"], "cross_file": []}, "requirement": {"Functionality": "Deserialize the input value and validate it as an email field. It first calls the superclass's deserialize method to perform the initial deserialization. Then, it checks if the deserialized value is None or empty. If it is, it returns None. Otherwise, it uses a regular expression to validate the email format. If the email is valid, it returns the first match. If not, it raises a value error.", "Arguments": ":param cls: Class. The class object itself.\n:param value: Any. The value to be deserialized and validated as an email field.\n:param *args: Any. Additional positional arguments.\n:param **kwargs: Any. Additional keyword arguments.\n:return: Object. The deserialized and validated email value, or None if the input value is None or empty."}, "tests": ["tests/tests_fields.py::FieldsTestCase::test_EmailField"], "indent": 4, "domain": "Text-Processing", "code": "    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n", "context": "# coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n###The function: deserialize###\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "prompt": "Please write a python function called 'deserialize' base the context. Deserialize the input value and validate it as an email field. It first calls the superclass's deserialize method to perform the initial deserialization. Then, it checks if the deserialized value is None or empty. If it is, it returns None. Otherwise, it uses a regular expression to validate the email format. If the email is valid, it returns the first match. If not, it raises a value error.:param cls: Class. The class object itself.\n:param value: Any. The value to be deserialized and validated as an email field.\n:param *args: Any. Additional positional arguments.\n:param **kwargs: Any. Additional keyword arguments.\n:return: Object. The deserialized and validated email value, or None if the input value is None or empty..\n        The context you need to refer to is as follows: # coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n###The function: deserialize###\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "test_list": ["def test_EmailField(self):\n    serialized = 'test@domain.com'\n    self.assertEqual(fields.EmailField.TYPE, (six.text_type,))\n    deserialized = fields.EmailField.deserialize(serialized)\n    self.assertIn(type(deserialized), fields.EmailField.TYPE)\n    self.assertEqual(fields.EmailField.serialize(None), '')\n    self.assertIs(type(fields.EmailField.serialize(None)), six.text_type)\n    self.assertEqual(fields.EmailField.serialize(serialized), serialized)\n    self.assertEqual(fields.EmailField.deserialize(serialized), serialized)\n    self.assertEqual(fields.EmailField.deserialize(None), None)\n    self.assertEqual(fields.EmailField.deserialize(''), None)\n    self.assertIs(type(fields.EmailField.serialize(serialized)), six.text_type)\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize(42)\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('2015-01-01')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('\u00c1lvaro')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('test@example.com'.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'deserialize' function should correctly return None when the input value is None or an empty string, and should return a valid email string when the input is a valid email.", "unit_test": "def test_deserialize_input_output_conditions(self):\n    self.assertIsNone(fields.EmailField.deserialize(None))\n    self.assertIsNone(fields.EmailField.deserialize(''))\n    self.assertEqual(fields.EmailField.deserialize('test@domain.com'), 'test@domain.com')", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_input_output_conditions"}, "Exception Handling": {"requirement": "The 'deserialize' function should raise a ValueError when the input value is not a valid email format.", "unit_test": "def test_deserialize_exception_handling(self):\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('invalid-email')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('missing@domain')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('missingdomain.com')", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_exception_handling"}, "Edge Case Handling": {"requirement": "The 'deserialize' function should handle edge cases such as emails with subdomains and plus signs correctly.", "unit_test": "def test_deserialize_edge_case_handling(self):\n    self.assertEqual(fields.EmailField.deserialize('user+tag@sub.domain.com'), 'user+tag@sub.domain.com')\n    self.assertEqual(fields.EmailField.deserialize('user@sub.domain.com'), 'user@sub.domain.com')", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'deserialize' function to support email validation with international domain names.", "unit_test": "def test_deserialize_functionality_extension(self):\n    self.assertEqual(fields.EmailField.deserialize('user@xn--d1acj3b.com'), 'user@xn--d1acj3b.com')\n    self.assertEqual(fields.EmailField.deserialize('user@xn--bcher-kva.ch'), 'user@xn--bcher-kva.ch')", "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'deserialize' function has complete parameter and return type annotations.", "unit_test": "def test_annotation_coverage(self):\n    from inspect import signature\n    sig = signature(fields.EmailField.deserialize)\n    self.assertEqual(sig.parameters['value'].annotation, 'Any')\n    self.assertEqual(sig.return_annotation, 'Optional[str]')", "test": "tests/tests_fields.py::FieldsTestCase::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'deserialize' function should maintain a cyclomatic complexity of 5 or lower.", "unit_test": "def test_code_complexity(self):\n    import radon.complexity as rc\n    from radon.visitors import ComplexityVisitor\n    code = '''\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        value = as_string(value)\n        if cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError('Invalid email format')\n    '''\n    visitor = ComplexityVisitor.from_code(code)\n    self.assertLessEqual(visitor.functions[0].complexity, 5)", "test": "tests/tests_fields.py::FieldsTestCase::test_EmailField_complexity"}, "Code Standard": {"requirement": "The 'deserialize' function should adhere to PEP 8 standards, including proper indentation and line length.", "unit_test": "def test_code_standard(self):\n    import pep8\n    style_guide = pep8.StyleGuide(quiet=True)\n    result = style_guide.check_files(['fields.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')", "test": "tests/tests_fields.py::FieldsTestCase::test_EmailField_style"}, "Context Usage Verification": {"requirement": "The 'deserialize' function should utilize the EMAIL_REGEXP from the context for email validation.", "unit_test": "def test_context_usage_verification(self):\n    self.assertTrue(hasattr(fields.EmailField, 'EMAIL_REGEXP'))\n    self.assertIsInstance(fields.EmailField.EMAIL_REGEXP, re.Pattern)", "test": "tests/tests_fields.py::FieldsTestCase::test_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'deserialize' function should correctly use the EMAIL_REGEXP to match valid email formats.", "unit_test": "def test_context_usage_correctness_verification(self):\n    valid_email = 'test@domain.com'\n    invalid_email = 'invalid-email'\n    self.assertIsNotNone(fields.EmailField.EMAIL_REGEXP.match(valid_email))\n    self.assertIsNone(fields.EmailField.EMAIL_REGEXP.match(invalid_email))", "test": "tests/tests_fields.py::FieldsTestCase::test_context_usage_correctness_verification"}}}
{"namespace": "playhouse.dataset.DataSet.freeze", "type": "method", "project_path": "Software-Development/peewee", "completion_path": "Software-Development/peewee/playhouse/dataset.py", "signature_position": [162, 163], "body_position": [164, 172], "dependency": {"intra_class": ["playhouse.dataset.DataSet._check_arguments", "playhouse.dataset.DataSet._export_formats"], "intra_file": ["playhouse.dataset.TSVExporter.export", "playhouse.dataset.open_file"], "cross_file": []}, "requirement": {"Functionality": "Freeze the dataset by exporting it to a file in the specified format. It checks the arguments, opens the file if a filename is provided, creates an exporter instance based on the format, and exports the dataset to the file. Finally, it closes the file if it was opened.", "Arguments": ":param self: DataSet. An instance of the DataSet class.\n:param query: The query to export.\n:param format: String. The format in which to export the dataset. Defaults to 'csv'.\n:param filename: String. The name of the file to export to. If provided, the file will be opened and closed automatically.\n:param file_obj: File object. The file object to export to. If provided, the file will not be opened or closed automatically.\n:param encoding: String. The encoding to use when opening the file. Defaults to 'utf8'.\n:param kwargs: Additional keyword arguments to pass to the exporter's export method.\n:return: No return values."}, "tests": ["tests/dataset.py::TestDataSet::test_export", "tests/dataset.py::TestDataSet::test_freeze_thaw"], "indent": 4, "domain": "Software-Development", "code": "    def freeze(self, query, format='csv', filename=None, file_obj=None,\n               encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n", "context": "import csv\nimport datetime\nfrom decimal import Decimal\nimport json\nimport operator\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse\nimport sys\nimport uuid\n\nfrom peewee import *\nfrom playhouse.db_url import connect\nfrom playhouse.migrate import migrate\nfrom playhouse.migrate import SchemaMigrator\nfrom playhouse.reflection import Introspector\n\nif sys.version_info[0] == 3:\n    basestring = str\n    from functools import reduce\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode, encoding=encoding)\nelse:\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode)\n\n\nclass DataSet(object):\n    def __init__(self, url, include_views=False, **kwargs):\n        if isinstance(url, Database):\n            self._url = None\n            self._database = url\n            self._database_path = self._database.database\n        else:\n            self._url = url\n            parse_result = urlparse(url)\n            self._database_path = parse_result.path[1:]\n\n            # Connect to the database.\n            self._database = connect(url)\n\n        # Open a connection if one does not already exist.\n        self._database.connect(reuse_if_open=True)\n\n        # Introspect the database and generate models.\n        self._introspector = Introspector.from_database(self._database)\n        self._include_views = include_views\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views,\n            **kwargs)\n        self._migrator = SchemaMigrator.from_database(self._database)\n\n        class BaseModel(Model):\n            class Meta:\n                database = self._database\n        self._base_model = BaseModel\n        self._export_formats = self.get_export_formats()\n        self._import_formats = self.get_import_formats()\n\n    def __repr__(self):\n        return '<DataSet: %s>' % self._database_path\n\n    def get_export_formats(self):\n        return {\n            'csv': CSVExporter,\n            'json': JSONExporter,\n            'tsv': TSVExporter}\n\n    def get_import_formats(self):\n        return {\n            'csv': CSVImporter,\n            'json': JSONImporter,\n            'tsv': TSVImporter}\n\n    def __getitem__(self, table):\n        if table not in self._models and table in self.tables:\n            self.update_cache(table)\n        return Table(self, table, self._models.get(table))\n\n    @property\n    def tables(self):\n        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables\n\n    @property\n    def views(self):\n        return [v.name for v in self._database.get_views()]\n\n    def __contains__(self, table):\n        return table in self.tables\n\n    def connect(self, reuse_if_open=False):\n        self._database.connect(reuse_if_open=reuse_if_open)\n\n    def close(self):\n        self._database.close()\n\n    def update_cache(self, table=None):\n        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)\n\n    def get_table_dependencies(self, table):\n        stack = [table]\n        accum = []\n        seen = set()\n        while stack:\n            table = stack.pop()\n            for fk_meta in self._database.get_foreign_keys(table):\n                dest = fk_meta.dest_table\n                if dest not in seen:\n                    stack.append(dest)\n                    accum.append(dest)\n        return accum\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if not self._database.is_closed():\n            self.close()\n\n    def query(self, sql, params=None):\n        return self._database.execute_sql(sql, params)\n\n    def transaction(self):\n        return self._database.atomic()\n\n    def _check_arguments(self, filename, file_obj, format, format_dict):\n        if filename and file_obj:\n            raise ValueError('file is over-specified. Please use either '\n                             'filename or file_obj, but not both.')\n        if not filename and not file_obj:\n            raise ValueError('A filename or file-like object must be '\n                             'specified.')\n        if format not in format_dict:\n            valid_formats = ', '.join(sorted(format_dict.keys()))\n            raise ValueError('Unsupported format \"%s\". Use one of %s.' % (\n                format, valid_formats))\n\n###The function: freeze###\n    def thaw(self, table, format='csv', filename=None, file_obj=None,\n             strict=False, encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'r', encoding)\n\n        importer = self._import_formats[format](self[table], strict)\n        count = importer.load(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n\n        return count\n\n\nclass Table(object):\n    def __init__(self, dataset, name, model_class):\n        self.dataset = dataset\n        self.name = name\n        if model_class is None:\n            model_class = self._create_model()\n            model_class.create_table()\n            self.dataset._models[name] = model_class\n\n    @property\n    def model_class(self):\n        return self.dataset._models[self.name]\n\n    def __repr__(self):\n        return '<Table: %s>' % self.name\n\n    def __len__(self):\n        return self.find().count()\n\n    def __iter__(self):\n        return iter(self.find().iterator())\n\n    def _create_model(self):\n        class Meta:\n            table_name = self.name\n        return type(\n            str(self.name),\n            (self.dataset._base_model,),\n            {'Meta': Meta})\n\n    def create_index(self, columns, unique=False):\n        index = ModelIndex(self.model_class, columns, unique=unique)\n        self.model_class.add_index(index)\n        self.dataset._database.execute(index)\n\n    def _guess_field_type(self, value):\n        if isinstance(value, basestring):\n            return TextField\n        if isinstance(value, (datetime.date, datetime.datetime)):\n            return DateTimeField\n        elif value is True or value is False:\n            return BooleanField\n        elif isinstance(value, int):\n            return IntegerField\n        elif isinstance(value, float):\n            return FloatField\n        elif isinstance(value, Decimal):\n            return DecimalField\n        return TextField\n\n    @property\n    def columns(self):\n        return [f.name for f in self.model_class._meta.sorted_fields]\n\n    def _migrate_new_columns(self, data):\n        new_keys = set(data) - set(self.model_class._meta.fields)\n        new_keys -= set(self.model_class._meta.columns)\n        if new_keys:\n            operations = []\n            for key in new_keys:\n                field_class = self._guess_field_type(data[key])\n                field = field_class(null=True)\n                operations.append(\n                    self.dataset._migrator.add_column(self.name, key, field))\n                field.bind(self.model_class, key)\n\n            migrate(*operations)\n\n            self.dataset.update_cache(self.name)\n\n    def __getitem__(self, item):\n        try:\n            return self.model_class[item]\n        except self.model_class.DoesNotExist:\n            pass\n\n    def __setitem__(self, item, value):\n        if not isinstance(value, dict):\n            raise ValueError('Table.__setitem__() value must be a dict')\n\n        pk = self.model_class._meta.primary_key\n        value[pk.name] = item\n\n        try:\n            with self.dataset.transaction() as txn:\n                self.insert(**value)\n        except IntegrityError:\n            self.dataset.update_cache(self.name)\n            self.update(columns=[pk.name], **value)\n\n    def __delitem__(self, item):\n        del self.model_class[item]\n\n    def insert(self, **data):\n        self._migrate_new_columns(data)\n        return self.model_class.insert(**data).execute()\n\n    def _apply_where(self, query, filters, conjunction=None):\n        conjunction = conjunction or operator.and_\n        if filters:\n            expressions = [\n                (self.model_class._meta.fields[column] == value)\n                for column, value in filters.items()]\n            query = query.where(reduce(conjunction, expressions))\n        return query\n\n    def update(self, columns=None, conjunction=None, **data):\n        self._migrate_new_columns(data)\n        filters = {}\n        if columns:\n            for column in columns:\n                filters[column] = data.pop(column)\n\n        return self._apply_where(\n            self.model_class.update(**data),\n            filters,\n            conjunction).execute()\n\n    def _query(self, **query):\n        return self._apply_where(self.model_class.select(), query)\n\n    def find(self, **query):\n        return self._query(**query).dicts()\n\n    def find_one(self, **query):\n        try:\n            return self.find(**query).get()\n        except self.model_class.DoesNotExist:\n            return None\n\n    def all(self):\n        return self.find()\n\n    def delete(self, **query):\n        return self._apply_where(self.model_class.delete(), query).execute()\n\n    def freeze(self, *args, **kwargs):\n        return self.dataset.freeze(self.all(), *args, **kwargs)\n\n    def thaw(self, *args, **kwargs):\n        return self.dataset.thaw(self.name, *args, **kwargs)\n\n\nclass Exporter(object):\n    def __init__(self, query):\n        self.query = query\n\n    def export(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONExporter(Exporter):\n    def __init__(self, query, iso8601_datetimes=False):\n        super(JSONExporter, self).__init__(query)\n        self.iso8601_datetimes = iso8601_datetimes\n\n    def _make_default(self):\n        datetime_types = (datetime.datetime, datetime.date, datetime.time)\n\n        if self.iso8601_datetimes:\n            def default(o):\n                if isinstance(o, datetime_types):\n                    return o.isoformat()\n                elif isinstance(o, (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        else:\n            def default(o):\n                if isinstance(o, datetime_types + (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        return default\n\n    def export(self, file_obj, **kwargs):\n        json.dump(\n            list(self.query),\n            file_obj,\n            default=self._make_default(),\n            **kwargs)\n\n\nclass CSVExporter(Exporter):\n    def export(self, file_obj, header=True, **kwargs):\n        writer = csv.writer(file_obj, **kwargs)\n        tuples = self.query.tuples().execute()\n        tuples.initialize()\n        if header and getattr(tuples, 'columns', None):\n            writer.writerow([column for column in tuples.columns])\n        for row in tuples:\n            writer.writerow(row)\n\n\nclass TSVExporter(CSVExporter):\n    def export(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVExporter, self).export(file_obj, header, **kwargs)\n\n\nclass Importer(object):\n    def __init__(self, table, strict=False):\n        self.table = table\n        self.strict = strict\n\n        model = self.table.model_class\n        self.columns = model._meta.columns\n        self.columns.update(model._meta.fields)\n\n    def load(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONImporter(Importer):\n    def load(self, file_obj, **kwargs):\n        data = json.load(file_obj, **kwargs)\n        count = 0\n\n        for row in data:\n            if self.strict:\n                obj = {}\n                for key in row:\n                    field = self.columns.get(key)\n                    if field is not None:\n                        obj[field.name] = field.python_value(row[key])\n            else:\n                obj = row\n\n            if obj:\n                self.table.insert(**obj)\n                count += 1\n\n        return count\n\n\nclass CSVImporter(Importer):\n    def load(self, file_obj, header=True, **kwargs):\n        count = 0\n        reader = csv.reader(file_obj, **kwargs)\n        if header:\n            try:\n                header_keys = next(reader)\n            except StopIteration:\n                return count\n\n            if self.strict:\n                header_fields = []\n                for idx, key in enumerate(header_keys):\n                    if key in self.columns:\n                        header_fields.append((idx, self.columns[key]))\n            else:\n                header_fields = list(enumerate(header_keys))\n        else:\n            header_fields = list(enumerate(self.model._meta.sorted_fields))\n\n        if not header_fields:\n            return count\n\n        for row in reader:\n            obj = {}\n            for idx, field in header_fields:\n                if self.strict:\n                    obj[field.name] = field.python_value(row[idx])\n                else:\n                    obj[field] = row[idx]\n\n            self.table.insert(**obj)\n            count += 1\n\n        return count\n\n\nclass TSVImporter(CSVImporter):\n    def load(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVImporter, self).load(file_obj, header, **kwargs)\n", "prompt": "Please write a python function called 'freeze' base the context. Freeze the dataset by exporting it to a file in the specified format. It checks the arguments, opens the file if a filename is provided, creates an exporter instance based on the format, and exports the dataset to the file. Finally, it closes the file if it was opened.:param self: DataSet. An instance of the DataSet class.\n:param query: The query to export.\n:param format: String. The format in which to export the dataset. Defaults to 'csv'.\n:param filename: String. The name of the file to export to. If provided, the file will be opened and closed automatically.\n:param file_obj: File object. The file object to export to. If provided, the file will not be opened or closed automatically.\n:param encoding: String. The encoding to use when opening the file. Defaults to 'utf8'.\n:param kwargs: Additional keyword arguments to pass to the exporter's export method.\n:return: No return values..\n        The context you need to refer to is as follows: import csv\nimport datetime\nfrom decimal import Decimal\nimport json\nimport operator\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse\nimport sys\nimport uuid\n\nfrom peewee import *\nfrom playhouse.db_url import connect\nfrom playhouse.migrate import migrate\nfrom playhouse.migrate import SchemaMigrator\nfrom playhouse.reflection import Introspector\n\nif sys.version_info[0] == 3:\n    basestring = str\n    from functools import reduce\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode, encoding=encoding)\nelse:\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode)\n\n\nclass DataSet(object):\n    def __init__(self, url, include_views=False, **kwargs):\n        if isinstance(url, Database):\n            self._url = None\n            self._database = url\n            self._database_path = self._database.database\n        else:\n            self._url = url\n            parse_result = urlparse(url)\n            self._database_path = parse_result.path[1:]\n\n            # Connect to the database.\n            self._database = connect(url)\n\n        # Open a connection if one does not already exist.\n        self._database.connect(reuse_if_open=True)\n\n        # Introspect the database and generate models.\n        self._introspector = Introspector.from_database(self._database)\n        self._include_views = include_views\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views,\n            **kwargs)\n        self._migrator = SchemaMigrator.from_database(self._database)\n\n        class BaseModel(Model):\n            class Meta:\n                database = self._database\n        self._base_model = BaseModel\n        self._export_formats = self.get_export_formats()\n        self._import_formats = self.get_import_formats()\n\n    def __repr__(self):\n        return '<DataSet: %s>' % self._database_path\n\n    def get_export_formats(self):\n        return {\n            'csv': CSVExporter,\n            'json': JSONExporter,\n            'tsv': TSVExporter}\n\n    def get_import_formats(self):\n        return {\n            'csv': CSVImporter,\n            'json': JSONImporter,\n            'tsv': TSVImporter}\n\n    def __getitem__(self, table):\n        if table not in self._models and table in self.tables:\n            self.update_cache(table)\n        return Table(self, table, self._models.get(table))\n\n    @property\n    def tables(self):\n        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables\n\n    @property\n    def views(self):\n        return [v.name for v in self._database.get_views()]\n\n    def __contains__(self, table):\n        return table in self.tables\n\n    def connect(self, reuse_if_open=False):\n        self._database.connect(reuse_if_open=reuse_if_open)\n\n    def close(self):\n        self._database.close()\n\n    def update_cache(self, table=None):\n        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)\n\n    def get_table_dependencies(self, table):\n        stack = [table]\n        accum = []\n        seen = set()\n        while stack:\n            table = stack.pop()\n            for fk_meta in self._database.get_foreign_keys(table):\n                dest = fk_meta.dest_table\n                if dest not in seen:\n                    stack.append(dest)\n                    accum.append(dest)\n        return accum\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if not self._database.is_closed():\n            self.close()\n\n    def query(self, sql, params=None):\n        return self._database.execute_sql(sql, params)\n\n    def transaction(self):\n        return self._database.atomic()\n\n    def _check_arguments(self, filename, file_obj, format, format_dict):\n        if filename and file_obj:\n            raise ValueError('file is over-specified. Please use either '\n                             'filename or file_obj, but not both.')\n        if not filename and not file_obj:\n            raise ValueError('A filename or file-like object must be '\n                             'specified.')\n        if format not in format_dict:\n            valid_formats = ', '.join(sorted(format_dict.keys()))\n            raise ValueError('Unsupported format \"%s\". Use one of %s.' % (\n                format, valid_formats))\n\n###The function: freeze###\n    def thaw(self, table, format='csv', filename=None, file_obj=None,\n             strict=False, encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'r', encoding)\n\n        importer = self._import_formats[format](self[table], strict)\n        count = importer.load(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n\n        return count\n\n\nclass Table(object):\n    def __init__(self, dataset, name, model_class):\n        self.dataset = dataset\n        self.name = name\n        if model_class is None:\n            model_class = self._create_model()\n            model_class.create_table()\n            self.dataset._models[name] = model_class\n\n    @property\n    def model_class(self):\n        return self.dataset._models[self.name]\n\n    def __repr__(self):\n        return '<Table: %s>' % self.name\n\n    def __len__(self):\n        return self.find().count()\n\n    def __iter__(self):\n        return iter(self.find().iterator())\n\n    def _create_model(self):\n        class Meta:\n            table_name = self.name\n        return type(\n            str(self.name),\n            (self.dataset._base_model,),\n            {'Meta': Meta})\n\n    def create_index(self, columns, unique=False):\n        index = ModelIndex(self.model_class, columns, unique=unique)\n        self.model_class.add_index(index)\n        self.dataset._database.execute(index)\n\n    def _guess_field_type(self, value):\n        if isinstance(value, basestring):\n            return TextField\n        if isinstance(value, (datetime.date, datetime.datetime)):\n            return DateTimeField\n        elif value is True or value is False:\n            return BooleanField\n        elif isinstance(value, int):\n            return IntegerField\n        elif isinstance(value, float):\n            return FloatField\n        elif isinstance(value, Decimal):\n            return DecimalField\n        return TextField\n\n    @property\n    def columns(self):\n        return [f.name for f in self.model_class._meta.sorted_fields]\n\n    def _migrate_new_columns(self, data):\n        new_keys = set(data) - set(self.model_class._meta.fields)\n        new_keys -= set(self.model_class._meta.columns)\n        if new_keys:\n            operations = []\n            for key in new_keys:\n                field_class = self._guess_field_type(data[key])\n                field = field_class(null=True)\n                operations.append(\n                    self.dataset._migrator.add_column(self.name, key, field))\n                field.bind(self.model_class, key)\n\n            migrate(*operations)\n\n            self.dataset.update_cache(self.name)\n\n    def __getitem__(self, item):\n        try:\n            return self.model_class[item]\n        except self.model_class.DoesNotExist:\n            pass\n\n    def __setitem__(self, item, value):\n        if not isinstance(value, dict):\n            raise ValueError('Table.__setitem__() value must be a dict')\n\n        pk = self.model_class._meta.primary_key\n        value[pk.name] = item\n\n        try:\n            with self.dataset.transaction() as txn:\n                self.insert(**value)\n        except IntegrityError:\n            self.dataset.update_cache(self.name)\n            self.update(columns=[pk.name], **value)\n\n    def __delitem__(self, item):\n        del self.model_class[item]\n\n    def insert(self, **data):\n        self._migrate_new_columns(data)\n        return self.model_class.insert(**data).execute()\n\n    def _apply_where(self, query, filters, conjunction=None):\n        conjunction = conjunction or operator.and_\n        if filters:\n            expressions = [\n                (self.model_class._meta.fields[column] == value)\n                for column, value in filters.items()]\n            query = query.where(reduce(conjunction, expressions))\n        return query\n\n    def update(self, columns=None, conjunction=None, **data):\n        self._migrate_new_columns(data)\n        filters = {}\n        if columns:\n            for column in columns:\n                filters[column] = data.pop(column)\n\n        return self._apply_where(\n            self.model_class.update(**data),\n            filters,\n            conjunction).execute()\n\n    def _query(self, **query):\n        return self._apply_where(self.model_class.select(), query)\n\n    def find(self, **query):\n        return self._query(**query).dicts()\n\n    def find_one(self, **query):\n        try:\n            return self.find(**query).get()\n        except self.model_class.DoesNotExist:\n            return None\n\n    def all(self):\n        return self.find()\n\n    def delete(self, **query):\n        return self._apply_where(self.model_class.delete(), query).execute()\n\n    def freeze(self, *args, **kwargs):\n        return self.dataset.freeze(self.all(), *args, **kwargs)\n\n    def thaw(self, *args, **kwargs):\n        return self.dataset.thaw(self.name, *args, **kwargs)\n\n\nclass Exporter(object):\n    def __init__(self, query):\n        self.query = query\n\n    def export(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONExporter(Exporter):\n    def __init__(self, query, iso8601_datetimes=False):\n        super(JSONExporter, self).__init__(query)\n        self.iso8601_datetimes = iso8601_datetimes\n\n    def _make_default(self):\n        datetime_types = (datetime.datetime, datetime.date, datetime.time)\n\n        if self.iso8601_datetimes:\n            def default(o):\n                if isinstance(o, datetime_types):\n                    return o.isoformat()\n                elif isinstance(o, (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        else:\n            def default(o):\n                if isinstance(o, datetime_types + (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        return default\n\n    def export(self, file_obj, **kwargs):\n        json.dump(\n            list(self.query),\n            file_obj,\n            default=self._make_default(),\n            **kwargs)\n\n\nclass CSVExporter(Exporter):\n    def export(self, file_obj, header=True, **kwargs):\n        writer = csv.writer(file_obj, **kwargs)\n        tuples = self.query.tuples().execute()\n        tuples.initialize()\n        if header and getattr(tuples, 'columns', None):\n            writer.writerow([column for column in tuples.columns])\n        for row in tuples:\n            writer.writerow(row)\n\n\nclass TSVExporter(CSVExporter):\n    def export(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVExporter, self).export(file_obj, header, **kwargs)\n\n\nclass Importer(object):\n    def __init__(self, table, strict=False):\n        self.table = table\n        self.strict = strict\n\n        model = self.table.model_class\n        self.columns = model._meta.columns\n        self.columns.update(model._meta.fields)\n\n    def load(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONImporter(Importer):\n    def load(self, file_obj, **kwargs):\n        data = json.load(file_obj, **kwargs)\n        count = 0\n\n        for row in data:\n            if self.strict:\n                obj = {}\n                for key in row:\n                    field = self.columns.get(key)\n                    if field is not None:\n                        obj[field.name] = field.python_value(row[key])\n            else:\n                obj = row\n\n            if obj:\n                self.table.insert(**obj)\n                count += 1\n\n        return count\n\n\nclass CSVImporter(Importer):\n    def load(self, file_obj, header=True, **kwargs):\n        count = 0\n        reader = csv.reader(file_obj, **kwargs)\n        if header:\n            try:\n                header_keys = next(reader)\n            except StopIteration:\n                return count\n\n            if self.strict:\n                header_fields = []\n                for idx, key in enumerate(header_keys):\n                    if key in self.columns:\n                        header_fields.append((idx, self.columns[key]))\n            else:\n                header_fields = list(enumerate(header_keys))\n        else:\n            header_fields = list(enumerate(self.model._meta.sorted_fields))\n\n        if not header_fields:\n            return count\n\n        for row in reader:\n            obj = {}\n            for idx, field in header_fields:\n                if self.strict:\n                    obj[field.name] = field.python_value(row[idx])\n                else:\n                    obj[field] = row[idx]\n\n            self.table.insert(**obj)\n            count += 1\n\n        return count\n\n\nclass TSVImporter(CSVImporter):\n    def load(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVImporter, self).load(file_obj, header, **kwargs)\n", "test_list": ["def test_export(self):\n    self.create_users()\n    user = self.dataset['user']\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'json', file_obj=buf)\n    self.assertEqual(buf.getvalue(), '[{\"username\": \"charlie\"}, {\"username\": \"huey\"}]')\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'csv', file_obj=buf)\n    self.assertEqual(buf.getvalue().splitlines(), ['username', 'charlie', 'huey'])", "def test_freeze_thaw(self):\n    user = self.dataset['user']\n    user.insert(username='charlie')\n    note = self.dataset['note']\n    note_ts = datetime.datetime(2017, 1, 2, 3, 4, 5)\n    note.insert(content='foo', timestamp=note_ts, user_id='charlie', status=2)\n    buf = StringIO()\n    self.dataset.freeze(note.all(), 'json', file_obj=buf)\n    self.assertEqual(json.loads(buf.getvalue()), [{'id': 1, 'user_id': 'charlie', 'content': 'foo', 'status': 2, 'timestamp': '2017-01-02 03:04:05'}])\n    note.delete(id=1)\n    self.assertEqual(list(note.all()), [])\n    buf.seek(0)\n    note.thaw(format='json', file_obj=buf)\n    self.assertEqual(list(note.all()), [{'id': 1, 'user_id': 'charlie', 'content': 'foo', 'status': 2, 'timestamp': note_ts}])"], "requirements": {"Input-Output Conditions": {"requirement": "The 'freeze' function should validate that the 'query' parameter is iterable and contains valid data before proceeding with the export process.", "unit_test": "def test_freeze_query_validation(self):\n    with self.assertRaises(TypeError):\n        self.dataset.freeze(None, 'csv', file_obj=StringIO())\n    with self.assertRaises(ValueError):\n        self.dataset.freeze([], 'csv', file_obj=StringIO())", "test": "tests/dataset.py::TestDataSet::test_freeze_query_validation"}, "Exception Handling": {"requirement": "The 'freeze' function should raise a ValueError with a clear message if both 'filename' and 'file_obj' are provided.", "unit_test": "def test_freeze_file_specification_error(self):\n    with self.assertRaises(ValueError) as cm:\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', filename='test.csv', file_obj=StringIO())\n    self.assertEqual(str(cm.exception), 'file is over-specified. Please use either filename or file_obj, but not both.')", "test": "tests/dataset.py::TestDataSet::test_freeze_file_specification_error"}, "Edge Case Handling": {"requirement": "The 'freeze' function should handle the case where the dataset is empty and export an empty file without errors.", "unit_test": "def test_freeze_empty_dataset(self):\n    buf = StringIO()\n    self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=buf)\n    self.assertEqual(buf.getvalue(), 'username\\n')", "test": "tests/dataset.py::TestDataSet::test_freeze_empty_dataset"}, "Functionality Extension": {"requirement": "Extend the 'freeze' function to support exporting datasets in XML format.", "unit_test": "def test_freeze_xml_export(self):\n    self.create_users()\n    user = self.dataset['user']\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'xml', file_obj=buf)\n    self.assertTrue('<users>' in buf.getvalue())\n    self.assertTrue('<username>charlie</username>' in buf.getvalue())", "test": "tests/dataset.py::TestDataSet::test_freeze_xml_export"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'freeze' function are annotated with appropriate type hints.", "unit_test": "def test_freeze_annotations(self):\n    annotations = self.dataset.freeze.__annotations__\n    self.assertEqual(annotations['query'], 'Iterable')\n    self.assertEqual(annotations['format'], 'str')\n    self.assertEqual(annotations['filename'], 'Optional[str]')\n    self.assertEqual(annotations['file_obj'], 'Optional[IO]')\n    self.assertEqual(annotations['encoding'], 'str')", "test": "tests/dataset.py::TestDataSet::test_freeze_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'freeze' function should not exceed 5.", "unit_test": "def test_freeze_cyclomatic_complexity(self):\n    from radon.complexity import cc_visit\n    with open('dataset.py') as f:\n        code = f.read()\n    complexity = [c.complexity for c in cc_visit(code) if c.name == 'freeze']\n    self.assertTrue(all(c <= 10 for c in complexity))", "test": "tests/dataset.py::TestDataSet::test_code_complexity"}, "Code Standard": {"requirement": "Ensure that the 'freeze' function adheres to PEP 8 style guidelines.", "unit_test": "def test_code_style(self):\n    import subprocess\n    result = subprocess.run(['pycodestyle', 'dataset.py'], capture_output=True, text=True)\n    self.assertEqual(result.returncode, 0, msg=result.stdout)", "test": "tests/dataset.py::TestDataSet::test_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'freeze' function utilizes the '_check_arguments' method to validate input parameters.", "unit_test": "def test_freeze_uses_check_arguments(self):\n    with patch('playhouse.dataset.DataSet._check_arguments') as mock_check:\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=StringIO())\n        mock_check.assert_called_once()", "test": "tests/dataset.py::TestDataSet::test_freeze_uses_check_arguments"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'freeze' function correctly uses the '_export_formats' dictionary to select the appropriate exporter class.", "unit_test": "def test_freeze_uses_export_formats(self):\n    with patch.dict('playhouse.dataset.DataSet._export_formats', {'csv': MockExporter}) as mock_formats:\n        buf = StringIO()\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=buf)\n        self.assertTrue(mock_formats['csv'].called)", "test": "tests/dataset.py::TestDataSet::test_freeze_uses_export_formats"}}}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "type": "method", "project_path": "Security/pycoin", "completion_path": "Security/pycoin/pycoin/message/PeerAddress.py", "signature_position": [35, 35], "body_position": [36, 38], "dependency": {"intra_class": ["pycoin.message.PeerAddress.IP4_HEADER", "pycoin.message.PeerAddress.PeerAddress.ip_bin"], "intra_file": ["pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "pycoin.message.PeerAddress.ip_bin_to_ip6_addr"], "cross_file": []}, "requirement": {"Functionality": "This function determines the host address based on the IP binary string. If the IP binary string starts with the IP4 header, it converts the last 4 characters of the IP binary string to an IP4 address. Otherwise, it converts the entire IP binary string to an IP6 address.", "Arguments": ":param self: PeerAddress. An instance of the PeerAddress class.\n:return: The host address based on the IP binary string."}, "tests": ["tests/message_test.py::MessageTest::test_PeerAddress"], "indent": 4, "domain": "Security", "code": "    def host(self):\n        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "context": "import functools\nimport struct\n\n\nfrom pycoin.satoshi.satoshi_struct import parse_struct\nfrom pycoin.encoding.hexbytes import h2b\n\n\nIP4_HEADER = h2b(\"00000000000000000000FFFF\")\n\n\ndef ip_bin_to_ip6_addr(ip_bin):\n    return \":\".join(\"%x\" % v for v in struct.unpack(\">HHHHHHHH\", ip_bin))\n\n\ndef ip_bin_to_ip4_addr(ip_bin):\n    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))\n\n\n@functools.total_ordering\nclass PeerAddress(object):\n    def __init__(self, services, ip_bin, port):\n        self.services = int(services)\n        assert isinstance(ip_bin, bytes)\n        if len(ip_bin) == 4:\n            ip_bin = IP4_HEADER + ip_bin\n        assert len(ip_bin) == 16\n        self.ip_bin = ip_bin\n        self.port = port\n\n    def __repr__(self):\n        return \"%s/%d\" % (self.host(), self.port)\n\n###The function: host###\n    def stream(self, f):\n        f.write(struct.pack(\"<Q\", self.services))\n        f.write(self.ip_bin)\n        f.write(struct.pack(\"!H\", self.port))\n\n    @classmethod\n    def parse(self, f):\n        services, ip_bin, port = parse_struct(\"Q@h\", f)\n        self.ip_bin = ip_bin\n        return self(services, self.ip_bin, port)\n\n    def __lt__(self, other):\n        return (self.ip_bin, self.port, self.services) < (other.ip_bin, other.port, other.services)\n\n    def __eq__(self, other):\n        return self.services == other.services and \\\n            self.ip_bin == other.ip_bin and self.port == other.port\n", "prompt": "Please write a python function called 'host' base the context. This function determines the host address based on the IP binary string. If the IP binary string starts with the IP4 header, it converts the last 4 characters of the IP binary string to an IP4 address. Otherwise, it converts the entire IP binary string to an IP6 address.:param self: PeerAddress. An instance of the PeerAddress class.\n:return: The host address based on the IP binary string..\n        The context you need to refer to is as follows: import functools\nimport struct\n\n\nfrom pycoin.satoshi.satoshi_struct import parse_struct\nfrom pycoin.encoding.hexbytes import h2b\n\n\nIP4_HEADER = h2b(\"00000000000000000000FFFF\")\n\n\ndef ip_bin_to_ip6_addr(ip_bin):\n    return \":\".join(\"%x\" % v for v in struct.unpack(\">HHHHHHHH\", ip_bin))\n\n\ndef ip_bin_to_ip4_addr(ip_bin):\n    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))\n\n\n@functools.total_ordering\nclass PeerAddress(object):\n    def __init__(self, services, ip_bin, port):\n        self.services = int(services)\n        assert isinstance(ip_bin, bytes)\n        if len(ip_bin) == 4:\n            ip_bin = IP4_HEADER + ip_bin\n        assert len(ip_bin) == 16\n        self.ip_bin = ip_bin\n        self.port = port\n\n    def __repr__(self):\n        return \"%s/%d\" % (self.host(), self.port)\n\n###The function: host###\n    def stream(self, f):\n        f.write(struct.pack(\"<Q\", self.services))\n        f.write(self.ip_bin)\n        f.write(struct.pack(\"!H\", self.port))\n\n    @classmethod\n    def parse(self, f):\n        services, ip_bin, port = parse_struct(\"Q@h\", f)\n        self.ip_bin = ip_bin\n        return self(services, self.ip_bin, port)\n\n    def __lt__(self, other):\n        return (self.ip_bin, self.port, self.services) < (other.ip_bin, other.port, other.services)\n\n    def __eq__(self, other):\n        return self.services == other.services and \\\n            self.ip_bin == other.ip_bin and self.port == other.port\n", "test_list": ["def test_PeerAddress(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    pa_bytes = to_bin(pa)\n    pa1 = from_bin(PeerAddress, pa_bytes)\n    self.assertEqual(pa, pa1)\n    pa2 = PeerAddress(188, IP4_HEADER + h2b('c0a80162'), 8333)\n    self.assertTrue(pa1 > pa2)\n    self.assertTrue(pa1 >= pa2)\n    self.assertTrue(pa2 < pa1)\n    self.assertTrue(pa2 <= pa1)\n    self.assertNotEqual(pa2, pa1)\n    self.assertNotEqual(pa1, pa2)\n    self.assertEqual(pa1.host(), '192.168.1.99')\n    self.assertEqual(repr(pa1), '192.168.1.99/8333')\n    pa_v6 = PeerAddress(945, h2b('2607f8b04006080a000000000000200e'), 8333)\n    self.assertEqual(pa_v6.host(), '2607:f8b0:4006:80a:0:0:0:200e')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'host' function should correctly determine the host address based on the IP binary string, ensuring that the output is a valid IP4 or IP6 address.", "unit_test": "def test_host_function_output(self):\n    pa_ip4 = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertEqual(pa_ip4.host(), '192.168.1.99')\n    pa_ip6 = PeerAddress(945, h2b('2607f8b04006080a000000000000200e'), 8333)\n    self.assertEqual(pa_ip6.host(), '2607:f8b0:4006:80a:0:0:0:200e')", "test": "tests/message_test.py::MessageTest::test_host_function_output"}, "Exception Handling": {"requirement": "The 'host' function should raise a ValueError if the IP binary string is not of length 16.", "unit_test": "def test_host_function_exception(self):\n    with self.assertRaises(ValueError):\n        PeerAddress(188, h2b('c0a801'), 8333).host()", "test": "tests/message_test.py::MessageTest::test_host_function_exception"}, "Edge Case Handling": {"requirement": "The 'host' function should handle edge cases where the IP binary string is exactly the IP4 header followed by zeros.", "unit_test": "def test_host_function_edge_case(self):\n    pa_edge = PeerAddress(188, IP4_HEADER + h2b('00000000'), 8333)\n    self.assertEqual(pa_edge.host(), '0.0.0.0')", "test": "tests/message_test.py::MessageTest::test_host_function_edge_case"}, "Functionality Extension": {"requirement": "Extend the 'host' function to support conversion of IP binary strings that are exactly 4 bytes long, treating them as IP4 addresses.", "unit_test": "def test_host_function_extension(self):\n    pa_short_ip4 = PeerAddress(188, h2b('c0a80163'), 8333)\n    self.assertEqual(pa_short_ip4.host(), '192.168.1.99')", "test": "tests/message_test.py::MessageTest::test_host_function_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'host' function has complete parameter and return type annotations.", "unit_test": "def test_host_function_annotations(self):\n    from inspect import signature\n    sig = signature(PeerAddress.host)\n    self.assertEqual(str(sig.parameters['self'].annotation), '<class \\'__main__.PeerAddress\\'>')\n    self.assertEqual(sig.return_annotation, str)", "test": "tests/message_test.py::MessageTest::test_host_function_extension"}, "Code Complexity": {"requirement": "The 'host' function should maintain a cyclomatic complexity of 2, indicating a simple function with no branching.", "unit_test": "def test_host_function_complexity(self):\n    from radon.complexity import cc_visit\n    code = '''def host(self):\n    if self.ip_bin.startswith(IP4_HEADER):\n        return ip_bin_to_ip4_addr(self.ip_bin)\n    return ip_bin_to_ip6_addr(self.ip_bin)'''\n    complexity = cc_visit(code)\n    self.assertEqual(complexity[0].complexity, 1)", "test": "tests/message_test.py::MessageTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'host' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": "def test_host_function_pep8(self):\n    import pep8\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['peer_address.py'])\n    self.assertEqual(result.total_errors, 0)", "test": "tests/message_test.py::MessageTest::test_code_style"}, "Context Usage Verification": {"requirement": "The 'host' function should utilize the 'IP4_HEADER' and 'ip_bin' attributes from the PeerAddress class.", "unit_test": "def test_host_function_context_usage(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertIn('IP4_HEADER', pa.host.__code__.co_names)\n    self.assertIn('ip_bin', pa.host.__code__.co_names)", "test": "tests/message_test.py::MessageTest::test_host_function_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'host' function should correctly use the 'IP4_HEADER' to determine if the IP binary string is an IP4 address.", "unit_test": "def test_host_function_context_correctness(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertTrue(pa.ip_bin.startswith(IP4_HEADER))\n    self.assertEqual(pa.host(), '192.168.1.99')", "test": "tests/message_test.py::MessageTest::test_host_function_context_correctness"}}}
